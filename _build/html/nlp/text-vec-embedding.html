

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Word Embeddings &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/text-vec-embedding';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Attention and Transformers: Intuitions" href="dl-attention-transformer-intuition.html" />
    <link rel="prev" title="Neural Language Model: A Start" href="dl-neural-language-model-primer.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-common-nlp-tasks.html">Common NLP Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-neural-network-from-scratch.html">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">Neural Language Model: A Start</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">Sequence Model with Attention for Addition Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-machine-learning.html">5. Assignment V: Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6-topic-modeling.html">6. Assignment VI: Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7-dl-chinese-name-gender.html">7. Assignment VII: Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/8-sentiment-analysis-dl.html">8. Assignment VIII: Sentiment Analysis Using Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exams</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/midterm-exam-112.html">Midterm Exam</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/text-vec-embedding.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/text-vec-embedding.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Word Embeddings</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-word2vec">What is <code class="docutils literal notranslate"><span class="pre">word2vec</span></code>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-of-word-embeddings-distributional-semantics">Basis of Word Embeddings: Distributional Semantics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-training-algorithms-of-word2vec">Main training algorithms of <code class="docutils literal notranslate"><span class="pre">word2vec</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-intuitive-understanding-of-cbow">An Intuitive Understanding of CBOW</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-intuitive-understanding-of-skip-gram">An Intuitive Understanding of Skip-gram</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-necessary-dependencies-and-settings">Import necessary dependencies and settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-corpus-a-naive-example">Sample Corpus: A Naive Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-pre-processing">Simple text pre-processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#suggestions">Suggestions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-embeddings-using-word2vec">Training Embeddings Using word2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-word-embeddings">Visualizing Word Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-word-embeddings-to-document-embeddings">From Word Embeddings to Document Embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-embeddings-glove-in-spacy">Using Pre-trained Embeddings:  GloVe in <code class="docutils literal notranslate"><span class="pre">spacy</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-glove-word-embeddings">Visualize GloVe word embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fasttext"><code class="docutils literal notranslate"><span class="pre">fasttext</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrap-up">Wrap-up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="word-embeddings">
<h1>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>The state-of-art method of vectorizing texts is to learn the numeric representations of words using deep learning methods.</p></li>
<li><p>These deep-learning based numeric representations of linguistic units are commonly referred to as <strong>embeddings</strong>.</p></li>
<li><p>Word embeddings can be learned either along with the target NLP task (e.g., the <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer in RNN Language Model) or via an <strong>unsupervised</strong> method based on a large number of texts.</p></li>
<li><p>In this tutorial, we will look at two main algorithms in <code class="docutils literal notranslate"><span class="pre">word2vec</span></code> that allow us to learn the word embeddings in an <strong>unsupervised</strong> manner from a large collection of texts.</p></li>
</ul>
<ul class="simple">
<li><p>Strengths of word embeddings</p>
<ul>
<li><p>They can be learned using <strong>unsupervised</strong> methods.</p></li>
<li><p>They include quite a proportion of the lexical <strong>semantics</strong>.</p></li>
<li><p>They can be learned by <strong>batch</strong>. We don’t have to process the entire corpus and create the word-by-document matrix for vectorization.</p></li>
<li><p>Therefore, it is less likely to run into the <strong>memory</strong> capacity issue for huge corpora.</p></li>
</ul>
</li>
</ul>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h2>
<section id="what-is-word2vec">
<h3>What is <code class="docutils literal notranslate"><span class="pre">word2vec</span></code>?<a class="headerlink" href="#what-is-word2vec" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Word2vec</span></code> is one of the most popular techniques to learn word embeddings using a two-layer neural network.</p></li>
<li><p>The input is a <strong>text corpus</strong> and the output is a set of <strong>word vectors</strong>.</p></li>
<li><p>Research has shown that these embeddings include rich semantic information of words, which allow us to perform interesting <strong>semantic computation</strong> (See Mikolov et al’s works in References).</p></li>
</ul>
</section>
<section id="basis-of-word-embeddings-distributional-semantics">
<h3>Basis of Word Embeddings: Distributional Semantics<a class="headerlink" href="#basis-of-word-embeddings-distributional-semantics" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>“<em>You shall know a word by the company it keeps</em>” (Firth, 1975).</p></li>
<li><p>Word distributions show a considerable amount of <strong>lexical semantics</strong>.</p></li>
<li><p>Construction/Pattern distributions show a considerable amount of the <strong>constructional semantics</strong>.</p></li>
<li><p>Semantics of linguistic units are implicitly or explicitly embedded in their distributions (i.e., <em>occurrences</em> and <em>co-occurrences</em>) in language use (<strong>Distributional Semantics</strong>).</p></li>
</ul>
</section>
<section id="main-training-algorithms-of-word2vec">
<h3>Main training algorithms of <code class="docutils literal notranslate"><span class="pre">word2vec</span></code><a class="headerlink" href="#main-training-algorithms-of-word2vec" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Continuous Bag-of-Words (<strong>CBOW</strong>): The general language modeling task for embeddings training is to learn a model that is capable of using the <em><strong>context</strong></em> words to predict a <em><strong>target</strong></em> word.</p></li>
<li><p><strong>Skip-Gram</strong>: The general language modeling task for embeddings training is to learn a model that is capable of using a <em><strong>target word</strong></em> to predict its <em><strong>context</strong></em> words.</p></li>
</ul>
<p><img alt="" src="../_images/word2vec.png" /></p>
<ul class="simple">
<li><p>Other variants of embeddings training:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">fasttext</span></code> from Facebook</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GloVe</span></code> from Stanford NLP Group</p></li>
</ul>
</li>
<li><p>There are many ways to train work embeddings.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gensim</span></code>: Simplest and straightforward implementation of <code class="docutils literal notranslate"><span class="pre">word2vec</span></code>.</p></li>
<li><p>Training based on deep learning packages (e.g., <code class="docutils literal notranslate"><span class="pre">keras</span></code>, <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spacy</span></code> (It comes with the pre-trained embeddings models, using GloVe.)</p></li>
</ul>
</li>
<li><p>See Sarkar (2019), Chapter 4, for more comprehensive reviews.</p></li>
</ul>
</section>
<section id="an-intuitive-understanding-of-cbow">
<h3>An Intuitive Understanding of CBOW<a class="headerlink" href="#an-intuitive-understanding-of-cbow" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/word2vec-text-to-sequences.gif" /></p>
<p><img alt="" src="../_images/word2vec-cbow.gif" /></p>
</section>
<section id="an-intuitive-understanding-of-skip-gram">
<h3>An Intuitive Understanding of Skip-gram<a class="headerlink" href="#an-intuitive-understanding-of-skip-gram" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/word2vec-skipgram.gif" /></p>
</section>
</section>
<section id="import-necessary-dependencies-and-settings">
<h2>Import necessary dependencies and settings<a class="headerlink" href="#import-necessary-dependencies-and-settings" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_colwidth</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Google Colab Adhoc Setting</span>
<span class="c1"># !nvidia-smi</span>
<span class="c1"># nltk.download([&#39;gutenberg&#39;,&#39;punkt&#39;,&#39;stopwords&#39;])</span>
<span class="c1"># !pip show spacy</span>
<span class="c1"># !pip install --upgrade spacy</span>
<span class="c1"># #!python -m spacy download en_core_web_trf</span>
<span class="c1"># !python -m spacy download en_core_web_lg</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sample-corpus-a-naive-example">
<h2>Sample Corpus: A Naive Example<a class="headerlink" href="#sample-corpus-a-naive-example" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;The sky is blue and beautiful.&#39;</span><span class="p">,</span> <span class="s1">&#39;Love this blue and beautiful sky!&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The quick brown fox jumps over the lazy dog.&#39;</span><span class="p">,</span>
    <span class="s2">&quot;A king&#39;s breakfast has sausages, ham, bacon, eggs, toast and beans&quot;</span><span class="p">,</span>
    <span class="s1">&#39;I love green eggs, ham, sausages and bacon!&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The brown fox is quick and the blue dog is lazy!&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The sky is very blue and the sky is very beautiful today&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The dog is lazy but the brown fox is quick!&#39;</span>
<span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">,</span> <span class="s1">&#39;weather&#39;</span><span class="p">,</span>
    <span class="s1">&#39;animals&#39;</span>
<span class="p">]</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">corpus_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Document&#39;</span><span class="p">:</span> <span class="n">corpus</span><span class="p">,</span> <span class="s1">&#39;Category&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">})</span>
<span class="n">corpus_df</span> <span class="o">=</span> <span class="n">corpus_df</span><span class="p">[[</span><span class="s1">&#39;Document&#39;</span><span class="p">,</span> <span class="s1">&#39;Category&#39;</span><span class="p">]]</span>
<span class="n">corpus_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Document</th>
      <th>Category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The sky is blue and beautiful.</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Love this blue and beautiful sky!</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>2</th>
      <td>The quick brown fox jumps over the lazy dog.</td>
      <td>animals</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>
      <td>food</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I love green eggs, ham, sausages and bacon!</td>
      <td>food</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The brown fox is quick and the blue dog is lazy!</td>
      <td>animals</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The sky is very blue and the sky is very beautiful today</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The dog is lazy but the brown fox is quick!</td>
      <td>animals</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="simple-text-pre-processing">
<h3>Simple text pre-processing<a class="headerlink" href="#simple-text-pre-processing" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Usually for unsupervised <code class="docutils literal notranslate"><span class="pre">word2vec</span></code> learning, we don’t really need much text preprocessing.</p></li>
<li><p>So we keep our preprocessing to the minimum.</p>
<ul>
<li><p>Remove only symbols/punctuations, as well as redundant whitespaces.</p></li>
<li><p>Perform word tokenization, which would also determine the base units for embeddings learning.</p></li>
</ul>
</li>
</ul>
</section>
<section id="suggestions">
<h3>Suggestions<a class="headerlink" href="#suggestions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>If you are using <code class="docutils literal notranslate"><span class="pre">keras</span></code> to build the network for embeddings training, please prepare your input corpus data for <code class="docutils literal notranslate"><span class="pre">Tokenizer()</span></code>in the format where each <strong>token</strong> is delimited by a <strong>whitespace</strong>.</p></li>
<li><p>If you are using <code class="docutils literal notranslate"><span class="pre">gensim</span></code> to train word embeddings, please tokenize your corpus data first. That is, the <code class="docutils literal notranslate"><span class="pre">gensim</span></code> only requires a tokenized version of the corpus and it will learn the word embeddings for you.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wpt</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordPunctTokenizer</span><span class="p">()</span>
<span class="c1"># stop_words = nltk.corpus.stopwords.words(&#39;english&#39;)</span>
<span class="k">def</span> <span class="nf">preprocess_document</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="c1"># lower case and remove special characters\whitespaces</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-zA-Z\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">I</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">A</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="c1"># tokenize document</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">wpt</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>

<span class="n">corpus_norm</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess_document</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
<span class="n">corpus_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess_document</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">corpus_norm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;the sky is blue and beautiful&#39;, &#39;love this blue and beautiful sky&#39;, &#39;the quick brown fox jumps over the lazy dog&#39;, &#39;a kings breakfast has sausages ham bacon eggs toast and beans&#39;, &#39;i love green eggs ham sausages and bacon&#39;, &#39;the brown fox is quick and the blue dog is lazy&#39;, &#39;the sky is very blue and the sky is very beautiful today&#39;, &#39;the dog is lazy but the brown fox is quick&#39;]
[[&#39;the&#39;, &#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;and&#39;, &#39;beautiful&#39;], [&#39;love&#39;, &#39;this&#39;, &#39;blue&#39;, &#39;and&#39;, &#39;beautiful&#39;, &#39;sky&#39;], [&#39;the&#39;, &#39;quick&#39;, &#39;brown&#39;, &#39;fox&#39;, &#39;jumps&#39;, &#39;over&#39;, &#39;the&#39;, &#39;lazy&#39;, &#39;dog&#39;], [&#39;a&#39;, &#39;kings&#39;, &#39;breakfast&#39;, &#39;has&#39;, &#39;sausages&#39;, &#39;ham&#39;, &#39;bacon&#39;, &#39;eggs&#39;, &#39;toast&#39;, &#39;and&#39;, &#39;beans&#39;], [&#39;i&#39;, &#39;love&#39;, &#39;green&#39;, &#39;eggs&#39;, &#39;ham&#39;, &#39;sausages&#39;, &#39;and&#39;, &#39;bacon&#39;], [&#39;the&#39;, &#39;brown&#39;, &#39;fox&#39;, &#39;is&#39;, &#39;quick&#39;, &#39;and&#39;, &#39;the&#39;, &#39;blue&#39;, &#39;dog&#39;, &#39;is&#39;, &#39;lazy&#39;], [&#39;the&#39;, &#39;sky&#39;, &#39;is&#39;, &#39;very&#39;, &#39;blue&#39;, &#39;and&#39;, &#39;the&#39;, &#39;sky&#39;, &#39;is&#39;, &#39;very&#39;, &#39;beautiful&#39;, &#39;today&#39;], [&#39;the&#39;, &#39;dog&#39;, &#39;is&#39;, &#39;lazy&#39;, &#39;but&#39;, &#39;the&#39;, &#39;brown&#39;, &#39;fox&#39;, &#39;is&#39;, &#39;quick&#39;]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-embeddings-using-word2vec">
<h3>Training Embeddings Using word2vec<a class="headerlink" href="#training-embeddings-using-word2vec" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The expected inputs of <code class="docutils literal notranslate"><span class="pre">gensim.model.word2vec</span></code> is token-based corpus object.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">word2vec</span>

<span class="c1"># Set values for various parameters</span>
<span class="n">feature_size</span> <span class="o">=</span> <span class="mi">10</span>  
<span class="n">window_context</span> <span class="o">=</span> <span class="mi">5</span>  
<span class="n">min_word_count</span> <span class="o">=</span> <span class="mi">1</span>  

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span>
    <span class="n">corpus_tokens</span><span class="p">,</span>
    <span class="n">vector_size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span>        <span class="c1"># Word embeddings dimensionality</span>
    <span class="n">window</span><span class="o">=</span><span class="n">window_context</span><span class="p">,</span>    <span class="c1"># Context window size</span>
    <span class="n">min_count</span><span class="o">=</span><span class="n">min_word_count</span><span class="p">,</span> <span class="c1"># Minimum word count</span>
    <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>                     <span class="c1"># `1` for skip-gram; otherwise CBOW.</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">123</span><span class="p">,</span>               <span class="c1"># random seed</span>
    <span class="n">workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>                <span class="c1"># number of cores to use</span>
    <span class="n">negative</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>             <span class="c1"># how many negative samples should be drawn</span>
    <span class="n">cbow_mean</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>            <span class="c1"># whether to use the average of context word embeddings or sum(concat)</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>               <span class="c1"># number of epochs for the entire corpus</span>
    <span class="n">batch_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>        <span class="c1"># batch size</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 906 ms, sys: 549 ms, total: 1.45 s
Wall time: 1.23 s
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-word-embeddings">
<h3>Visualizing Word Embeddings<a class="headerlink" href="#visualizing-word-embeddings" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Embeddings represent words in multidimensional space.</p></li>
<li><p>We can inspect the quality of embeddings using dimensional reduction and visualize words in a 2D plot.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span> <span class="c1">## get the word forms of voculary</span>
<span class="n">wvs</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">words</span><span class="p">]</span> <span class="c1">## get embeddings of all word forms</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wvs</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">words</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/603096834789e14e3745b6a3f71dc0737f4338e7d35de3cf731a123a20473d6e.png" src="../_images/603096834789e14e3745b6a3f71dc0737f4338e7d35de3cf731a123a20473d6e.png" />
</div>
</div>
<ul class="simple">
<li><p>All trained word embeddings are included in <code class="docutils literal notranslate"><span class="pre">w2v_model.wv</span></code>.</p></li>
<li><p>We can extract all word forms in the vocabulary from <code class="docutils literal notranslate"><span class="pre">w2v_model.wv.index2word</span></code>.</p></li>
<li><p>We can easily extract embeddings for any specific words from <code class="docutils literal notranslate"><span class="pre">w2v_model.wv</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;the&#39;, &#39;is&#39;, &#39;and&#39;, &#39;sky&#39;, &#39;blue&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[:</span><span class="mi">5</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[array([-1.0949969 ,  0.28954926, -1.114496  , -0.42258066,  0.66788393,
        -1.3750594 , -0.70987606,  0.11280105,  0.05580178, -0.6932866 ],
       dtype=float32),
 array([-0.88506925,  0.9598221 , -1.5771676 , -0.1170712 ,  0.42075434,
        -1.6401567 , -0.19193946,  0.34415627,  0.61294013, -0.7693054 ],
       dtype=float32),
 array([-0.8441105 ,  0.55627626, -0.44809738, -0.00006142, -0.5046599 ,
         0.58909607, -0.5197542 , -0.83424497,  0.6392552 ,  0.27922842],
       dtype=float32),
 array([-1.9151906 ,  0.02310654, -0.37337798,  1.1486837 , -1.944635  ,
        -0.89013064, -0.15339209,  0.06195976,  0.87404585, -0.88589054],
       dtype=float32),
 array([-0.49760106,  0.33475614, -1.5774511 ,  0.16075088, -1.2333795 ,
        -0.64314014, -0.9123499 ,  0.9150968 , -0.01311138, -0.1773322 ],
       dtype=float32)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="from-word-embeddings-to-document-embeddings">
<h3>From Word Embeddings to Document Embeddings<a class="headerlink" href="#from-word-embeddings-to-document-embeddings" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>With word embeddings, we can compute the <strong>average embeddings</strong> for the entire document, i.e., the <em><strong>document embeddings</strong></em>.</p></li>
<li><p>These document embeddings are also assumed to have included considerable semantic information of the document.</p></li>
<li><p>We can for example use them for document classification/clustering.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">average_word_vectors</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>

    <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_features</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>
    <span class="n">nwords</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
            <span class="n">nwords</span> <span class="o">=</span> <span class="n">nwords</span> <span class="o">+</span> <span class="mf">1.</span>
            <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">feature_vector</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">nwords</span><span class="p">:</span>
        <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">feature_vector</span><span class="p">,</span> <span class="n">nwords</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">feature_vector</span>


<span class="k">def</span> <span class="nf">averaged_word_vectorizer</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
    <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">average_word_vectors</span><span class="p">(</span><span class="n">tokenized_sentence</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span>
                             <span class="n">num_features</span><span class="p">)</span> <span class="k">for</span> <span class="n">tokenized_sentence</span> <span class="ow">in</span> <span class="n">corpus</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w2v_feature_array</span> <span class="o">=</span> <span class="n">averaged_word_vectorizer</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">corpus_tokens</span><span class="p">,</span>
                                             <span class="n">model</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span>
                                             <span class="n">num_features</span><span class="o">=</span><span class="n">feature_size</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">w2v_feature_array</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">corpus_norm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>the sky is blue and beautiful</th>
      <td>-1.025673</td>
      <td>0.385626</td>
      <td>-1.049794</td>
      <td>0.510430</td>
      <td>-0.660862</td>
      <td>-0.868236</td>
      <td>-0.548901</td>
      <td>0.089492</td>
      <td>0.567460</td>
      <td>-0.425037</td>
    </tr>
    <tr>
      <th>love this blue and beautiful sky</th>
      <td>-1.204774</td>
      <td>0.458085</td>
      <td>-0.833338</td>
      <td>1.104553</td>
      <td>-0.919012</td>
      <td>-0.179183</td>
      <td>-0.639440</td>
      <td>0.066505</td>
      <td>0.541388</td>
      <td>-0.689463</td>
    </tr>
    <tr>
      <th>the quick brown fox jumps over the lazy dog</th>
      <td>-0.392979</td>
      <td>-0.568204</td>
      <td>-1.745686</td>
      <td>-0.445058</td>
      <td>0.459197</td>
      <td>-1.097641</td>
      <td>-0.019236</td>
      <td>-0.410119</td>
      <td>0.395439</td>
      <td>-1.143568</td>
    </tr>
    <tr>
      <th>a kings breakfast has sausages ham bacon eggs toast and beans</th>
      <td>0.403289</td>
      <td>0.133352</td>
      <td>0.090500</td>
      <td>-0.387340</td>
      <td>-0.589835</td>
      <td>0.508636</td>
      <td>-1.155621</td>
      <td>-0.795676</td>
      <td>1.076691</td>
      <td>-1.408591</td>
    </tr>
    <tr>
      <th>i love green eggs ham sausages and bacon</th>
      <td>-0.260290</td>
      <td>-0.112252</td>
      <td>-0.215950</td>
      <td>0.544712</td>
      <td>-0.439302</td>
      <td>1.108033</td>
      <td>-1.191799</td>
      <td>-0.361297</td>
      <td>0.810694</td>
      <td>-1.341653</td>
    </tr>
    <tr>
      <th>the brown fox is quick and the blue dog is lazy</th>
      <td>-0.655273</td>
      <td>-0.112480</td>
      <td>-1.412061</td>
      <td>-0.171668</td>
      <td>0.275225</td>
      <td>-1.128703</td>
      <td>-0.291054</td>
      <td>-0.052113</td>
      <td>0.424552</td>
      <td>-0.763964</td>
    </tr>
    <tr>
      <th>the sky is very blue and the sky is very beautiful today</th>
      <td>-1.168732</td>
      <td>0.556899</td>
      <td>-0.959907</td>
      <td>0.521333</td>
      <td>-0.719075</td>
      <td>-1.211951</td>
      <td>-0.400086</td>
      <td>0.057257</td>
      <td>0.723485</td>
      <td>-0.610505</td>
    </tr>
    <tr>
      <th>the dog is lazy but the brown fox is quick</th>
      <td>-0.651009</td>
      <td>-0.233851</td>
      <td>-1.622870</td>
      <td>-0.251843</td>
      <td>0.412877</td>
      <td>-1.314846</td>
      <td>-0.215856</td>
      <td>-0.195488</td>
      <td>0.437747</td>
      <td>-0.867432</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Let’s cluster these documents based on their <strong>document embeddings</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">similarity_doc_matrix</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">w2v_feature_array</span><span class="p">)</span>
<span class="n">similarity_doc_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarity_doc_matrix</span><span class="p">)</span>
<span class="n">similarity_doc_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.000000</td>
      <td>0.901241</td>
      <td>0.561713</td>
      <td>0.202711</td>
      <td>0.330573</td>
      <td>0.767413</td>
      <td>0.983230</td>
      <td>0.708357</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.901241</td>
      <td>1.000000</td>
      <td>0.313854</td>
      <td>0.301785</td>
      <td>0.583546</td>
      <td>0.509708</td>
      <td>0.864261</td>
      <td>0.437844</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.561713</td>
      <td>0.313854</td>
      <td>1.000000</td>
      <td>0.208633</td>
      <td>0.144338</td>
      <td>0.942269</td>
      <td>0.561763</td>
      <td>0.966901</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.202711</td>
      <td>0.301785</td>
      <td>0.208633</td>
      <td>1.000000</td>
      <td>0.822919</td>
      <td>0.156876</td>
      <td>0.196729</td>
      <td>0.138610</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.330573</td>
      <td>0.583546</td>
      <td>0.144338</td>
      <td>0.822919</td>
      <td>1.000000</td>
      <td>0.145031</td>
      <td>0.266458</td>
      <td>0.103276</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.767413</td>
      <td>0.509708</td>
      <td>0.942269</td>
      <td>0.156876</td>
      <td>0.145031</td>
      <td>1.000000</td>
      <td>0.766367</td>
      <td>0.994382</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.983230</td>
      <td>0.864261</td>
      <td>0.561763</td>
      <td>0.196729</td>
      <td>0.266458</td>
      <td>0.766367</td>
      <td>1.000000</td>
      <td>0.711530</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.708357</td>
      <td>0.437844</td>
      <td>0.966901</td>
      <td>0.138610</td>
      <td>0.103276</td>
      <td>0.994382</td>
      <td>0.711530</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">similarity_doc_matrix</span><span class="p">,</span> <span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Hierarchical Clustering Dendrogram&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Data point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">)</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span>
           <span class="n">labels</span><span class="o">=</span><span class="n">corpus_norm</span><span class="p">,</span>
           <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
           <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
           <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span>
           <span class="n">color_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x15b363790&gt;
</pre></div>
</div>
<img alt="../_images/1ccc03bd46736103a58b06183180ca3ca8a7828a7b439463407d1de796e01402.png" src="../_images/1ccc03bd46736103a58b06183180ca3ca8a7828a7b439463407d1de796e01402.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Other Clustering Methods</span>

<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AffinityPropagation</span>

<span class="n">ap</span> <span class="o">=</span> <span class="n">AffinityPropagation</span><span class="p">()</span>
<span class="n">ap</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">w2v_feature_array</span><span class="p">)</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">ap</span><span class="o">.</span><span class="n">labels_</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cluster_labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ClusterLabel&#39;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">corpus_df</span><span class="p">,</span> <span class="n">cluster_labels</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">## PCA Plotting</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pcs</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">w2v_feature_array</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">ap</span><span class="o">.</span><span class="n">labels_</span>
<span class="n">categories</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">corpus_df</span><span class="p">[</span><span class="s1">&#39;Category&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;green&#39;</span>
    <span class="n">annotation_label</span> <span class="o">=</span> <span class="n">categories</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">pcs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">annotation_label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">1e-3</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f7439067b9d02ba49d719f8b171f8147f9af9eb9bcb93133bde4926db8132b53.png" src="../_images/f7439067b9d02ba49d719f8b171f8147f9af9eb9bcb93133bde4926db8132b53.png" />
</div>
</div>
</section>
</section>
<section id="using-pre-trained-embeddings-glove-in-spacy">
<h2>Using Pre-trained Embeddings:  GloVe in <code class="docutils literal notranslate"><span class="pre">spacy</span></code><a class="headerlink" href="#using-pre-trained-embeddings-glove-in-spacy" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>


<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">,</span><span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parse&#39;</span><span class="p">,</span><span class="s1">&#39;entity&#39;</span><span class="p">])</span>

<span class="n">total_vectors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total word vectors:&#39;</span><span class="p">,</span> <span class="n">total_vectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total word vectors: 0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">spacy</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.5.4
</pre></div>
</div>
</div>
</div>
<section id="visualize-glove-word-embeddings">
<h3>Visualize GloVe word embeddings<a class="headerlink" href="#visualize-glove-word-embeddings" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Let’s extract the GloVe pretrained embeddings for all the words in our simple corpus.</p></li>
<li><p>And we visualize their embeddings in a 2D plot via dimensional reduction.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using pre-trained embeddings, there are two important things:</p>
<ul class="simple">
<li><p>Be very careful of the <strong>tokenization</strong> methods used in your text preprocessing. If you use a very different word tokenization method, you may find a lot of <strong>unknown</strong> words that are not included in the pretrained model.</p></li>
<li><p>Always check the <strong>proportion of the unknown words</strong> when vectorizing your corpus texts with pre-trained embeddings.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get vocab of the corpus</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">corpus_tokens</span><span class="p">,[]))</span>

<span class="c1"># extract pre-trained embeddings of all words</span>
<span class="n">word_glove_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">nlp</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">unique_words</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">word_glove_vectors</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">unique_words</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>86</th>
      <th>87</th>
      <th>88</th>
      <th>89</th>
      <th>90</th>
      <th>91</th>
      <th>92</th>
      <th>93</th>
      <th>94</th>
      <th>95</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>eggs</th>
      <td>-1.297392</td>
      <td>1.158260</td>
      <td>-0.816274</td>
      <td>1.056100</td>
      <td>0.815064</td>
      <td>-0.748527</td>
      <td>1.410166</td>
      <td>0.772055</td>
      <td>0.981865</td>
      <td>-0.946172</td>
      <td>...</td>
      <td>-0.929507</td>
      <td>0.640614</td>
      <td>-0.525295</td>
      <td>0.420553</td>
      <td>-1.224993</td>
      <td>-0.113389</td>
      <td>1.944303</td>
      <td>0.202356</td>
      <td>0.006850</td>
      <td>0.784401</td>
    </tr>
    <tr>
      <th>beautiful</th>
      <td>-1.981197</td>
      <td>-1.085511</td>
      <td>-0.495878</td>
      <td>0.908991</td>
      <td>-0.765072</td>
      <td>0.368210</td>
      <td>0.533828</td>
      <td>0.598339</td>
      <td>0.157835</td>
      <td>-1.119861</td>
      <td>...</td>
      <td>0.119182</td>
      <td>0.332911</td>
      <td>-1.044088</td>
      <td>0.527054</td>
      <td>0.264596</td>
      <td>0.141068</td>
      <td>1.396086</td>
      <td>0.909145</td>
      <td>-0.375545</td>
      <td>0.587217</td>
    </tr>
    <tr>
      <th>a</th>
      <td>0.445624</td>
      <td>0.040820</td>
      <td>-1.044201</td>
      <td>0.977903</td>
      <td>0.343366</td>
      <td>0.434702</td>
      <td>-0.532973</td>
      <td>0.914784</td>
      <td>0.640789</td>
      <td>1.057252</td>
      <td>...</td>
      <td>0.890635</td>
      <td>-0.494442</td>
      <td>-0.669522</td>
      <td>0.849890</td>
      <td>-0.475511</td>
      <td>-0.044901</td>
      <td>-1.019666</td>
      <td>-0.434723</td>
      <td>-0.855158</td>
      <td>-0.277925</td>
    </tr>
    <tr>
      <th>sausages</th>
      <td>-1.551236</td>
      <td>1.381497</td>
      <td>-0.690470</td>
      <td>0.578089</td>
      <td>1.516763</td>
      <td>-0.319808</td>
      <td>1.896638</td>
      <td>0.390757</td>
      <td>0.864699</td>
      <td>-0.613225</td>
      <td>...</td>
      <td>-0.803906</td>
      <td>-0.313376</td>
      <td>-0.481047</td>
      <td>-0.324135</td>
      <td>-1.256292</td>
      <td>0.100420</td>
      <td>1.357908</td>
      <td>0.602705</td>
      <td>0.325189</td>
      <td>0.501014</td>
    </tr>
    <tr>
      <th>sky</th>
      <td>-1.052380</td>
      <td>-0.623966</td>
      <td>-1.247646</td>
      <td>0.173334</td>
      <td>-0.280415</td>
      <td>0.423770</td>
      <td>1.136191</td>
      <td>1.594746</td>
      <td>-0.112143</td>
      <td>0.024372</td>
      <td>...</td>
      <td>-0.081687</td>
      <td>-0.557614</td>
      <td>-1.442796</td>
      <td>1.067169</td>
      <td>-0.304729</td>
      <td>-0.230831</td>
      <td>1.200994</td>
      <td>0.827422</td>
      <td>0.558651</td>
      <td>1.421518</td>
    </tr>
    <tr>
      <th>jumps</th>
      <td>-0.809289</td>
      <td>1.028575</td>
      <td>0.142154</td>
      <td>1.195198</td>
      <td>-0.892427</td>
      <td>-0.613777</td>
      <td>1.158633</td>
      <td>0.491403</td>
      <td>0.826156</td>
      <td>-0.908024</td>
      <td>...</td>
      <td>-0.968901</td>
      <td>0.572921</td>
      <td>0.224079</td>
      <td>-0.167506</td>
      <td>0.545909</td>
      <td>0.819458</td>
      <td>-0.053826</td>
      <td>0.179755</td>
      <td>-0.520523</td>
      <td>1.177554</td>
    </tr>
    <tr>
      <th>bacon</th>
      <td>-1.484836</td>
      <td>-0.526327</td>
      <td>-0.839054</td>
      <td>-0.176428</td>
      <td>0.484847</td>
      <td>0.140633</td>
      <td>0.748910</td>
      <td>1.224433</td>
      <td>0.124089</td>
      <td>0.034701</td>
      <td>...</td>
      <td>0.007322</td>
      <td>-0.371588</td>
      <td>-1.953568</td>
      <td>1.160531</td>
      <td>-0.463557</td>
      <td>-0.028599</td>
      <td>1.411305</td>
      <td>0.775448</td>
      <td>0.868626</td>
      <td>1.153851</td>
    </tr>
    <tr>
      <th>i</th>
      <td>-1.993811</td>
      <td>-0.266061</td>
      <td>-0.210171</td>
      <td>-0.306936</td>
      <td>-0.040057</td>
      <td>-0.104719</td>
      <td>2.012425</td>
      <td>0.467140</td>
      <td>0.367587</td>
      <td>-0.688097</td>
      <td>...</td>
      <td>-0.367412</td>
      <td>-0.188884</td>
      <td>0.805614</td>
      <td>0.037096</td>
      <td>1.218038</td>
      <td>-0.012631</td>
      <td>0.766386</td>
      <td>-0.338247</td>
      <td>-0.670582</td>
      <td>1.593813</td>
    </tr>
    <tr>
      <th>quick</th>
      <td>-0.808211</td>
      <td>-1.721902</td>
      <td>-0.849621</td>
      <td>-0.323724</td>
      <td>-0.733217</td>
      <td>-0.200044</td>
      <td>0.316565</td>
      <td>1.306867</td>
      <td>-0.375950</td>
      <td>-0.764119</td>
      <td>...</td>
      <td>-0.344666</td>
      <td>0.631241</td>
      <td>-0.250259</td>
      <td>1.216615</td>
      <td>1.283587</td>
      <td>0.036267</td>
      <td>1.192510</td>
      <td>1.531458</td>
      <td>-1.331324</td>
      <td>0.637828</td>
    </tr>
    <tr>
      <th>dog</th>
      <td>-1.680668</td>
      <td>-1.266374</td>
      <td>-0.712555</td>
      <td>0.221439</td>
      <td>0.285816</td>
      <td>0.239243</td>
      <td>1.299264</td>
      <td>1.068364</td>
      <td>-0.166650</td>
      <td>-0.593021</td>
      <td>...</td>
      <td>0.311112</td>
      <td>-0.469987</td>
      <td>-2.129951</td>
      <td>1.172860</td>
      <td>-1.284266</td>
      <td>-1.117903</td>
      <td>1.596463</td>
      <td>0.511931</td>
      <td>0.220327</td>
      <td>1.620028</td>
    </tr>
    <tr>
      <th>beans</th>
      <td>-1.068852</td>
      <td>1.167118</td>
      <td>-0.484017</td>
      <td>0.891425</td>
      <td>1.399791</td>
      <td>-0.477653</td>
      <td>1.326169</td>
      <td>0.747473</td>
      <td>1.092705</td>
      <td>-0.092148</td>
      <td>...</td>
      <td>-1.138134</td>
      <td>-0.107772</td>
      <td>0.300590</td>
      <td>0.501320</td>
      <td>-0.806155</td>
      <td>0.883533</td>
      <td>1.994725</td>
      <td>0.569794</td>
      <td>0.480497</td>
      <td>0.148873</td>
    </tr>
    <tr>
      <th>lazy</th>
      <td>-1.349924</td>
      <td>-1.008392</td>
      <td>-0.377253</td>
      <td>-0.534368</td>
      <td>-0.737286</td>
      <td>0.438626</td>
      <td>0.275777</td>
      <td>0.426540</td>
      <td>-0.384914</td>
      <td>-1.091949</td>
      <td>...</td>
      <td>-0.174180</td>
      <td>0.558309</td>
      <td>-0.641799</td>
      <td>1.153933</td>
      <td>1.125221</td>
      <td>0.256873</td>
      <td>0.825007</td>
      <td>1.489886</td>
      <td>0.163207</td>
      <td>0.548744</td>
    </tr>
    <tr>
      <th>has</th>
      <td>-0.816400</td>
      <td>-0.586754</td>
      <td>-1.149105</td>
      <td>0.388758</td>
      <td>0.561820</td>
      <td>1.324211</td>
      <td>0.726742</td>
      <td>0.121942</td>
      <td>0.886533</td>
      <td>1.260011</td>
      <td>...</td>
      <td>-1.520276</td>
      <td>0.172569</td>
      <td>1.202618</td>
      <td>-0.608210</td>
      <td>-0.100047</td>
      <td>-0.234273</td>
      <td>0.720248</td>
      <td>0.204601</td>
      <td>-1.233299</td>
      <td>-0.132075</td>
    </tr>
    <tr>
      <th>ham</th>
      <td>-0.952584</td>
      <td>-0.859383</td>
      <td>-0.512346</td>
      <td>-0.397440</td>
      <td>-0.272660</td>
      <td>0.141939</td>
      <td>-0.206824</td>
      <td>1.338140</td>
      <td>0.240586</td>
      <td>-0.777376</td>
      <td>...</td>
      <td>0.466846</td>
      <td>-0.185779</td>
      <td>-1.645292</td>
      <td>0.239415</td>
      <td>-0.960014</td>
      <td>0.846002</td>
      <td>0.562277</td>
      <td>0.380782</td>
      <td>0.955904</td>
      <td>0.844923</td>
    </tr>
    <tr>
      <th>today</th>
      <td>-0.818719</td>
      <td>-1.463490</td>
      <td>-0.576256</td>
      <td>-1.777408</td>
      <td>-0.816109</td>
      <td>-1.116335</td>
      <td>-0.125725</td>
      <td>2.077141</td>
      <td>-0.168442</td>
      <td>-0.217695</td>
      <td>...</td>
      <td>2.928388</td>
      <td>1.940016</td>
      <td>-0.906605</td>
      <td>1.027404</td>
      <td>-1.825106</td>
      <td>0.496492</td>
      <td>0.637674</td>
      <td>0.839445</td>
      <td>0.745934</td>
      <td>-0.141063</td>
    </tr>
    <tr>
      <th>toast</th>
      <td>-0.511126</td>
      <td>-1.450968</td>
      <td>-0.677772</td>
      <td>-0.366755</td>
      <td>-0.637132</td>
      <td>-0.223210</td>
      <td>-0.398685</td>
      <td>1.043328</td>
      <td>0.483847</td>
      <td>-0.570177</td>
      <td>...</td>
      <td>0.803359</td>
      <td>0.164432</td>
      <td>-1.173489</td>
      <td>0.853854</td>
      <td>-1.316678</td>
      <td>0.347120</td>
      <td>0.963565</td>
      <td>1.096257</td>
      <td>0.741654</td>
      <td>0.422103</td>
    </tr>
    <tr>
      <th>very</th>
      <td>0.179364</td>
      <td>-0.142232</td>
      <td>-1.542073</td>
      <td>-0.390681</td>
      <td>0.221738</td>
      <td>-0.352646</td>
      <td>1.095366</td>
      <td>0.634386</td>
      <td>0.249361</td>
      <td>-0.077876</td>
      <td>...</td>
      <td>0.737402</td>
      <td>-0.546005</td>
      <td>-0.673379</td>
      <td>0.705251</td>
      <td>1.651500</td>
      <td>-0.794369</td>
      <td>0.578590</td>
      <td>1.237230</td>
      <td>-1.189028</td>
      <td>-1.221863</td>
    </tr>
    <tr>
      <th>and</th>
      <td>-0.749836</td>
      <td>-0.864052</td>
      <td>-1.123196</td>
      <td>0.288821</td>
      <td>-0.685820</td>
      <td>2.549688</td>
      <td>-0.118120</td>
      <td>0.279010</td>
      <td>-0.123277</td>
      <td>1.001370</td>
      <td>...</td>
      <td>0.707131</td>
      <td>1.457647</td>
      <td>-0.714177</td>
      <td>-0.498196</td>
      <td>-0.281577</td>
      <td>0.980062</td>
      <td>-0.475814</td>
      <td>0.466685</td>
      <td>-0.855074</td>
      <td>-0.479473</td>
    </tr>
    <tr>
      <th>the</th>
      <td>0.171047</td>
      <td>-0.199269</td>
      <td>-0.745276</td>
      <td>0.178762</td>
      <td>-0.363477</td>
      <td>0.347444</td>
      <td>-1.003620</td>
      <td>0.021227</td>
      <td>0.238515</td>
      <td>1.270073</td>
      <td>...</td>
      <td>1.609505</td>
      <td>-0.592565</td>
      <td>-0.546699</td>
      <td>1.053596</td>
      <td>-0.962386</td>
      <td>0.352104</td>
      <td>-0.288018</td>
      <td>-0.706722</td>
      <td>-0.514224</td>
      <td>-0.020832</td>
    </tr>
    <tr>
      <th>green</th>
      <td>-1.185332</td>
      <td>-1.172187</td>
      <td>-0.342883</td>
      <td>0.147804</td>
      <td>-0.441741</td>
      <td>-0.421923</td>
      <td>-0.664020</td>
      <td>0.015956</td>
      <td>0.544560</td>
      <td>-1.225485</td>
      <td>...</td>
      <td>0.885219</td>
      <td>1.026413</td>
      <td>-0.446825</td>
      <td>1.726705</td>
      <td>0.148405</td>
      <td>0.407081</td>
      <td>0.922681</td>
      <td>1.281653</td>
      <td>-1.018726</td>
      <td>0.706706</td>
    </tr>
    <tr>
      <th>brown</th>
      <td>-0.876515</td>
      <td>-1.404033</td>
      <td>-0.920570</td>
      <td>0.893428</td>
      <td>0.632113</td>
      <td>-0.000901</td>
      <td>-0.270705</td>
      <td>1.450482</td>
      <td>1.094554</td>
      <td>-1.343019</td>
      <td>...</td>
      <td>0.900336</td>
      <td>0.143640</td>
      <td>-0.650320</td>
      <td>0.453211</td>
      <td>-0.052952</td>
      <td>0.508944</td>
      <td>1.487840</td>
      <td>0.490526</td>
      <td>0.128303</td>
      <td>1.059052</td>
    </tr>
    <tr>
      <th>blue</th>
      <td>-1.241533</td>
      <td>-1.403888</td>
      <td>-0.906116</td>
      <td>0.713846</td>
      <td>0.730656</td>
      <td>-0.184642</td>
      <td>-0.275212</td>
      <td>1.047278</td>
      <td>0.211378</td>
      <td>-0.573579</td>
      <td>...</td>
      <td>0.483817</td>
      <td>1.536532</td>
      <td>-0.535280</td>
      <td>0.327238</td>
      <td>-0.723836</td>
      <td>0.332048</td>
      <td>0.652643</td>
      <td>1.130325</td>
      <td>-0.458007</td>
      <td>0.830703</td>
    </tr>
    <tr>
      <th>fox</th>
      <td>-1.063213</td>
      <td>-1.183251</td>
      <td>0.301696</td>
      <td>0.807676</td>
      <td>-0.038768</td>
      <td>0.154243</td>
      <td>0.943619</td>
      <td>1.364023</td>
      <td>0.090330</td>
      <td>-0.779429</td>
      <td>...</td>
      <td>-0.005095</td>
      <td>-0.514685</td>
      <td>-1.359398</td>
      <td>1.482487</td>
      <td>-0.727252</td>
      <td>-0.679084</td>
      <td>1.143468</td>
      <td>0.792078</td>
      <td>0.315267</td>
      <td>1.307671</td>
    </tr>
    <tr>
      <th>is</th>
      <td>-0.686803</td>
      <td>1.201411</td>
      <td>-0.365061</td>
      <td>0.023410</td>
      <td>-0.911158</td>
      <td>-0.636755</td>
      <td>0.201880</td>
      <td>0.615606</td>
      <td>1.178941</td>
      <td>0.296924</td>
      <td>...</td>
      <td>-0.182100</td>
      <td>0.782834</td>
      <td>-0.252753</td>
      <td>-0.966379</td>
      <td>0.038495</td>
      <td>-0.473968</td>
      <td>-0.116784</td>
      <td>-0.699781</td>
      <td>-0.844511</td>
      <td>0.400906</td>
    </tr>
    <tr>
      <th>but</th>
      <td>-0.617007</td>
      <td>-1.335470</td>
      <td>-1.274921</td>
      <td>0.456699</td>
      <td>0.492685</td>
      <td>0.975719</td>
      <td>-0.089738</td>
      <td>0.473519</td>
      <td>0.433252</td>
      <td>1.396277</td>
      <td>...</td>
      <td>0.546477</td>
      <td>0.888823</td>
      <td>-0.181895</td>
      <td>-0.218617</td>
      <td>-0.720275</td>
      <td>0.372221</td>
      <td>-0.464548</td>
      <td>0.677568</td>
      <td>-1.169354</td>
      <td>-0.445014</td>
    </tr>
    <tr>
      <th>this</th>
      <td>-0.923892</td>
      <td>-0.346465</td>
      <td>-0.935396</td>
      <td>0.642046</td>
      <td>0.873360</td>
      <td>0.107631</td>
      <td>1.653424</td>
      <td>0.126784</td>
      <td>-0.441882</td>
      <td>0.085917</td>
      <td>...</td>
      <td>1.577924</td>
      <td>0.150236</td>
      <td>0.281095</td>
      <td>-0.267720</td>
      <td>-0.851461</td>
      <td>-0.777931</td>
      <td>0.251027</td>
      <td>-0.263745</td>
      <td>-0.962156</td>
      <td>0.877665</td>
    </tr>
    <tr>
      <th>over</th>
      <td>-0.440351</td>
      <td>-0.957340</td>
      <td>-0.288660</td>
      <td>-0.784095</td>
      <td>-0.453102</td>
      <td>-0.260627</td>
      <td>-0.156899</td>
      <td>1.161451</td>
      <td>0.216041</td>
      <td>-0.119296</td>
      <td>...</td>
      <td>0.818817</td>
      <td>-0.112223</td>
      <td>0.002978</td>
      <td>0.198639</td>
      <td>-0.664964</td>
      <td>1.044688</td>
      <td>-0.332886</td>
      <td>1.113894</td>
      <td>-0.737988</td>
      <td>-1.248392</td>
    </tr>
    <tr>
      <th>kings</th>
      <td>-0.871058</td>
      <td>1.877413</td>
      <td>-0.446523</td>
      <td>0.939120</td>
      <td>1.882271</td>
      <td>-0.894365</td>
      <td>1.959913</td>
      <td>0.237614</td>
      <td>1.054103</td>
      <td>-0.776251</td>
      <td>...</td>
      <td>-0.663307</td>
      <td>0.486514</td>
      <td>0.542900</td>
      <td>-0.114815</td>
      <td>-0.053794</td>
      <td>0.501748</td>
      <td>1.782944</td>
      <td>0.394344</td>
      <td>-0.109745</td>
      <td>0.131947</td>
    </tr>
    <tr>
      <th>love</th>
      <td>-2.062335</td>
      <td>-0.436482</td>
      <td>-0.819275</td>
      <td>0.988686</td>
      <td>0.089922</td>
      <td>-0.325258</td>
      <td>1.581294</td>
      <td>1.765489</td>
      <td>0.431824</td>
      <td>-0.991082</td>
      <td>...</td>
      <td>-0.014793</td>
      <td>-0.220141</td>
      <td>-1.260573</td>
      <td>-0.073471</td>
      <td>-0.762526</td>
      <td>-0.953943</td>
      <td>0.655179</td>
      <td>1.407103</td>
      <td>0.392512</td>
      <td>0.337918</td>
    </tr>
    <tr>
      <th>breakfast</th>
      <td>-0.829254</td>
      <td>-0.406226</td>
      <td>-0.967018</td>
      <td>0.675861</td>
      <td>-0.075334</td>
      <td>0.707547</td>
      <td>0.685737</td>
      <td>2.209723</td>
      <td>0.268043</td>
      <td>-0.333843</td>
      <td>...</td>
      <td>-0.162341</td>
      <td>-1.037519</td>
      <td>-1.418477</td>
      <td>0.797511</td>
      <td>-0.989848</td>
      <td>-0.342162</td>
      <td>1.093951</td>
      <td>0.917619</td>
      <td>0.077850</td>
      <td>0.654152</td>
    </tr>
  </tbody>
</table>
<p>30 rows × 96 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">word_glove_vectors</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">unique_words</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/44e804ef2a87591367c2e599bcb2c283b9d7d053574dfb285be85ac894a390ab.png" src="../_images/44e804ef2a87591367c2e599bcb2c283b9d7d053574dfb285be85ac894a390ab.png" />
</div>
</div>
<ul class="simple">
<li><p>It is clear to see that when embeddings are trained based on a larger corpus, they reflect more lexical semantic contents.</p></li>
<li><p>Semantically similar words are indeed closer to each other in the 2D plot.</p></li>
</ul>
<ul class="simple">
<li><p>We can of course perform the document-level clustering again using the GloVe embeddings.</p></li>
<li><p>The good thing about <code class="docutils literal notranslate"><span class="pre">spacy</span></code> is that it can compute the document average embeddings automatically.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_glove_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">nlp</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus_norm</span><span class="p">])</span>

<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">km</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">doc_glove_vectors</span><span class="p">)</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">km</span><span class="o">.</span><span class="n">labels_</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cluster_labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ClusterLabel&#39;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">corpus_df</span><span class="p">,</span> <span class="n">cluster_labels</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/alvinchen/anaconda3/envs/python-notes/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to &#39;auto&#39; in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Document</th>
      <th>Category</th>
      <th>ClusterLabel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The sky is blue and beautiful.</td>
      <td>weather</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Love this blue and beautiful sky!</td>
      <td>weather</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>The quick brown fox jumps over the lazy dog.</td>
      <td>animals</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>
      <td>food</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I love green eggs, ham, sausages and bacon!</td>
      <td>food</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The brown fox is quick and the blue dog is lazy!</td>
      <td>animals</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The sky is very blue and the sky is very beautiful today</td>
      <td>weather</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The dog is lazy but the brown fox is quick!</td>
      <td>animals</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="fasttext">
<h2><code class="docutils literal notranslate"><span class="pre">fasttext</span></code><a class="headerlink" href="#fasttext" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>This section shows a quick example how to train word embeddings based on the <code class="docutils literal notranslate"><span class="pre">nltk.corpus.brown</span></code> using another algorithm, i.e., <code class="docutils literal notranslate"><span class="pre">fasttext</span></code>.</p></li>
<li><p>The FastText model was introduced by Facebook in 2016 as an improved and extended version of the <code class="docutils literal notranslate"><span class="pre">word2vec</span></code> (See Bojanowski et al [2017] in References below).</p></li>
<li><p>We will focus more on the implementation. Please see the Bojanowski et al (2017) as well as Sarkar (2019) Chapter 4 for more comprehensive descriptions of the method.</p></li>
<li><p>Pretrained FastText Embeddings are available <a class="reference external" href="https://fasttext.cc/docs/en/english-vectors.html">here</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models.fasttext</span> <span class="kn">import</span> <span class="n">FastText</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>

<span class="n">brown_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">brown</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">fileids</span><span class="o">=</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">brown</span><span class="o">.</span><span class="n">fileids</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Set values for various parameters</span>
<span class="n">feature_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># Word vector dimensionality</span>
<span class="n">window_context</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Context window size</span>
<span class="n">min_word_count</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Minimum word count</span>

<span class="n">ft_model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">brown_tokens</span><span class="p">,</span>
                    <span class="n">vector_size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span>
                    <span class="n">window</span><span class="o">=</span><span class="n">window_context</span><span class="p">,</span>
                    <span class="n">min_count</span><span class="o">=</span><span class="n">min_word_count</span><span class="p">,</span>
                    <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 5min 34s, sys: 5.79 s, total: 5min 39s
Wall time: 2min 2s
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can use the trained embeddings model to identify words that are similar to a set of seed words.</p></li>
<li><p>And then we plot all these words (i.e., the seed words and their semantic neighbors) in one 2D plot based on the dimensional reduction of their embeddings.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># view similar words based on gensim&#39;s model</span>
<span class="n">similar_words</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">search_term</span><span class="p">:</span>
    <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="n">search_term</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">search_term</span> <span class="ow">in</span>
    <span class="p">[</span><span class="s1">&#39;think&#39;</span><span class="p">,</span> <span class="s1">&#39;say&#39;</span><span class="p">,</span><span class="s1">&#39;news&#39;</span><span class="p">,</span> <span class="s1">&#39;report&#39;</span><span class="p">,</span><span class="s1">&#39;nation&#39;</span><span class="p">,</span> <span class="s1">&#39;democracy&#39;</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">similar_words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;think&#39;: [&#39;know&#39;, &#39;Think&#39;, &#39;tell&#39;, &quot;don&#39;t&quot;, &#39;feel&#39;],
 &#39;say&#39;: [&#39;said&#39;, &#39;think&#39;, &#39;do&#39;, &#39;believe&#39;, &#39;despise&#39;],
 &#39;news&#39;: [&#39;newspapers&#39;,
  &#39;newspaperman&#39;,
  &#39;newspaper&#39;,
  &#39;forthcoming&#39;,
  &#39;inspector&#39;],
 &#39;report&#39;: [&#39;reports&#39;, &#39;reporting&#39;, &#39;Report&#39;, &#39;reporter&#39;, &#39;reported&#39;],
 &#39;nation&#39;: [&#39;nations&#39;, &#39;national&#39;, &#39;nationalism&#39;, &quot;nation&#39;s&quot;, &#39;nationally&#39;],
 &#39;democracy&#39;: [&#39;democratic&#39;,
  &#39;cultural&#39;,
  &#39;constitutional&#39;,
  &#39;constitution&#39;,
  &#39;bureaucracy&#39;]}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">words</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">similar_words</span><span class="o">.</span><span class="n">items</span><span class="p">()],</span> <span class="p">[])</span>
<span class="n">wvs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">words</span><span class="p">]</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wvs</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">words</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">P</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">P</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;lightgreen&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">P</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">P</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.03</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f6fbb71d442d73f3e5cc2012eb41a4de0a0b1acd83284a69fdd53d99f527d434.png" src="../_images/f6fbb71d442d73f3e5cc2012eb41a4de0a0b1acd83284a69fdd53d99f527d434.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;democracy&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.2523766 ,  1.2344824 ,  0.28590804, -0.18874285, -0.12484555,
       -0.49433994,  0.1602066 , -0.31401044,  0.22249952, -0.16373059,
        0.5416111 ,  0.17930526,  0.33985358,  0.33994997, -0.27761728,
        0.295529  ,  0.30032286, -0.16337612,  0.2726979 ,  0.286383  ,
        0.05036034, -0.4099072 ,  1.1245147 , -0.43144313, -0.12164368,
       -0.43757993,  0.3907696 , -0.506607  ,  0.8151896 , -0.1673889 ,
       -0.12556757,  0.4687364 ,  0.38069835,  0.29146397,  0.17975552,
        0.00345719,  0.3428171 , -0.5179925 ,  0.25784668, -0.2876231 ,
        0.2736586 , -0.32872424, -0.41551793, -0.39329693,  0.16495731,
        0.22620523,  0.22976732, -0.30155176,  0.12755516, -0.05715791,
       -0.47991446,  0.6938182 , -0.69235647, -0.27857274,  0.20222324,
       -0.5334721 ,  0.5623266 ,  0.2074901 , -0.0281765 , -0.15348567,
        1.3333222 ,  0.5020892 , -0.2768525 ,  0.11414671,  0.194682  ,
       -0.33986622,  0.47148588, -0.3239075 , -0.31997842,  0.41093224,
        0.511632  , -0.0720844 , -0.13437022, -0.58273226,  0.36699748,
       -0.10058013, -0.41690606, -0.12419936,  0.3415619 ,  0.05605935,
       -0.31064925, -0.09939189, -0.20340547,  0.7773749 ,  0.3970421 ,
        0.00630904, -0.29208088,  0.06216234, -0.01027498,  0.10407776,
        0.63741744,  0.05946476,  0.23245631,  0.09461077,  0.44289067,
        0.40805987, -0.08794071,  0.02890253,  0.3359511 , -0.28606507,
       -0.7213146 ,  0.6503062 ,  0.36324272,  0.3368714 , -0.01722197,
        0.25369242,  0.44865412, -0.03442893, -0.644235  ,  0.28318068,
       -0.7213056 ,  0.02290851,  0.22658262,  0.03600961, -0.38349828,
        0.01788636, -0.01218169,  0.7510187 , -0.40153882, -0.21668746,
       -0.04060327,  0.5902806 , -0.05519979, -0.20750351,  0.3325783 ,
        0.11138923, -0.12826458, -0.91107947], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">w1</span><span class="o">=</span><span class="s1">&#39;taiwan&#39;</span><span class="p">,</span> <span class="n">w2</span><span class="o">=</span><span class="s1">&#39;freedom&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">w1</span><span class="o">=</span><span class="s1">&#39;china&#39;</span><span class="p">,</span> <span class="n">w2</span><span class="o">=</span><span class="s1">&#39;freedom&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.2915468
0.1772014
</pre></div>
</div>
</div>
</div>
</section>
<section id="wrap-up">
<h2>Wrap-up<a class="headerlink" href="#wrap-up" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Two fundamental deep-learning-based models of word representation learning: CBOW and Skip-Gram.</p></li>
<li><p>From word embeddings to document embeddings</p></li>
<li><p>More advanced representation learning models: GloVe and FastText.</p></li>
<li><p>What is more challenging is how to assess the quality of the learned representations (embeddings). Usually embedding models can be evaluated based on their performance on semantics related tasks, such as word similarity and analogy. For those who are interested, you can start with the following two papers on Chinese embeddings:</p>
<ul>
<li><p>Chi-Yen Chen, Wei-Yun Ma. 2018. “<a class="reference external" href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/159.pdf">Word Embedding Evaluation Datasets and Wikipedia Title Embedding for Chinese</a>,” Language Resources and Evaluation Conference.</p></li>
<li><p>Chi-Yen Chen, Wei-Yun Ma. 2017. “<a class="reference external" href="https://ieeexplore.ieee.org/document/8300566">Embedding Wikipedia Title Based on Its Wikipedia Text and Categories</a>,” International Conference on Asian Language Processing.</p></li>
</ul>
</li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Sarkar (2020) Ch 4 Feature Engineering for Text Representation</p></li>
<li><p>Major Readings:</p>
<ul>
<li><p>Harris,Zellig. 1956. <a class="reference external" href="http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520">Distributional structure</a>.</p></li>
<li><p>Bengio, Yoshuan, et. al. 2003. <a class="reference external" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>.</p></li>
<li><p>Collobert, Ronana and Jason Weston. 2008. <a class="reference external" href="https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</a>.</p></li>
<li><p>Schwenk, Holger. 2007.<a class="reference external" href="https://pdfs.semanticscholar.org/0fcc/184b3b90405ec3ceafd6a4007c749df7c363.pdf">Continuous space language models</a>.</p></li>
<li><p>Mikolov, Tomas, et al. 2013. <a class="reference external" href="https://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a>. arXiv preprint arXiv:1301.3781.</p></li>
<li><p>Mikolov, Tomas, et al. 2013. <a class="reference external" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed representations of words and phrases and their compositionally</a>. <em>Advances in neural information processing systems</em>. 2013.</p></li>
<li><p>Baroni, Marco, et. al. 2014. <a class="reference external" href="https://www.aclweb.org/anthology/P14-1023/">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a>. <em>ACL</em>(1).</p></li>
<li><p>Pennington, Jeffrey, et al. 2014. <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>. <em>EMNLP</em>. Vol. 14.</p></li>
<li><p>Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2017). <a class="reference external" href="https://doi.org/10.1162/tacl_a_00051">Enriching word vectors with subword information</a>. <em>Transactions of the Association for Computational Linguistics</em>, 5, 135-146.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe Project Official Website</a>: You can download their pre-trained GloVe models.</p></li>
<li><p><a class="reference external" href="https://fasttext.cc/docs/en/english-vectors.html">FastText Project Website</a>: You can download the English pre-trained FastText models.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="dl-neural-language-model-primer.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Neural Language Model: A Start</p>
      </div>
    </a>
    <a class="right-next"
       href="dl-attention-transformer-intuition.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Attention and Transformers: Intuitions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-word2vec">What is <code class="docutils literal notranslate"><span class="pre">word2vec</span></code>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-of-word-embeddings-distributional-semantics">Basis of Word Embeddings: Distributional Semantics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-training-algorithms-of-word2vec">Main training algorithms of <code class="docutils literal notranslate"><span class="pre">word2vec</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-intuitive-understanding-of-cbow">An Intuitive Understanding of CBOW</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-intuitive-understanding-of-skip-gram">An Intuitive Understanding of Skip-gram</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-necessary-dependencies-and-settings">Import necessary dependencies and settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-corpus-a-naive-example">Sample Corpus: A Naive Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-pre-processing">Simple text pre-processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#suggestions">Suggestions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-embeddings-using-word2vec">Training Embeddings Using word2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-word-embeddings">Visualizing Word Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-word-embeddings-to-document-embeddings">From Word Embeddings to Document Embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-embeddings-glove-in-spacy">Using Pre-trained Embeddings:  GloVe in <code class="docutils literal notranslate"><span class="pre">spacy</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-glove-word-embeddings">Visualize GloVe word embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fasttext"><code class="docutils literal notranslate"><span class="pre">fasttext</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrap-up">Wrap-up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>