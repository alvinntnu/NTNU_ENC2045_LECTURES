
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Classification Models &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Sentiment Analysis Using Bag-of-Words" href="ml-sklearn-classification.html" />
    <link rel="prev" title="2. Machine Learning: A Simple Example" href="ml-simple-case.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-neural-network-from-scratch.html">
   1. Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-simple-case.html">
   2. Deep Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sentiment-case.html">
   3. Deep Learning: Sentiment Analysis
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neural Language Model and Embeddings
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sequence-models-intuition.html">
   1. Sequence Models Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-neural-language-model-primer.html">
   2. Neural Language Model: A Start
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/text-vec-embedding.html">
   3. Word Embeddings
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Seq2Seq, Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-seq-to-seq-types.html">
   Attention: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-seq-to-seq-attention-addition.html">
   Sequence Model with Attention for Addition Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-transformers-intuition.html">
   Transformers: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-transformers-keras.html">
   Text Classification with Transformer
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   Assignment III: Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   Assignment IV: Chinese Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   Assignment V: Text Vectorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/6-machine-learning.html">
   Assignment VI: Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/midterm-exam.html">
   Midterm Exam
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/7-topic-modeling.html">
   Assignment VII: Topic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/8-dl-chinese-name-gender.html">
   Assignment VIII: Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/9-sentiment-analysis-dl.html">
   Assignment IX: Sentiment Analysis Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/10-neural-language-model.html">
   Assignment X: Neural Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/11-word2vec.html">
   Assignment XI: Word Embeddings
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nlp/ml-algorithm.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/nlp/ml-algorithm.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/ml-algorithm.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-bayes">
   3.1. Naive Bayes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition-of-naive-bayes">
     Intuition of Naive Bayes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   3.2. Logistic Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#support-vector-machine-svm">
   3.3. Support Vector Machine (SVM)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-discriminative-models-skipped">
   3.4. Other Discriminative Models (skipped)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximum-entropy-model-maxent">
     Maximum Entropy Model (Maxent)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-classification-models-skipped">
   3.5. Other Classification Models (skipped)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#emsemble-methods">
   3.6. Emsemble Methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   3.7. Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   3.8. References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="classification-models">
<h1><span class="section-number">3. </span>Classification Models<a class="headerlink" href="#classification-models" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Before the era of deep learning, probability-based classifiers are commonly used in many ML NLP tasks.</p></li>
<li><p>There are two types of probability-based models: <strong>generative</strong> and <strong>disriminative</strong> models.</p></li>
<li><p>Let’s assume that we have our data as <em><strong>d</strong></em>, and their class labels as <em><strong>c</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Generative</strong>:</p>
<ul>
<li><p>The goal of training is NOT to find <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Rather, based on the <strong>Bayes Theorem</strong>, we can estimate the joint probability of <span class="math notranslate nohighlight">\(P(c,d)\)</span>, which, according to Bayes Theorem, can be reformulated as <span class="math notranslate nohighlight">\(P(c,d)=P(d)\times P(c|d) =P(c) \times P(d|c)\)</span></p></li>
<li><p>That is, the training in generative models is based on the <strong>joint</strong> probability of the data and the class, i.e., <span class="math notranslate nohighlight">\(P(c,d)\)</span></p></li>
<li><p>Examples: N-gram Language Models, Naive Bayes, Hidden Markov Model, Probabilistic Context-Free Grammars.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Discriminative</strong>:</p>
<ul>
<li><p>The goal of training is to directly find <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Training is based on the <strong>conditional</strong> Probability of the class given the data, i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Examples: Logistic regression, Maximum Entropy Models, Conditional Random Field, Support Vector Machine, Perceptron.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Strengths of <strong>discriminative</strong> models:</p>
<ul>
<li><p>They give high accuracy performance.</p></li>
<li><p>They make it easier to include linguistically relevant features.</p></li>
</ul>
</li>
<li><p>Other classification models:</p>
<ul>
<li><p>Tree-based methods (Decision tree, Random Forest)</p></li>
<li><p>Neural Network</p></li>
</ul>
</li>
</ul>
<div class="section" id="naive-bayes">
<h2><span class="section-number">3.1. </span>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/naive-bayes.png" /></p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In Bayes, the <span class="math notranslate nohighlight">\(P(Y)\)</span> is referred to as the <strong>prior probability</strong> of Y and the <span class="math notranslate nohighlight">\(P(Y|X)\)</span> is referred to as the <strong>posterior probability</strong> of Y.</p>
</div>
</div>
<ul class="simple">
<li><p>Naive Bayes features the Bayes Theorem:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes1">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-naive-bayes1" title="Permalink to this equation">¶</a></span>\[
P(Y|X) = P(Y) \frac{P(X|Y)}{P(X)} 
\]</div>
<div class="section" id="intuition-of-naive-bayes">
<h3>Intuition of Naive Bayes<a class="headerlink" href="#intuition-of-naive-bayes" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/ml-naive-bayes.gif" /></p>
<ul class="simple">
<li><p>In Naive Bayes, given a document <span class="math notranslate nohighlight">\(d\)</span> and a class <span class="math notranslate nohighlight">\(c\)</span>, the goal is to find the <strong>maximum joint probability</strong> <span class="math notranslate nohighlight">\(P(c,d)\)</span>. And according bayes rule, the goal (of finding the maximum joint probability) can be reformulated as finding the maximum of the <strong>posterior probability</strong> of the class, <span class="math notranslate nohighlight">\(P(c|d)\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-navie-bayes2">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-navie-bayes2" title="Permalink to this equation">¶</a></span>\[\begin{split}
P(c,d) = P(d|c)\times P(c) = P(c|d) \times P(d)\\
P(c|d) = \frac{P(d|c)\times P(c)}{P(d)}
\end{split}\]</div>
<ul class="simple">
<li><p>Because the <span class="math notranslate nohighlight">\(P(d)\)</span> is a constant for all classes estimation, we can drop the denominator. And now the goal is to find the class <span class="math notranslate nohighlight">\(c\)</span> that maximizes the <strong>posterior probability</strong> of the class, i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>. (MAP = Maximum A Posterior Estimation)</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes3">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-naive-bayes3" title="Permalink to this equation">¶</a></span>\[\begin{split}
C_{MAP} = \underset{c \in C}{\arg\max}P(c|d)\\
C_{MAP}= \underset{c \in C}{\arg\max}\frac{P(d|c)\times P(c)}{P(d)} \\
= \underset{c \in C}{\arg\max}P(d|c)\times P(c) \\
\end{split}\]</div>
<ul class="simple">
<li><p>In Naive Bayes, the probabilities <span class="math notranslate nohighlight">\(P(C=c_i)\)</span> and <span class="math notranslate nohighlight">\(P(X_i|C=c_i)\)</span> are <strong>parameters</strong>.</p></li>
<li><p>The standard, maximum likelihood, approach is to calculate these parameters (probabilities) using MLE estimators based on the training data.</p></li>
</ul>
<ul class="simple">
<li><p>For example, for class prior probabilities, <span class="math notranslate nohighlight">\(P(C=c_i)\)</span>, we count the cases where <span class="math notranslate nohighlight">\(C=c_i\)</span> and divide by the sample size.</p></li>
<li><p>Similarly, each feature <span class="math notranslate nohighlight">\(x_i\)</span> of the document is in the form of a probability value of an observed event given the class, <span class="math notranslate nohighlight">\(P(x_i|C=c_i)\)</span>. For example, the probability of observing the word ‘好看’ (<strong><span class="math notranslate nohighlight">\(x_i\)</span></strong>) when the document class is <strong><span class="math notranslate nohighlight">\(c_i\)</span></strong>. Same for conditional probabilities <span class="math notranslate nohighlight">\(P(好看|C=c_i)\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>A document needs to be vectorized through feature engineering into a numeric representation, i.e., a set of <strong>n</strong> features from the document, <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span></p></li>
<li><p>Given a class <span class="math notranslate nohighlight">\(c\)</span> and a set of <strong><span class="math notranslate nohighlight">\(n\)</span></strong> features from the document, <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span>, we can denote the <strong>posterior probability</strong> of each class <span class="math notranslate nohighlight">\(c\)</span> as follows:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes4">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-naive-bayes4" title="Permalink to this equation">¶</a></span>\[\begin{split}
C_{MAP}= \underset{c \in C}{\arg\max}P(c|d)\\\\
\underset{c \in C}{\arg\max}  P(d|c) \times P(c) \\
= \underset{c \in C}{\arg\max} P(x_1,x_2,...,x_n|c) \times P(c) \\
\end{split}\]</div>
<ul class="simple">
<li><p>Important assumptions in Naive Bayes:</p>
<ul>
<li><p>Independence Assumption: Assume features are independent of each other.</p></li>
<li><p>Conditional Independence: Assume the feature probabilities <span class="math notranslate nohighlight">\(P(x_i|C=c_i)\)</span> are independent given the label <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes5">
<span class="eqno">(3.5)<a class="headerlink" href="#equation-naive-bayes5" title="Permalink to this equation">¶</a></span>\[
P(x_1,x_2,...,x_n|c)=P(x_1|c)\times P(x_2|c)\times P(x_3|c) \times ... \times P(x_n|c)
\]</div>
<ul class="simple">
<li><p>Then we can simply the equation:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes6">
<span class="eqno">(3.6)<a class="headerlink" href="#equation-naive-bayes6" title="Permalink to this equation">¶</a></span>\[\begin{split}
C_{MAP} = \underset{c \in C}{\arg\max} P(x_1,x_2,...,x_n|c) \times P(c) \\
= \underset{c \in C}{\arg\max} P(c) \prod_{x\in X}P(x|c)
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Multinomial Naive Bayes</strong> is an extension of the Naive Bayes for predicting and classifying data points, where the number of distinct labels or outcomes are more than two.</p></li>
</ul>
<ul class="simple">
<li><p>Issues of <strong>Smoothing</strong></p>
<ul>
<li><p>For unseen features in a new document, e.g., <span class="math notranslate nohighlight">\(P(x|c)\)</span>, this would render the entire <strong>posterior probability</strong> of the class to be zero.</p></li>
<li><p>Laplace smoothing (Add-one)</p></li>
<li><p>Lidstone smooting (Self-defined <span class="math notranslate nohighlight">\(\alpha\)</span> &lt; 1)</p></li>
<li><p>cf. <code class="docutils literal notranslate"><span class="pre">sklearn.naive_bayes.GaussianNB(*,</span> <span class="pre">priors=None,</span> <span class="pre">var_smoothing=1e-09)</span></code></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="logistic-regression">
<h2><span class="section-number">3.2. </span>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression</strong> is also a form of probabilistic statistical classification model.</p></li>
<li><p>The model is trained by maximizing directly the probability of the <strong>class</strong> (c) given the observed <strong>data</strong> (d), i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Logistic Regression is similar to Linear Regression, whose predicted values are both numeric (cf. <a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Models</a>).</p></li>
</ul>
<ul class="simple">
<li><p>A document needs to be vectorized through feature engineering into a numeric representation, i.e., a set of <strong><span class="math notranslate nohighlight">\(n\)</span></strong> features characterizing the semantics of the document, <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span></p></li>
<li><p>Then Logistic Regression models the <strong>probability</strong> of the class given these observed values as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + ... + \beta_ix_i
= \sum_{i}\beta_ix_i
\]</div>
<ul>
<li><p>Each feature <span class="math notranslate nohighlight">\(x_i\)</span> of the document is a function that characterizes the relevant linguistic properties of the document. These features can be all <strong>manually</strong> annotated or created <strong>automatically</strong>.</p></li>
<li><p>These features are often in simple forms that are of <strong>binary</strong> values or <strong>numeric</strong> values within a range:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    x_1 &amp;= f_1(x_1) = [Contains(&quot;好看&quot;) ] = \{0, 1\} \\
    x_2 &amp;= f_2(x_2) = [Contains(&quot;絕配&quot;) ] = \{0, 1\} \\
    x_3 &amp;= f_3(x_3) = [FreqOf(&quot;覺得&quot;)] = \{\textit{Any Positive Number}\}\\
    &amp;...\\
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>But so far the model prediction (<span class="math notranslate nohighlight">\(y\)</span>) could be any number (<span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>) based on the current linear model.</p></li>
<li><p>In order to make sure the predicted values of the model are within the range of 0 and 1 (i.e., a probability-like value), the dependent variable <span class="math notranslate nohighlight">\(y\)</span> is transformed with a so-called <strong>link function</strong>.</p></li>
<li><p>That is, <strong>link functions</strong> transform the predicted <span class="math notranslate nohighlight">\(y\)</span> into a range more appropriate for linear modeling.</p></li>
</ul>
<ul class="simple">
<li><p>In particular, in Binary Logistic Regression, the <strong>inverse logit transformation</strong> is used, transforming <span class="math notranslate nohighlight">\(y\)</span> from the range of <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span> to into probability values ranging from 0 to 1.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + ... + \beta_ix_i
= \sum_{i}\beta_ix_i
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}\\
P(c|d) &amp;= \frac{1}{1+e^{-y}}\\
                    &amp;= \frac{1}{1+e^{-(\sum_{i}\beta_ix_i)}}\\
                    &amp;= P(c|x_1,x_2,...,x_i)\\
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>And we can re-formulate the model equation as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}\\
P(c|d) &amp;= P(c|x_1,x_2,...,x_i)\\
&amp;= \frac{1}{1+e^{-(\sum_{i}{\beta_ix_i})}}
\end{align}
\end{split}\]</div>
<ul class="simple">
<li><p>In model prediction, the model will then determine the <strong>class</strong> of the document by choosing the class that maximizes the conditional probability of <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>In training:</p>
<ul>
<li><p>Given a document in a training set, <span class="math notranslate nohighlight">\(d\)</span>, the initial Logistic Regression model will output the predicted conditional probability of <span class="math notranslate nohighlight">\(d\)</span> being the class, <span class="math notranslate nohighlight">\(c\)</span>, i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>This is the <strong>likelihood</strong> of the document.</p></li>
<li><p>And the optimal Logistic Regression is the one that <strong>maximizes the likelihoods</strong> of the documents in the entire training set.</p></li>
<li><p>This <strong>maximum likelihood</strong> approach is similar to the least squares method in linear regression.</p></li>
</ul>
</li>
</ul>
<ul>
<li><p>Interpreting the parameters (coefficients <span class="math notranslate nohighlight">\(\beta_i\)</span>)</p>
<ul class="simple">
<li><p>The coefficient refers to the change of the odds of having the target class label in relation to a specific predictor.</p></li>
<li><p>Odds are defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align}\\
    odds &amp;=\frac{\textit{The probability that the event will occur}}{\textit{The probability that the event will NOT occur}} \\&amp;= \frac{P_{\textit{relevant class}}}{1 - P_{\textit{relevant class}}}
    \end{align}
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>A quick example (based on Stefan Gries’ data): (skipped)</p>
<ul>
<li><p>We use the type of the subordinate clauses (<code class="docutils literal notranslate"><span class="pre">SUBORDTYPE</span></code>) to predict the <code class="docutils literal notranslate"><span class="pre">ORDER</span></code> of the main and subordinate clauses.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(X: \{caus, temp\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y: \{mc-sc, sc-mc\}\)</span></p></li>
</ul>
</li>
<li><p>Unit = sentences; Label = main-subordinate clauses orders.</p></li>
<li><p>We run a logistic regression, using SUBORDTYPE as the predictor and ORDER as the response variable.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> rpy2.ipython
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1">## data from Stefan Gries&#39; &quot;Statistics for Linguistics with R&quot;</span>
<span class="n">csv</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;../../../RepositoryData/data/gries_sflwr/_inputfiles/05-3_clauseorders.csv&#39;</span><span class="p">)</span>
<span class="n">csv</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CASE</th>
      <th>ORDER</th>
      <th>SUBORDTYPE</th>
      <th>LEN_MC</th>
      <th>LEN_SC</th>
      <th>LENGTH_DIFF</th>
      <th>CONJ</th>
      <th>MORETHAN2CL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4777</td>
      <td>sc-mc</td>
      <td>temp</td>
      <td>4</td>
      <td>10</td>
      <td>-6</td>
      <td>als/when</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1698</td>
      <td>mc-sc</td>
      <td>temp</td>
      <td>7</td>
      <td>6</td>
      <td>1</td>
      <td>als/when</td>
      <td>no</td>
    </tr>
    <tr>
      <th>2</th>
      <td>953</td>
      <td>sc-mc</td>
      <td>temp</td>
      <td>12</td>
      <td>7</td>
      <td>5</td>
      <td>als/when</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1681</td>
      <td>mc-sc</td>
      <td>temp</td>
      <td>6</td>
      <td>15</td>
      <td>-9</td>
      <td>als/when</td>
      <td>no</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4055</td>
      <td>sc-mc</td>
      <td>temp</td>
      <td>9</td>
      <td>5</td>
      <td>4</td>
      <td>als/when</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>398</th>
      <td>1794</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>13</td>
      <td>23</td>
      <td>-10</td>
      <td>weil/because</td>
      <td>no</td>
    </tr>
    <tr>
      <th>399</th>
      <td>4095</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>10</td>
      <td>7</td>
      <td>3</td>
      <td>weil/because</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>400</th>
      <td>2733</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>8</td>
      <td>5</td>
      <td>3</td>
      <td>weil/because</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>401</th>
      <td>350</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>8</td>
      <td>11</td>
      <td>-3</td>
      <td>weil/because</td>
      <td>no</td>
    </tr>
    <tr>
      <th>402</th>
      <td>208</td>
      <td>sc-mc</td>
      <td>caus</td>
      <td>9</td>
      <td>6</td>
      <td>3</td>
      <td>weil/because</td>
      <td>yes</td>
    </tr>
  </tbody>
</table>
<p>403 rows × 8 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>To explain the model in a more comprehensive way, I like to switch back to R for the statistical outputs. I think they are more intuitive.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">R</span> -i csv

# library(readr)
# csv= read_delim(&#39;../../../RepositoryData/data/gries_sflwr/_inputfiles/05-3_clauseorders.csv&#39;,&#39;\t&#39;)
print(head(csv))
print(table(csv$SUBORDTYPE, csv$ORDER))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF     CONJ MORETHAN2CL
0 4777 sc-mc       temp      4     10          -6 als/when          no
1 1698 mc-sc       temp      7      6           1 als/when          no
2  953 sc-mc       temp     12      7           5 als/when         yes
3 1681 mc-sc       temp      6     15          -9 als/when          no
4 4055 sc-mc       temp      9      5           4 als/when         yes
5  967 sc-mc       temp      9      5           4 als/when         yes
      
       mc-sc sc-mc
  caus   184    15
  temp    91   113
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">R</span>
lg = glm(factor(csv$ORDER)~factor(csv$SUBORDTYPE), family=&quot;binomial&quot;)
summary(lg) 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = factor(csv$ORDER) ~ factor(csv$SUBORDTYPE), family = &quot;binomial&quot;)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2706  -0.3959  -0.3959   1.0870   2.2739  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -2.5069     0.2685  -9.336   &lt;2e-16 ***
factor(csv$SUBORDTYPE)temp   2.7234     0.3032   8.982   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 503.80  on 402  degrees of freedom
Residual deviance: 386.82  on 401  degrees of freedom
AIC: 390.82

Number of Fisher Scoring iterations: 5
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Based on the model parameters, we get the formula for the probability prediction of our response variable:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 = -2.50 + 2.72x_1
\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To interpret the coefficients we need to know the order of the two class labels in the outcome variable. The most straightforward way to do this is to create a table of the outcome variable.</p>
<p>As shown above, the second level of <code class="docutils literal notranslate"><span class="pre">ORDER</span></code> is <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code>, this tells us that the coefficients in the Logistic Regression are predicting whether or not the clause order is <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code>.</p>
</div>
<ul>
<li><p>Now we can estimate the following probabilities:</p>
<ul class="simple">
<li><p>Probability of <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code> when subordinate is <code class="docutils literal notranslate"><span class="pre">cause</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}\\
    P(type=scmc|x_1=cause) &amp;= \frac{1}{1+e^{-y}}\\
    &amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times x_1})}} \\
    &amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times 0})}}\\
    &amp;= 0.0758
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>Probability of <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code> when subordinate is <code class="docutils literal notranslate"><span class="pre">temp</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}\\
P(type=scmc|x_1=temp) &amp;= \frac{1}{1+e^{-y}}\\
&amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times x_1})}} \\
&amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times 1})}}\\
&amp;= 0.5547
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Now we can also compute the <strong>odds</strong> of the predicted probability.</p></li>
<li><p>The odds of an event is the ratio of:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\textit{The probability that the event will occur}}{\textit{The probability that the event will NOT occur}}
\]</div>
<ul class="simple">
<li><p>Simply put, odds are just a ratio of two complementary probability values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Odds = \frac{P(c)}{1-P(c)} = \frac{P(type=scmc)}{1-P(type=scmc)}
\]</div>
<ul>
<li><p>Now we can compute the two odds:</p>
<ul class="simple">
<li><p>The odds of the probabilities when the subordinate is <code class="docutils literal notranslate"><span class="pre">caus</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    odds_1 = \frac{0.07}{1-0.07} = 0.08
    \]</div>
<ul class="simple">
<li><p>The odds of the probabilities when the subordinate is <code class="docutils literal notranslate"><span class="pre">temp</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    odds_2 = \frac{0.55}{1-0.55} = 1.25
    \]</div>
</li>
</ul>
<ul>
<li><p>Finally, we can compute the <strong>log odds ratios</strong> (the log of the ratios between two odds):</p>
<div class="math notranslate nohighlight">
\[
    \textit{Odds Ratio} = \frac{odds_2}{odds_1} = \frac{1.25}{0.08} = 15.18
    \]</div>
<div class="math notranslate nohighlight">
\[
    \textit{Log Odds Ratio} = log\left(\frac{odds_2}{odds_1}\right) = log\left(\frac{1.25}{0.08}\right) = 2.72
    \]</div>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 = -2.50 + 2.72x_1
\]</div>
<ul class="simple">
<li><p>That is the meaning of the coefficient <span class="math notranslate nohighlight">\(\beta_1, 2.72\)</span>:</p>
<ul>
<li><p>The <strong>odds</strong> (of having <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code> order) when the subordinate clause is <code class="docutils literal notranslate"><span class="pre">temp</span></code> are <span class="math notranslate nohighlight">\(e^{2.7234}= 15.23\)</span> times more than the <strong>odds</strong> when the subordinate clause is <code class="docutils literal notranslate"><span class="pre">caus</span></code>.</p></li>
<li><p>Or the <strong>log odds ratio</strong> increases 2.72 times when the subordinate clause is <code class="docutils literal notranslate"><span class="pre">temp</span></code> as compared to when the subordinate is <code class="docutils literal notranslate"><span class="pre">caus</span></code>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">R</span>
y1= 1/(1+exp(-(-2.50 + 2.72*0))) # prob of sc-mc when SUBORDTYPE = 0 (caus)
y2= 1/(1+exp(-(-2.50 + 2.72*1))) # prob of sc-mc when SUBORDTYPE = 1 (temp)
odds1 = y1/(1-y1) # odds when SUBORDTYPE = 0
odds2 = y2/(1-y2) # odds when SUBORDTYPE = 1

cat(&quot;Prob(type=sc-mc|subordtype=cause):&quot;, y1, &quot;\n&quot;)
cat(&quot;The odds of Prob(type=mc-sc|subordtype=cause) vs. Prob(type=sc-mc|subordtype=cause):&quot;, odds1,&quot;\n\n&quot;)

cat(&quot;Prob(type=sc-mc|subordtype=temp):&quot;, y2, &quot;\n&quot;)
cat(&quot;The odds of Prob(type=mc-sc|subordtype=temp) vs. Prob(type=sc-mc|subordtype=temp):&quot;, odds2, &quot;\n\n&quot;)

odds_ratio = odds2/odds1
log_odds_ratio = log(odds_ratio)
cat(&quot;Odds Ratioos of subordtype=temp vs. subordtype=cause:&quot;, odds_ratio, &quot;\n&quot;)
cat(&quot;Log Odds Ratios of subordtype=temp vs. subordtype=cause:&quot;, log_odds_ratio)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prob(type=sc-mc|subordtype=cause): 0.07585818 
The odds of Prob(type=mc-sc|subordtype=cause) vs. Prob(type=sc-mc|subordtype=cause): 0.082085 

Prob(type=sc-mc|subordtype=temp): 0.5547792 
The odds of Prob(type=mc-sc|subordtype=temp) vs. Prob(type=sc-mc|subordtype=temp): 1.246077 

Odds Ratioos of subordtype=temp vs. subordtype=cause: 15.18032 
Log Odds Ratios of subordtype=temp vs. subordtype=cause: 2.72
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="support-vector-machine-svm">
<h2><span class="section-number">3.3. </span>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>To understand what SVM is, we probably need to introduce the idea of <strong>support vectors</strong>.</p></li>
<li><p>Given a linearly separable dataset, it is easy to find a hyperplane (in 2D, that would be a line) that separates data into two distinct sub-groups.</p>
<ul>
<li><p>The shortest distance between the observations and the threshold (hyperplane) is called the <strong>margin</strong>.</p></li>
<li><p>A hyperplane that gives us the largest margin to make classification is referred to as <strong>Maximum Margin Classifier</strong>.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>These threshold cases are the vectors that support the maximum margin classifier, henceforth, known as the “support vectors”.
<img alt="" src="../_images/svm.002.png" /></p></li>
</ul>
<ul class="simple">
<li><p>Issues with Maximum Margin Classifier:</p>
<ul>
<li><p>Maximum Margin Classifier is very sensitive to outliers.</p></li>
<li><p>The classifier may be too biased toward the class with fewer outliers.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/svm.003.png" /></p>
<ul class="simple">
<li><p>If we choose a hyperplane which allows misclassifications, we may be able to find a better classifier.</p>
<ul>
<li><p>When we allow misclassifications, the distance between the observations and the hyperplane is called a <strong>soft margin</strong>.</p></li>
<li><p>A classifier allowing misclassifications (i.e., based on soft margins) is referred to as <strong>soft margin classifier</strong>, or <strong>support vector classifier</strong>.</p></li>
<li><p>The observations on the edge and within the <strong>soft margin</strong> are called <strong>support vectors</strong>.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/svm.004.png" /></p>
<ul class="simple">
<li><p>Support Vector Classifiers can deal with observations with outliers and the assumption is that the observations are linearly separable.</p></li>
<li><p>What if the observations are not linearly separable in the first place? This is where <strong>Support Vector Machine</strong> comes in!
<img alt="" src="../_images/svm.005.png" /></p></li>
</ul>
<ul class="simple">
<li><p>Intuition of Support Vector Machine:</p>
<ul>
<li><p>Start with data in a relatively low dimension and seek linear solutions</p></li>
<li><p>Move the data into a higher dimension (when no obvious linear classifier found)</p></li>
<li><p>Find a <strong>Support Vector Classifier</strong> that separates the higher dimensional data into two groups</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/svm-kernel.gif" /></p>
<ul class="simple">
<li><p>The mathematical tricks of moving data into a higher dimension are the <strong>kernel functions</strong>. This process is called the <strong>Kernel Tricks</strong>.</p></li>
<li><p>SVM often uses two kernel functions to find support vector classifiers in higher dimensions.</p>
<ul>
<li><p>Polynomial Kernel</p></li>
<li><p>Radial Basis Function Kernel (RBF)</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>SVM has a <strong>cost function (C)</strong>, which controls the compromise of the <strong>misclassifications</strong> and <strong>margin</strong> sizes.</p></li>
<li><p>C is the model penalty for the data points that fall on the wrong side of the hyperplane.</p>
<ul>
<li><p>A smaller C creates a softer (larger) margin, allowing more misclassifications.</p></li>
<li><p>A larger C creates a narrower margin, allowing fewer misclassifications.</p></li>
</ul>
</li>
<li><p>The greater the cost parameter, the harder the optimization will try to achieve 100 percent separation.</p></li>
</ul>
<ul class="simple">
<li><p>Heuristics for SVM</p>
<ul>
<li><p>There is no reliable rule for matching a kernel to a particular learning task.</p></li>
<li><p>The fit depends heavily on the concept to be learned as well as the amount of training data and the relationships among the features.</p></li>
<li><p>Trial and error is required by training and evaluating several SVMs on a validation dataset (i.e., <em><strong>k</strong></em>-fold cross validation)</p></li>
<li><p>The choice of kernel is often arbitrary because the performances with different kernels may vary only slightly.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="other-discriminative-models-skipped">
<h2><span class="section-number">3.4. </span>Other Discriminative Models (skipped)<a class="headerlink" href="#other-discriminative-models-skipped" title="Permalink to this headline">¶</a></h2>
<p>There are many more ways to learn the weights (i.e., <span class="math notranslate nohighlight">\(\beta_i\)</span>) in the discriminative models. Different algorithms may prioritize different <strong>link functions</strong> to train the model parameters.</p>
<p>In this course, we will not cover all the details of these variants. Please refer to machine learning courses for more details on these algorithms.</p>
<div class="section" id="maximum-entropy-model-maxent">
<h3>Maximum Entropy Model (Maxent)<a class="headerlink" href="#maximum-entropy-model-maxent" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Maxent is very similar to logistic regression, and differs in mainly its way of parameterization (e.g., feature functions, link functions)</p></li>
<li><p>In Maxent, each feature function is a function of both the text feature and the class, i.e., <span class="math notranslate nohighlight">\(f(x_i, c_i)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
f_1(c_i, x_1) \equiv [C = c_i \land Contains(&quot;好看&quot;)]\\
f_2(c_i, x_2) \equiv [C = c_i \land Contains(&quot;絕配&quot;)]\\
f_3(c_i, x_3) \equiv [C = c_1 \land Ends(w,&quot;c&quot;)]\\
...
\end{split}\]</div>
<ul class="simple">
<li><p>The model will assign to each feature a <strong>weight</strong>.</p></li>
<li><p>Maxent uses a <em>softmax</em> function to transform the sum of the weigthed feature values into probabilities. (The exponetial transformation of the sum of the weighted feature values make all predicted values positive.)</p>
<ul>
<li><p>Numerator = The probability of the feature i co-occuring with specific class.</p></li>
<li><p>Denominator = The sum of the probability of the feature co-occurring with the other classes.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
p(c|d,\lambda) = \frac{e^{\sum_{i}\lambda_if_i(c,x_i)}}{\sum_{c'}{e^{\sum_{i}\lambda_if_i(c',x_i)}}}\\
\end{split}\]</div>
</div>
</div>
<div class="section" id="other-classification-models-skipped">
<h2><span class="section-number">3.5. </span>Other Classification Models (skipped)<a class="headerlink" href="#other-classification-models-skipped" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>There are other classification models that are not probabilistic</p>
<ul>
<li><p><strong>Decision Tree</strong></p>
<ul>
<li><p>It uses a split condition to predict class labels based on one or multiple input features.</p></li>
<li><p>The classification process starts from the root node of the tree; at each node, the classifier will check which input feature will most effectively split the data into sub-groups.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="emsemble-methods">
<h2><span class="section-number">3.6. </span>Emsemble Methods<a class="headerlink" href="#emsemble-methods" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Bagging</p>
<ul>
<li><p>Take subsets of data (e.g., bootstrap samples) and train a model on each subset in parallel</p></li>
<li><p>The subsets are allowed to simultaneously vote on the outcome</p></li>
<li><p>The final outcome is often an average aggregation.</p></li>
<li><p>Example: <strong>Random Forests</strong></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Boosting</p>
<ul>
<li><p>Utilize sequential modeling by improving one model from the errors of the previous model</p></li>
<li><p>Often use the output of one model as an input into the next in a form of sequential processing</p></li>
<li><p>Example: <strong>Gradient boosting machines</strong></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="conclusion">
<h2><span class="section-number">3.7. </span>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Feature engineering is a lot more important than algorithm selection.</p></li>
<li><p>With good feature representations of the data, usually it doesn’t really matter which machine learning algorithm we choose.</p></li>
<li><p>Generally, <strong>discriminative</strong> models out-perform <strong>generative</strong> models.</p></li>
<li><p>Maybe one day the optimal algorithm can be determined <em>automatically</em> via machine learning as well!!</p></li>
<li><p>Please see <a class="reference external" href="https://www.marktechpost.com/2021/02/28/google-ai-introduces-model-search-an-open-source-platform-for-finding-optimal-machine-learning-ml-models/?fbclid=IwAR3FkqJzXGIqG-X25BXslDZ077F9s4onOaQzlosc9u7qeto9LNd7prt_PNE">Google’s AutoML effort: Google AI Introduces Model Search, An Open Source Platform for Finding Optimal Machine Learning Models</a>.</p></li>
</ul>
</div>
<div class="section" id="references">
<h2><span class="section-number">3.8. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>NLTK Book, Chapter 6.</p></li>
<li><p>Sarkar (2020), Chapter 5.</p></li>
<li><p>Geron (2019), Chapter 5-7</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml-simple-case.html" title="previous page"><span class="section-number">2. </span>Machine Learning: A Simple Example</a>
    <a class='right-next' id="next-link" href="ml-sklearn-classification.html" title="next page"><span class="section-number">1. </span>Sentiment Analysis Using Bag-of-Words</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>