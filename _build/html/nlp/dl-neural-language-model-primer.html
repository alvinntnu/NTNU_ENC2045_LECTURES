
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Neural Language Model: A Start &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Word Embeddings" href="../temp/text-vec-embedding.html" />
    <link rel="prev" title="1. Sequence Models Intuition" href="dl-sequence-models-intuition.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml-algorithm.html">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-neural-network-from-scratch.html">
   1. Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-simple-case.html">
   2. Deep Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sentiment-case.html">
   3. Deep Learning: Sentiment Analysis
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neural Language Model and Embeddings
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sequence-models-intuition.html">
   1. Sequence Models Intuition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Neural Language Model: A Start
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/text-vec-embedding.html">
   3. Word Embeddings
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Seq2Seq, Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-seq-to-seq-types.html">
   Attention: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-seq-to-seq-attention-addition.html">
   Sequence Model with Attention for Addition Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-transformers-intuition.html">
   Transformers: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-transformers-keras.html">
   Text Classification with Transformer
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   Assignment III: Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   Assignment IV: Chinese Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   Assignment V: Text Vectorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/6-machine-learning.html">
   Assignment VI: Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/midterm-exam.html">
   Midterm Exam
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/7-topic-modeling.html">
   Assignment VII: Topic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/8-dl-chinese-name-gender.html">
   Assignment VIII: Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/9-sentiment-analysis-dl.html">
   Assignment IX: Sentiment Analysis Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/10-neural-language-model.html">
   Assignment X: Neural Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/11-word2vec.html">
   Assignment XI: Word Embeddings
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nlp/dl-neural-language-model-primer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/nlp/dl-neural-language-model-primer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-neural-language-model-primer.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflow-of-neural-language-model">
   2.1. Workflow of Neural Language Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bigram-model">
   2.2. Bigram Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     Tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-to-sequences-and-training-testing-sets">
     Text-to-Sequences and Training-Testing Sets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-representation-of-the-next-word">
     One-hot Representation of the Next-Word
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-rnn-language-model">
     Define RNN Language Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-generation-using-the-model">
     Text Generation Using the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-strategies-for-text-generation">
     Sampling Strategies for Text Generation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#beam-search-skipped">
   2.3. Beam Search (skipped)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#searching-in-nlp">
     Searching in NLP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beam-search-decoding">
     Beam Search Decoding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   2.4. References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-language-model-a-start">
<h1><span class="section-number">2. </span>Neural Language Model: A Start<a class="headerlink" href="#neural-language-model-a-start" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>In this tutorial, we will look at a naive example of <strong>neural language model</strong>.</p></li>
<li><p>Given a corpus, we can build a neural language model, which will learn to predict the next word given a specified limited context.</p></li>
<li><p>Depending on the size of the <strong>limited context</strong>, we can implement different types of neural language model:</p>
<ul>
<li><p>Bigram-based neural language model: The LM model only uses one preceding word for the next-word prediction.</p></li>
<li><p>Trigram-based neural language model: The model will use two preceding words for the next-word prediction.</p></li>
<li><p><em>Line</em>-based neural language model: The model with use all the existing fore-going words for the next-word prediction</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>This tutorial will demonstarte how to build a bigram-based language model.</p></li>
<li><p>In the Assignments, you need to extend the same rationale to other types of language models.</p></li>
</ul>
<div class="section" id="workflow-of-neural-language-model">
<h2><span class="section-number">2.1. </span>Workflow of Neural Language Model<a class="headerlink" href="#workflow-of-neural-language-model" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/neural-language-model-flowchart.png" /></p>
</div>
<div class="section" id="bigram-model">
<h2><span class="section-number">2.2. </span>Bigram Model<a class="headerlink" href="#bigram-model" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A bigram-based language model assumes that the next word (to be predicted) depends only on the previous word.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Dependencies</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span><span class="p">,</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A quick reminder of important parameters for <code class="docutils literal notranslate"><span class="pre">Tokenzier()</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">num_words</span></code>: the maximum number of words to keep, based on word frequency. Only the most common <code class="docutils literal notranslate"><span class="pre">num_words-1</span></code> words will be kept.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">filters</span></code>: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the <code class="docutils literal notranslate"><span class="pre">'</span></code> character.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lower</span></code>: boolean. Whether to convert the texts to lowercase.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">split</span></code>: str. Separator for word splitting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">char_level</span></code>: if True, every character will be treated as a token.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">oov_token</span></code>: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># source text</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot; Jack and Jill went up the hill</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">		To fetch a pail of water</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">		Jack fell down and broke his crown</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">		And Jill came tumbling after</span><span class="se">\n</span><span class="s2"> &quot;&quot;&quot;</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

<span class="c1"># integer encode text</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># now the data consists of a sequence of word index integers</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># determine the vocabulary size</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Size: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary Size: 22
{&#39;and&#39;: 1, &#39;jack&#39;: 2, &#39;jill&#39;: 3, &#39;went&#39;: 4, &#39;up&#39;: 5, &#39;the&#39;: 6, &#39;hill&#39;: 7, &#39;to&#39;: 8, &#39;fetch&#39;: 9, &#39;a&#39;: 10, &#39;pail&#39;: 11, &#39;of&#39;: 12, &#39;water&#39;: 13, &#39;fell&#39;: 14, &#39;down&#39;: 15, &#39;broke&#39;: 16, &#39;his&#39;: 17, &#39;crown&#39;: 18, &#39;came&#39;: 19, &#39;tumbling&#39;: 20, &#39;after&#39;: 21}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="text-to-sequences-and-training-testing-sets">
<h3>Text-to-Sequences and Training-Testing Sets<a class="headerlink" href="#text-to-sequences-and-training-testing-sets" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Principles for bigrams extraction</p>
<ul>
<li><p>When we create bigrams as the input sequences for network training, we need to make sure that we do not include <strong>unmeaningful</strong> bigrams, such as bigrams spanning the text boundaries, or sentence boundaries.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create bigrams sequences</span>

<span class="c1">## bigrams holder</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>


<span class="c1">## Extract bigrams from each text</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">encoded</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">e</span><span class="p">)):</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Sequences: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total Sequences: 21
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[2, 1],
       [1, 3],
       [3, 4],
       [4, 5],
       [5, 6]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>A sequence contains both our input and also output of the network.</p></li>
<li><p>That is, for bigram-based LM, the first word is the input <em>X</em> and the second word is the expected output <em>y</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split into X and y elements</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 1]
 [1 3]
 [3 4]
 [4 5]
 [5 6]]
[2 1 3 4 5]
[1 3 4 5 6]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="one-hot-representation-of-the-next-word">
<h3>One-hot Representation of the Next-Word<a class="headerlink" href="#one-hot-representation-of-the-next-word" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Because the neural language model is going to be a multi-class classifier (for word prediction), we need to convert our <code class="docutils literal notranslate"><span class="pre">y</span></code> into one-hot encoding.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># one hot encode outputs</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(21, 22)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-rnn-language-model">
<h3>Define RNN Language Model<a class="headerlink" href="#define-rnn-language-model" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>  <span class="c1"># LSTM Complexity</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 1, 10)             220       
_________________________________________________________________
lstm (LSTM)                  (None, 50)                12200     
_________________________________________________________________
dense (Dense)                (None, 22)                1122      
=================================================================
Total params: 13,542
Trainable params: 13,542
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compile network</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="c1"># fit network</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/500
1/1 - 2s - loss: 3.0898 - accuracy: 0.2381
Epoch 2/500
1/1 - 0s - loss: 3.0889 - accuracy: 0.2381
Epoch 3/500
1/1 - 0s - loss: 3.0881 - accuracy: 0.2381
Epoch 4/500
1/1 - 0s - loss: 3.0872 - accuracy: 0.2381
Epoch 5/500
1/1 - 0s - loss: 3.0863 - accuracy: 0.2857
Epoch 6/500
1/1 - 0s - loss: 3.0855 - accuracy: 0.2857
Epoch 7/500
1/1 - 0s - loss: 3.0846 - accuracy: 0.2857
Epoch 8/500
1/1 - 0s - loss: 3.0837 - accuracy: 0.3333
Epoch 9/500
1/1 - 0s - loss: 3.0828 - accuracy: 0.3333
Epoch 10/500
1/1 - 0s - loss: 3.0819 - accuracy: 0.3333
Epoch 11/500
1/1 - 0s - loss: 3.0810 - accuracy: 0.3333
Epoch 12/500
1/1 - 0s - loss: 3.0800 - accuracy: 0.3333
Epoch 13/500
1/1 - 0s - loss: 3.0791 - accuracy: 0.3333
Epoch 14/500
1/1 - 0s - loss: 3.0781 - accuracy: 0.3333
Epoch 15/500
1/1 - 0s - loss: 3.0771 - accuracy: 0.3333
Epoch 16/500
1/1 - 0s - loss: 3.0760 - accuracy: 0.3333
Epoch 17/500
1/1 - 0s - loss: 3.0750 - accuracy: 0.3333
Epoch 18/500
1/1 - 0s - loss: 3.0739 - accuracy: 0.3333
Epoch 19/500
1/1 - 0s - loss: 3.0728 - accuracy: 0.3333
Epoch 20/500
1/1 - 0s - loss: 3.0717 - accuracy: 0.3333
Epoch 21/500
1/1 - 0s - loss: 3.0706 - accuracy: 0.3333
Epoch 22/500
1/1 - 0s - loss: 3.0694 - accuracy: 0.2857
Epoch 23/500
1/1 - 0s - loss: 3.0682 - accuracy: 0.2381
Epoch 24/500
1/1 - 0s - loss: 3.0669 - accuracy: 0.2381
Epoch 25/500
1/1 - 0s - loss: 3.0657 - accuracy: 0.2381
Epoch 26/500
1/1 - 0s - loss: 3.0644 - accuracy: 0.2381
Epoch 27/500
1/1 - 0s - loss: 3.0630 - accuracy: 0.2381
Epoch 28/500
1/1 - 0s - loss: 3.0617 - accuracy: 0.2381
Epoch 29/500
1/1 - 0s - loss: 3.0602 - accuracy: 0.2381
Epoch 30/500
1/1 - 0s - loss: 3.0588 - accuracy: 0.2381
Epoch 31/500
1/1 - 0s - loss: 3.0573 - accuracy: 0.2381
Epoch 32/500
1/1 - 0s - loss: 3.0557 - accuracy: 0.2381
Epoch 33/500
1/1 - 0s - loss: 3.0542 - accuracy: 0.2381
Epoch 34/500
1/1 - 0s - loss: 3.0525 - accuracy: 0.2381
Epoch 35/500
1/1 - 0s - loss: 3.0509 - accuracy: 0.2381
Epoch 36/500
1/1 - 0s - loss: 3.0491 - accuracy: 0.2381
Epoch 37/500
1/1 - 0s - loss: 3.0474 - accuracy: 0.2381
Epoch 38/500
1/1 - 0s - loss: 3.0455 - accuracy: 0.2381
Epoch 39/500
1/1 - 0s - loss: 3.0436 - accuracy: 0.2381
Epoch 40/500
1/1 - 0s - loss: 3.0417 - accuracy: 0.2381
Epoch 41/500
1/1 - 0s - loss: 3.0397 - accuracy: 0.2381
Epoch 42/500
1/1 - 0s - loss: 3.0377 - accuracy: 0.2381
Epoch 43/500
1/1 - 0s - loss: 3.0355 - accuracy: 0.2381
Epoch 44/500
1/1 - 0s - loss: 3.0334 - accuracy: 0.2381
Epoch 45/500
1/1 - 0s - loss: 3.0311 - accuracy: 0.1905
Epoch 46/500
1/1 - 0s - loss: 3.0288 - accuracy: 0.1905
Epoch 47/500
1/1 - 0s - loss: 3.0264 - accuracy: 0.2381
Epoch 48/500
1/1 - 0s - loss: 3.0240 - accuracy: 0.2381
Epoch 49/500
1/1 - 0s - loss: 3.0215 - accuracy: 0.2381
Epoch 50/500
1/1 - 0s - loss: 3.0189 - accuracy: 0.2381
Epoch 51/500
1/1 - 0s - loss: 3.0162 - accuracy: 0.2381
Epoch 52/500
1/1 - 0s - loss: 3.0135 - accuracy: 0.2381
Epoch 53/500
1/1 - 0s - loss: 3.0106 - accuracy: 0.2381
Epoch 54/500
1/1 - 0s - loss: 3.0077 - accuracy: 0.2381
Epoch 55/500
1/1 - 0s - loss: 3.0047 - accuracy: 0.2381
Epoch 56/500
1/1 - 0s - loss: 3.0017 - accuracy: 0.2381
Epoch 57/500
1/1 - 0s - loss: 2.9985 - accuracy: 0.2381
Epoch 58/500
1/1 - 0s - loss: 2.9952 - accuracy: 0.2381
Epoch 59/500
1/1 - 0s - loss: 2.9919 - accuracy: 0.2381
Epoch 60/500
1/1 - 0s - loss: 2.9884 - accuracy: 0.2381
Epoch 61/500
1/1 - 0s - loss: 2.9849 - accuracy: 0.2381
Epoch 62/500
1/1 - 0s - loss: 2.9812 - accuracy: 0.2381
Epoch 63/500
1/1 - 0s - loss: 2.9774 - accuracy: 0.2381
Epoch 64/500
1/1 - 0s - loss: 2.9736 - accuracy: 0.2381
Epoch 65/500
1/1 - 0s - loss: 2.9696 - accuracy: 0.2381
Epoch 66/500
1/1 - 0s - loss: 2.9655 - accuracy: 0.2381
Epoch 67/500
1/1 - 0s - loss: 2.9613 - accuracy: 0.2381
Epoch 68/500
1/1 - 0s - loss: 2.9570 - accuracy: 0.2381
Epoch 69/500
1/1 - 0s - loss: 2.9526 - accuracy: 0.2381
Epoch 70/500
1/1 - 0s - loss: 2.9480 - accuracy: 0.2857
Epoch 71/500
1/1 - 0s - loss: 2.9433 - accuracy: 0.2857
Epoch 72/500
1/1 - 0s - loss: 2.9385 - accuracy: 0.2857
Epoch 73/500
1/1 - 0s - loss: 2.9336 - accuracy: 0.2857
Epoch 74/500
1/1 - 0s - loss: 2.9285 - accuracy: 0.2857
Epoch 75/500
1/1 - 0s - loss: 2.9233 - accuracy: 0.2857
Epoch 76/500
1/1 - 0s - loss: 2.9179 - accuracy: 0.2857
Epoch 77/500
1/1 - 0s - loss: 2.9125 - accuracy: 0.2857
Epoch 78/500
1/1 - 0s - loss: 2.9068 - accuracy: 0.2857
Epoch 79/500
1/1 - 0s - loss: 2.9010 - accuracy: 0.2857
Epoch 80/500
1/1 - 0s - loss: 2.8951 - accuracy: 0.2857
Epoch 81/500
1/1 - 0s - loss: 2.8890 - accuracy: 0.2857
Epoch 82/500
1/1 - 0s - loss: 2.8828 - accuracy: 0.2857
Epoch 83/500
1/1 - 0s - loss: 2.8764 - accuracy: 0.2857
Epoch 84/500
1/1 - 0s - loss: 2.8698 - accuracy: 0.2857
Epoch 85/500
1/1 - 0s - loss: 2.8631 - accuracy: 0.2857
Epoch 86/500
1/1 - 0s - loss: 2.8562 - accuracy: 0.2857
Epoch 87/500
1/1 - 0s - loss: 2.8492 - accuracy: 0.2857
Epoch 88/500
1/1 - 0s - loss: 2.8420 - accuracy: 0.2857
Epoch 89/500
1/1 - 0s - loss: 2.8346 - accuracy: 0.2857
Epoch 90/500
1/1 - 0s - loss: 2.8270 - accuracy: 0.2857
Epoch 91/500
1/1 - 0s - loss: 2.8193 - accuracy: 0.2857
Epoch 92/500
1/1 - 0s - loss: 2.8114 - accuracy: 0.2857
Epoch 93/500
1/1 - 0s - loss: 2.8033 - accuracy: 0.2857
Epoch 94/500
1/1 - 0s - loss: 2.7951 - accuracy: 0.2857
Epoch 95/500
1/1 - 0s - loss: 2.7866 - accuracy: 0.2857
Epoch 96/500
1/1 - 0s - loss: 2.7780 - accuracy: 0.2857
Epoch 97/500
1/1 - 0s - loss: 2.7692 - accuracy: 0.2857
Epoch 98/500
1/1 - 0s - loss: 2.7602 - accuracy: 0.2857
Epoch 99/500
1/1 - 0s - loss: 2.7510 - accuracy: 0.2857
Epoch 100/500
1/1 - 0s - loss: 2.7416 - accuracy: 0.2857
Epoch 101/500
1/1 - 0s - loss: 2.7321 - accuracy: 0.2857
Epoch 102/500
1/1 - 0s - loss: 2.7224 - accuracy: 0.2857
Epoch 103/500
1/1 - 0s - loss: 2.7124 - accuracy: 0.2857
Epoch 104/500
1/1 - 0s - loss: 2.7023 - accuracy: 0.2857
Epoch 105/500
1/1 - 0s - loss: 2.6920 - accuracy: 0.2857
Epoch 106/500
1/1 - 0s - loss: 2.6816 - accuracy: 0.2857
Epoch 107/500
1/1 - 0s - loss: 2.6709 - accuracy: 0.2857
Epoch 108/500
1/1 - 0s - loss: 2.6601 - accuracy: 0.2857
Epoch 109/500
1/1 - 0s - loss: 2.6491 - accuracy: 0.2857
Epoch 110/500
1/1 - 0s - loss: 2.6379 - accuracy: 0.2857
Epoch 111/500
1/1 - 0s - loss: 2.6266 - accuracy: 0.2857
Epoch 112/500
1/1 - 0s - loss: 2.6151 - accuracy: 0.2857
Epoch 113/500
1/1 - 0s - loss: 2.6034 - accuracy: 0.2857
Epoch 114/500
1/1 - 0s - loss: 2.5915 - accuracy: 0.2857
Epoch 115/500
1/1 - 0s - loss: 2.5795 - accuracy: 0.2857
Epoch 116/500
1/1 - 0s - loss: 2.5673 - accuracy: 0.2857
Epoch 117/500
1/1 - 0s - loss: 2.5550 - accuracy: 0.2857
Epoch 118/500
1/1 - 0s - loss: 2.5426 - accuracy: 0.2857
Epoch 119/500
1/1 - 0s - loss: 2.5300 - accuracy: 0.2857
Epoch 120/500
1/1 - 0s - loss: 2.5172 - accuracy: 0.2857
Epoch 121/500
1/1 - 0s - loss: 2.5044 - accuracy: 0.3333
Epoch 122/500
1/1 - 0s - loss: 2.4914 - accuracy: 0.3333
Epoch 123/500
1/1 - 0s - loss: 2.4782 - accuracy: 0.3333
Epoch 124/500
1/1 - 0s - loss: 2.4650 - accuracy: 0.3333
Epoch 125/500
1/1 - 0s - loss: 2.4517 - accuracy: 0.3810
Epoch 126/500
1/1 - 0s - loss: 2.4382 - accuracy: 0.3810
Epoch 127/500
1/1 - 0s - loss: 2.4247 - accuracy: 0.4286
Epoch 128/500
1/1 - 0s - loss: 2.4111 - accuracy: 0.4286
Epoch 129/500
1/1 - 0s - loss: 2.3974 - accuracy: 0.4286
Epoch 130/500
1/1 - 0s - loss: 2.3836 - accuracy: 0.4286
Epoch 131/500
1/1 - 0s - loss: 2.3697 - accuracy: 0.4762
Epoch 132/500
1/1 - 0s - loss: 2.3558 - accuracy: 0.4762
Epoch 133/500
1/1 - 0s - loss: 2.3419 - accuracy: 0.4762
Epoch 134/500
1/1 - 0s - loss: 2.3278 - accuracy: 0.5238
Epoch 135/500
1/1 - 0s - loss: 2.3138 - accuracy: 0.5238
Epoch 136/500
1/1 - 0s - loss: 2.2997 - accuracy: 0.5238
Epoch 137/500
1/1 - 0s - loss: 2.2856 - accuracy: 0.5238
Epoch 138/500
1/1 - 0s - loss: 2.2714 - accuracy: 0.5238
Epoch 139/500
1/1 - 0s - loss: 2.2572 - accuracy: 0.5238
Epoch 140/500
1/1 - 0s - loss: 2.2431 - accuracy: 0.5238
Epoch 141/500
1/1 - 0s - loss: 2.2289 - accuracy: 0.5238
Epoch 142/500
1/1 - 0s - loss: 2.2147 - accuracy: 0.5238
Epoch 143/500
1/1 - 0s - loss: 2.2005 - accuracy: 0.5238
Epoch 144/500
1/1 - 0s - loss: 2.1863 - accuracy: 0.5238
Epoch 145/500
1/1 - 0s - loss: 2.1721 - accuracy: 0.5238
Epoch 146/500
1/1 - 0s - loss: 2.1579 - accuracy: 0.5238
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 147/500
1/1 - 0s - loss: 2.1438 - accuracy: 0.5238
Epoch 148/500
1/1 - 0s - loss: 2.1296 - accuracy: 0.5238
Epoch 149/500
1/1 - 0s - loss: 2.1155 - accuracy: 0.5238
Epoch 150/500
1/1 - 0s - loss: 2.1014 - accuracy: 0.5238
Epoch 151/500
1/1 - 0s - loss: 2.0874 - accuracy: 0.5238
Epoch 152/500
1/1 - 0s - loss: 2.0733 - accuracy: 0.5238
Epoch 153/500
1/1 - 0s - loss: 2.0593 - accuracy: 0.5238
Epoch 154/500
1/1 - 0s - loss: 2.0454 - accuracy: 0.5238
Epoch 155/500
1/1 - 0s - loss: 2.0314 - accuracy: 0.5238
Epoch 156/500
1/1 - 0s - loss: 2.0175 - accuracy: 0.5238
Epoch 157/500
1/1 - 0s - loss: 2.0037 - accuracy: 0.5238
Epoch 158/500
1/1 - 0s - loss: 1.9898 - accuracy: 0.5714
Epoch 159/500
1/1 - 0s - loss: 1.9760 - accuracy: 0.5714
Epoch 160/500
1/1 - 0s - loss: 1.9622 - accuracy: 0.5714
Epoch 161/500
1/1 - 0s - loss: 1.9485 - accuracy: 0.5714
Epoch 162/500
1/1 - 0s - loss: 1.9348 - accuracy: 0.5714
Epoch 163/500
1/1 - 0s - loss: 1.9211 - accuracy: 0.5714
Epoch 164/500
1/1 - 0s - loss: 1.9075 - accuracy: 0.5714
Epoch 165/500
1/1 - 0s - loss: 1.8939 - accuracy: 0.5714
Epoch 166/500
1/1 - 0s - loss: 1.8803 - accuracy: 0.5714
Epoch 167/500
1/1 - 0s - loss: 1.8668 - accuracy: 0.5714
Epoch 168/500
1/1 - 0s - loss: 1.8533 - accuracy: 0.6190
Epoch 169/500
1/1 - 0s - loss: 1.8398 - accuracy: 0.6190
Epoch 170/500
1/1 - 0s - loss: 1.8263 - accuracy: 0.6190
Epoch 171/500
1/1 - 0s - loss: 1.8129 - accuracy: 0.6190
Epoch 172/500
1/1 - 0s - loss: 1.7995 - accuracy: 0.6190
Epoch 173/500
1/1 - 0s - loss: 1.7861 - accuracy: 0.6190
Epoch 174/500
1/1 - 0s - loss: 1.7728 - accuracy: 0.6190
Epoch 175/500
1/1 - 0s - loss: 1.7595 - accuracy: 0.6190
Epoch 176/500
1/1 - 0s - loss: 1.7462 - accuracy: 0.6667
Epoch 177/500
1/1 - 0s - loss: 1.7329 - accuracy: 0.6667
Epoch 178/500
1/1 - 0s - loss: 1.7196 - accuracy: 0.6667
Epoch 179/500
1/1 - 0s - loss: 1.7064 - accuracy: 0.6667
Epoch 180/500
1/1 - 0s - loss: 1.6932 - accuracy: 0.6667
Epoch 181/500
1/1 - 0s - loss: 1.6800 - accuracy: 0.6667
Epoch 182/500
1/1 - 0s - loss: 1.6668 - accuracy: 0.6667
Epoch 183/500
1/1 - 0s - loss: 1.6537 - accuracy: 0.6667
Epoch 184/500
1/1 - 0s - loss: 1.6406 - accuracy: 0.6667
Epoch 185/500
1/1 - 0s - loss: 1.6275 - accuracy: 0.6667
Epoch 186/500
1/1 - 0s - loss: 1.6144 - accuracy: 0.6667
Epoch 187/500
1/1 - 0s - loss: 1.6013 - accuracy: 0.6667
Epoch 188/500
1/1 - 0s - loss: 1.5883 - accuracy: 0.7143
Epoch 189/500
1/1 - 0s - loss: 1.5753 - accuracy: 0.7619
Epoch 190/500
1/1 - 0s - loss: 1.5623 - accuracy: 0.7619
Epoch 191/500
1/1 - 0s - loss: 1.5493 - accuracy: 0.7619
Epoch 192/500
1/1 - 0s - loss: 1.5364 - accuracy: 0.7619
Epoch 193/500
1/1 - 0s - loss: 1.5235 - accuracy: 0.7619
Epoch 194/500
1/1 - 0s - loss: 1.5106 - accuracy: 0.7619
Epoch 195/500
1/1 - 0s - loss: 1.4977 - accuracy: 0.7619
Epoch 196/500
1/1 - 0s - loss: 1.4849 - accuracy: 0.7619
Epoch 197/500
1/1 - 0s - loss: 1.4721 - accuracy: 0.7619
Epoch 198/500
1/1 - 0s - loss: 1.4593 - accuracy: 0.7619
Epoch 199/500
1/1 - 0s - loss: 1.4466 - accuracy: 0.7619
Epoch 200/500
1/1 - 0s - loss: 1.4338 - accuracy: 0.8095
Epoch 201/500
1/1 - 0s - loss: 1.4212 - accuracy: 0.8095
Epoch 202/500
1/1 - 0s - loss: 1.4085 - accuracy: 0.8095
Epoch 203/500
1/1 - 0s - loss: 1.3959 - accuracy: 0.8095
Epoch 204/500
1/1 - 0s - loss: 1.3833 - accuracy: 0.8095
Epoch 205/500
1/1 - 0s - loss: 1.3708 - accuracy: 0.8095
Epoch 206/500
1/1 - 0s - loss: 1.3583 - accuracy: 0.8095
Epoch 207/500
1/1 - 0s - loss: 1.3459 - accuracy: 0.8095
Epoch 208/500
1/1 - 0s - loss: 1.3335 - accuracy: 0.8095
Epoch 209/500
1/1 - 0s - loss: 1.3211 - accuracy: 0.8095
Epoch 210/500
1/1 - 0s - loss: 1.3088 - accuracy: 0.8095
Epoch 211/500
1/1 - 0s - loss: 1.2966 - accuracy: 0.8095
Epoch 212/500
1/1 - 0s - loss: 1.2844 - accuracy: 0.8095
Epoch 213/500
1/1 - 0s - loss: 1.2722 - accuracy: 0.8095
Epoch 214/500
1/1 - 0s - loss: 1.2601 - accuracy: 0.8095
Epoch 215/500
1/1 - 0s - loss: 1.2481 - accuracy: 0.8095
Epoch 216/500
1/1 - 0s - loss: 1.2361 - accuracy: 0.8095
Epoch 217/500
1/1 - 0s - loss: 1.2242 - accuracy: 0.8095
Epoch 218/500
1/1 - 0s - loss: 1.2123 - accuracy: 0.8095
Epoch 219/500
1/1 - 0s - loss: 1.2005 - accuracy: 0.8095
Epoch 220/500
1/1 - 0s - loss: 1.1888 - accuracy: 0.8095
Epoch 221/500
1/1 - 0s - loss: 1.1772 - accuracy: 0.8095
Epoch 222/500
1/1 - 0s - loss: 1.1656 - accuracy: 0.8095
Epoch 223/500
1/1 - 0s - loss: 1.1541 - accuracy: 0.8095
Epoch 224/500
1/1 - 0s - loss: 1.1427 - accuracy: 0.8095
Epoch 225/500
1/1 - 0s - loss: 1.1313 - accuracy: 0.8571
Epoch 226/500
1/1 - 0s - loss: 1.1200 - accuracy: 0.8571
Epoch 227/500
1/1 - 0s - loss: 1.1088 - accuracy: 0.8571
Epoch 228/500
1/1 - 0s - loss: 1.0977 - accuracy: 0.8571
Epoch 229/500
1/1 - 0s - loss: 1.0867 - accuracy: 0.8571
Epoch 230/500
1/1 - 0s - loss: 1.0757 - accuracy: 0.8571
Epoch 231/500
1/1 - 0s - loss: 1.0649 - accuracy: 0.8571
Epoch 232/500
1/1 - 0s - loss: 1.0541 - accuracy: 0.8571
Epoch 233/500
1/1 - 0s - loss: 1.0434 - accuracy: 0.8571
Epoch 234/500
1/1 - 0s - loss: 1.0328 - accuracy: 0.8571
Epoch 235/500
1/1 - 0s - loss: 1.0223 - accuracy: 0.8571
Epoch 236/500
1/1 - 0s - loss: 1.0119 - accuracy: 0.8571
Epoch 237/500
1/1 - 0s - loss: 1.0015 - accuracy: 0.8571
Epoch 238/500
1/1 - 0s - loss: 0.9913 - accuracy: 0.8571
Epoch 239/500
1/1 - 0s - loss: 0.9812 - accuracy: 0.8571
Epoch 240/500
1/1 - 0s - loss: 0.9711 - accuracy: 0.8571
Epoch 241/500
1/1 - 0s - loss: 0.9612 - accuracy: 0.8571
Epoch 242/500
1/1 - 0s - loss: 0.9513 - accuracy: 0.8571
Epoch 243/500
1/1 - 0s - loss: 0.9416 - accuracy: 0.8571
Epoch 244/500
1/1 - 0s - loss: 0.9319 - accuracy: 0.8571
Epoch 245/500
1/1 - 0s - loss: 0.9223 - accuracy: 0.8571
Epoch 246/500
1/1 - 0s - loss: 0.9129 - accuracy: 0.8571
Epoch 247/500
1/1 - 0s - loss: 0.9035 - accuracy: 0.8571
Epoch 248/500
1/1 - 0s - loss: 0.8943 - accuracy: 0.8571
Epoch 249/500
1/1 - 0s - loss: 0.8851 - accuracy: 0.8571
Epoch 250/500
1/1 - 0s - loss: 0.8760 - accuracy: 0.8571
Epoch 251/500
1/1 - 0s - loss: 0.8671 - accuracy: 0.8571
Epoch 252/500
1/1 - 0s - loss: 0.8582 - accuracy: 0.8571
Epoch 253/500
1/1 - 0s - loss: 0.8495 - accuracy: 0.8571
Epoch 254/500
1/1 - 0s - loss: 0.8408 - accuracy: 0.8571
Epoch 255/500
1/1 - 0s - loss: 0.8322 - accuracy: 0.8571
Epoch 256/500
1/1 - 0s - loss: 0.8238 - accuracy: 0.8571
Epoch 257/500
1/1 - 0s - loss: 0.8154 - accuracy: 0.8571
Epoch 258/500
1/1 - 0s - loss: 0.8071 - accuracy: 0.8571
Epoch 259/500
1/1 - 0s - loss: 0.7990 - accuracy: 0.8571
Epoch 260/500
1/1 - 0s - loss: 0.7909 - accuracy: 0.8571
Epoch 261/500
1/1 - 0s - loss: 0.7829 - accuracy: 0.8571
Epoch 262/500
1/1 - 0s - loss: 0.7750 - accuracy: 0.8571
Epoch 263/500
1/1 - 0s - loss: 0.7673 - accuracy: 0.8571
Epoch 264/500
1/1 - 0s - loss: 0.7596 - accuracy: 0.8571
Epoch 265/500
1/1 - 0s - loss: 0.7520 - accuracy: 0.8571
Epoch 266/500
1/1 - 0s - loss: 0.7445 - accuracy: 0.8571
Epoch 267/500
1/1 - 0s - loss: 0.7371 - accuracy: 0.8571
Epoch 268/500
1/1 - 0s - loss: 0.7298 - accuracy: 0.8571
Epoch 269/500
1/1 - 0s - loss: 0.7226 - accuracy: 0.8571
Epoch 270/500
1/1 - 0s - loss: 0.7155 - accuracy: 0.8571
Epoch 271/500
1/1 - 0s - loss: 0.7085 - accuracy: 0.8571
Epoch 272/500
1/1 - 0s - loss: 0.7016 - accuracy: 0.8571
Epoch 273/500
1/1 - 0s - loss: 0.6948 - accuracy: 0.8571
Epoch 274/500
1/1 - 0s - loss: 0.6880 - accuracy: 0.8571
Epoch 275/500
1/1 - 0s - loss: 0.6814 - accuracy: 0.8571
Epoch 276/500
1/1 - 0s - loss: 0.6748 - accuracy: 0.8571
Epoch 277/500
1/1 - 0s - loss: 0.6683 - accuracy: 0.8571
Epoch 278/500
1/1 - 0s - loss: 0.6620 - accuracy: 0.8571
Epoch 279/500
1/1 - 0s - loss: 0.6557 - accuracy: 0.8571
Epoch 280/500
1/1 - 0s - loss: 0.6494 - accuracy: 0.8571
Epoch 281/500
1/1 - 0s - loss: 0.6433 - accuracy: 0.8571
Epoch 282/500
1/1 - 0s - loss: 0.6373 - accuracy: 0.8571
Epoch 283/500
1/1 - 0s - loss: 0.6313 - accuracy: 0.8571
Epoch 284/500
1/1 - 0s - loss: 0.6255 - accuracy: 0.8571
Epoch 285/500
1/1 - 0s - loss: 0.6197 - accuracy: 0.8571
Epoch 286/500
1/1 - 0s - loss: 0.6140 - accuracy: 0.8571
Epoch 287/500
1/1 - 0s - loss: 0.6084 - accuracy: 0.8571
Epoch 288/500
1/1 - 0s - loss: 0.6028 - accuracy: 0.8571
Epoch 289/500
1/1 - 0s - loss: 0.5974 - accuracy: 0.8571
Epoch 290/500
1/1 - 0s - loss: 0.5920 - accuracy: 0.8571
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 291/500
1/1 - 0s - loss: 0.5867 - accuracy: 0.8571
Epoch 292/500
1/1 - 0s - loss: 0.5815 - accuracy: 0.8571
Epoch 293/500
1/1 - 0s - loss: 0.5763 - accuracy: 0.8571
Epoch 294/500
1/1 - 0s - loss: 0.5712 - accuracy: 0.8571
Epoch 295/500
1/1 - 0s - loss: 0.5663 - accuracy: 0.8571
Epoch 296/500
1/1 - 0s - loss: 0.5613 - accuracy: 0.8571
Epoch 297/500
1/1 - 0s - loss: 0.5565 - accuracy: 0.8571
Epoch 298/500
1/1 - 0s - loss: 0.5517 - accuracy: 0.8571
Epoch 299/500
1/1 - 0s - loss: 0.5470 - accuracy: 0.8571
Epoch 300/500
1/1 - 0s - loss: 0.5424 - accuracy: 0.8571
Epoch 301/500
1/1 - 0s - loss: 0.5378 - accuracy: 0.8571
Epoch 302/500
1/1 - 0s - loss: 0.5333 - accuracy: 0.8571
Epoch 303/500
1/1 - 0s - loss: 0.5289 - accuracy: 0.8571
Epoch 304/500
1/1 - 0s - loss: 0.5245 - accuracy: 0.8571
Epoch 305/500
1/1 - 0s - loss: 0.5203 - accuracy: 0.8571
Epoch 306/500
1/1 - 0s - loss: 0.5160 - accuracy: 0.8571
Epoch 307/500
1/1 - 0s - loss: 0.5119 - accuracy: 0.8571
Epoch 308/500
1/1 - 0s - loss: 0.5078 - accuracy: 0.8571
Epoch 309/500
1/1 - 0s - loss: 0.5038 - accuracy: 0.8571
Epoch 310/500
1/1 - 0s - loss: 0.4998 - accuracy: 0.8571
Epoch 311/500
1/1 - 0s - loss: 0.4959 - accuracy: 0.8571
Epoch 312/500
1/1 - 0s - loss: 0.4921 - accuracy: 0.8571
Epoch 313/500
1/1 - 0s - loss: 0.4883 - accuracy: 0.8571
Epoch 314/500
1/1 - 0s - loss: 0.4846 - accuracy: 0.8571
Epoch 315/500
1/1 - 0s - loss: 0.4809 - accuracy: 0.8571
Epoch 316/500
1/1 - 0s - loss: 0.4773 - accuracy: 0.8571
Epoch 317/500
1/1 - 0s - loss: 0.4738 - accuracy: 0.8571
Epoch 318/500
1/1 - 0s - loss: 0.4703 - accuracy: 0.8571
Epoch 319/500
1/1 - 0s - loss: 0.4668 - accuracy: 0.8571
Epoch 320/500
1/1 - 0s - loss: 0.4635 - accuracy: 0.8571
Epoch 321/500
1/1 - 0s - loss: 0.4602 - accuracy: 0.8571
Epoch 322/500
1/1 - 0s - loss: 0.4569 - accuracy: 0.8571
Epoch 323/500
1/1 - 0s - loss: 0.4537 - accuracy: 0.8571
Epoch 324/500
1/1 - 0s - loss: 0.4505 - accuracy: 0.8571
Epoch 325/500
1/1 - 0s - loss: 0.4474 - accuracy: 0.8571
Epoch 326/500
1/1 - 0s - loss: 0.4443 - accuracy: 0.8571
Epoch 327/500
1/1 - 0s - loss: 0.4413 - accuracy: 0.8571
Epoch 328/500
1/1 - 0s - loss: 0.4384 - accuracy: 0.8571
Epoch 329/500
1/1 - 0s - loss: 0.4354 - accuracy: 0.8571
Epoch 330/500
1/1 - 0s - loss: 0.4326 - accuracy: 0.8571
Epoch 331/500
1/1 - 0s - loss: 0.4298 - accuracy: 0.8571
Epoch 332/500
1/1 - 0s - loss: 0.4270 - accuracy: 0.8571
Epoch 333/500
1/1 - 0s - loss: 0.4243 - accuracy: 0.8571
Epoch 334/500
1/1 - 0s - loss: 0.4216 - accuracy: 0.8571
Epoch 335/500
1/1 - 0s - loss: 0.4189 - accuracy: 0.8571
Epoch 336/500
1/1 - 0s - loss: 0.4164 - accuracy: 0.8571
Epoch 337/500
1/1 - 0s - loss: 0.4138 - accuracy: 0.8571
Epoch 338/500
1/1 - 0s - loss: 0.4113 - accuracy: 0.8571
Epoch 339/500
1/1 - 0s - loss: 0.4088 - accuracy: 0.8571
Epoch 340/500
1/1 - 0s - loss: 0.4064 - accuracy: 0.8571
Epoch 341/500
1/1 - 0s - loss: 0.4040 - accuracy: 0.8571
Epoch 342/500
1/1 - 0s - loss: 0.4017 - accuracy: 0.8571
Epoch 343/500
1/1 - 0s - loss: 0.3994 - accuracy: 0.8571
Epoch 344/500
1/1 - 0s - loss: 0.3971 - accuracy: 0.8571
Epoch 345/500
1/1 - 0s - loss: 0.3948 - accuracy: 0.8571
Epoch 346/500
1/1 - 0s - loss: 0.3927 - accuracy: 0.8571
Epoch 347/500
1/1 - 0s - loss: 0.3905 - accuracy: 0.8571
Epoch 348/500
1/1 - 0s - loss: 0.3884 - accuracy: 0.8571
Epoch 349/500
1/1 - 0s - loss: 0.3863 - accuracy: 0.8571
Epoch 350/500
1/1 - 0s - loss: 0.3842 - accuracy: 0.8571
Epoch 351/500
1/1 - 0s - loss: 0.3822 - accuracy: 0.8571
Epoch 352/500
1/1 - 0s - loss: 0.3802 - accuracy: 0.8571
Epoch 353/500
1/1 - 0s - loss: 0.3783 - accuracy: 0.8571
Epoch 354/500
1/1 - 0s - loss: 0.3763 - accuracy: 0.8571
Epoch 355/500
1/1 - 0s - loss: 0.3744 - accuracy: 0.8571
Epoch 356/500
1/1 - 0s - loss: 0.3726 - accuracy: 0.8571
Epoch 357/500
1/1 - 0s - loss: 0.3707 - accuracy: 0.8571
Epoch 358/500
1/1 - 0s - loss: 0.3689 - accuracy: 0.8571
Epoch 359/500
1/1 - 0s - loss: 0.3672 - accuracy: 0.8571
Epoch 360/500
1/1 - 0s - loss: 0.3654 - accuracy: 0.8571
Epoch 361/500
1/1 - 0s - loss: 0.3637 - accuracy: 0.8571
Epoch 362/500
1/1 - 0s - loss: 0.3620 - accuracy: 0.8571
Epoch 363/500
1/1 - 0s - loss: 0.3604 - accuracy: 0.8571
Epoch 364/500
1/1 - 0s - loss: 0.3587 - accuracy: 0.8571
Epoch 365/500
1/1 - 0s - loss: 0.3571 - accuracy: 0.8571
Epoch 366/500
1/1 - 0s - loss: 0.3556 - accuracy: 0.8571
Epoch 367/500
1/1 - 0s - loss: 0.3540 - accuracy: 0.8571
Epoch 368/500
1/1 - 0s - loss: 0.3525 - accuracy: 0.8571
Epoch 369/500
1/1 - 0s - loss: 0.3510 - accuracy: 0.8571
Epoch 370/500
1/1 - 0s - loss: 0.3495 - accuracy: 0.8571
Epoch 371/500
1/1 - 0s - loss: 0.3480 - accuracy: 0.8571
Epoch 372/500
1/1 - 0s - loss: 0.3466 - accuracy: 0.8571
Epoch 373/500
1/1 - 0s - loss: 0.3452 - accuracy: 0.8571
Epoch 374/500
1/1 - 0s - loss: 0.3438 - accuracy: 0.8571
Epoch 375/500
1/1 - 0s - loss: 0.3424 - accuracy: 0.8571
Epoch 376/500
1/1 - 0s - loss: 0.3411 - accuracy: 0.8571
Epoch 377/500
1/1 - 0s - loss: 0.3398 - accuracy: 0.8571
Epoch 378/500
1/1 - 0s - loss: 0.3385 - accuracy: 0.8571
Epoch 379/500
1/1 - 0s - loss: 0.3372 - accuracy: 0.8571
Epoch 380/500
1/1 - 0s - loss: 0.3359 - accuracy: 0.8571
Epoch 381/500
1/1 - 0s - loss: 0.3347 - accuracy: 0.8571
Epoch 382/500
1/1 - 0s - loss: 0.3335 - accuracy: 0.8571
Epoch 383/500
1/1 - 0s - loss: 0.3322 - accuracy: 0.8571
Epoch 384/500
1/1 - 0s - loss: 0.3311 - accuracy: 0.8571
Epoch 385/500
1/1 - 0s - loss: 0.3299 - accuracy: 0.8571
Epoch 386/500
1/1 - 0s - loss: 0.3287 - accuracy: 0.8571
Epoch 387/500
1/1 - 0s - loss: 0.3276 - accuracy: 0.8571
Epoch 388/500
1/1 - 0s - loss: 0.3265 - accuracy: 0.8571
Epoch 389/500
1/1 - 0s - loss: 0.3254 - accuracy: 0.8571
Epoch 390/500
1/1 - 0s - loss: 0.3243 - accuracy: 0.8571
Epoch 391/500
1/1 - 0s - loss: 0.3233 - accuracy: 0.8571
Epoch 392/500
1/1 - 0s - loss: 0.3222 - accuracy: 0.8571
Epoch 393/500
1/1 - 0s - loss: 0.3212 - accuracy: 0.8571
Epoch 394/500
1/1 - 0s - loss: 0.3202 - accuracy: 0.8571
Epoch 395/500
1/1 - 0s - loss: 0.3192 - accuracy: 0.8571
Epoch 396/500
1/1 - 0s - loss: 0.3182 - accuracy: 0.8571
Epoch 397/500
1/1 - 0s - loss: 0.3172 - accuracy: 0.8571
Epoch 398/500
1/1 - 0s - loss: 0.3162 - accuracy: 0.8571
Epoch 399/500
1/1 - 0s - loss: 0.3153 - accuracy: 0.8571
Epoch 400/500
1/1 - 0s - loss: 0.3144 - accuracy: 0.8571
Epoch 401/500
1/1 - 0s - loss: 0.3134 - accuracy: 0.8571
Epoch 402/500
1/1 - 0s - loss: 0.3125 - accuracy: 0.8571
Epoch 403/500
1/1 - 0s - loss: 0.3117 - accuracy: 0.8571
Epoch 404/500
1/1 - 0s - loss: 0.3108 - accuracy: 0.8571
Epoch 405/500
1/1 - 0s - loss: 0.3099 - accuracy: 0.8571
Epoch 406/500
1/1 - 0s - loss: 0.3091 - accuracy: 0.8571
Epoch 407/500
1/1 - 0s - loss: 0.3082 - accuracy: 0.8571
Epoch 408/500
1/1 - 0s - loss: 0.3074 - accuracy: 0.8571
Epoch 409/500
1/1 - 0s - loss: 0.3066 - accuracy: 0.8571
Epoch 410/500
1/1 - 0s - loss: 0.3058 - accuracy: 0.8571
Epoch 411/500
1/1 - 0s - loss: 0.3050 - accuracy: 0.8571
Epoch 412/500
1/1 - 0s - loss: 0.3042 - accuracy: 0.8571
Epoch 413/500
1/1 - 0s - loss: 0.3034 - accuracy: 0.8571
Epoch 414/500
1/1 - 0s - loss: 0.3027 - accuracy: 0.8571
Epoch 415/500
1/1 - 0s - loss: 0.3019 - accuracy: 0.8571
Epoch 416/500
1/1 - 0s - loss: 0.3012 - accuracy: 0.8571
Epoch 417/500
1/1 - 0s - loss: 0.3004 - accuracy: 0.8571
Epoch 418/500
1/1 - 0s - loss: 0.2997 - accuracy: 0.8571
Epoch 419/500
1/1 - 0s - loss: 0.2990 - accuracy: 0.8571
Epoch 420/500
1/1 - 0s - loss: 0.2983 - accuracy: 0.8571
Epoch 421/500
1/1 - 0s - loss: 0.2976 - accuracy: 0.8571
Epoch 422/500
1/1 - 0s - loss: 0.2969 - accuracy: 0.8571
Epoch 423/500
1/1 - 0s - loss: 0.2963 - accuracy: 0.8571
Epoch 424/500
1/1 - 0s - loss: 0.2956 - accuracy: 0.8571
Epoch 425/500
1/1 - 0s - loss: 0.2950 - accuracy: 0.8571
Epoch 426/500
1/1 - 0s - loss: 0.2943 - accuracy: 0.8571
Epoch 427/500
1/1 - 0s - loss: 0.2937 - accuracy: 0.8571
Epoch 428/500
1/1 - 0s - loss: 0.2931 - accuracy: 0.8571
Epoch 429/500
1/1 - 0s - loss: 0.2924 - accuracy: 0.8571
Epoch 430/500
1/1 - 0s - loss: 0.2918 - accuracy: 0.8571
Epoch 431/500
1/1 - 0s - loss: 0.2912 - accuracy: 0.8571
Epoch 432/500
1/1 - 0s - loss: 0.2906 - accuracy: 0.8571
Epoch 433/500
1/1 - 0s - loss: 0.2900 - accuracy: 0.8571
Epoch 434/500
1/1 - 0s - loss: 0.2895 - accuracy: 0.8571
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 435/500
1/1 - 0s - loss: 0.2889 - accuracy: 0.8571
Epoch 436/500
1/1 - 0s - loss: 0.2883 - accuracy: 0.8571
Epoch 437/500
1/1 - 0s - loss: 0.2878 - accuracy: 0.8571
Epoch 438/500
1/1 - 0s - loss: 0.2872 - accuracy: 0.8571
Epoch 439/500
1/1 - 0s - loss: 0.2867 - accuracy: 0.8571
Epoch 440/500
1/1 - 0s - loss: 0.2861 - accuracy: 0.8571
Epoch 441/500
1/1 - 0s - loss: 0.2856 - accuracy: 0.8571
Epoch 442/500
1/1 - 0s - loss: 0.2851 - accuracy: 0.8571
Epoch 443/500
1/1 - 0s - loss: 0.2846 - accuracy: 0.8571
Epoch 444/500
1/1 - 0s - loss: 0.2840 - accuracy: 0.8571
Epoch 445/500
1/1 - 0s - loss: 0.2835 - accuracy: 0.8571
Epoch 446/500
1/1 - 0s - loss: 0.2830 - accuracy: 0.8571
Epoch 447/500
1/1 - 0s - loss: 0.2826 - accuracy: 0.8571
Epoch 448/500
1/1 - 0s - loss: 0.2821 - accuracy: 0.8571
Epoch 449/500
1/1 - 0s - loss: 0.2816 - accuracy: 0.8571
Epoch 450/500
1/1 - 0s - loss: 0.2811 - accuracy: 0.8571
Epoch 451/500
1/1 - 0s - loss: 0.2806 - accuracy: 0.8571
Epoch 452/500
1/1 - 0s - loss: 0.2802 - accuracy: 0.8571
Epoch 453/500
1/1 - 0s - loss: 0.2797 - accuracy: 0.8571
Epoch 454/500
1/1 - 0s - loss: 0.2793 - accuracy: 0.8571
Epoch 455/500
1/1 - 0s - loss: 0.2788 - accuracy: 0.8571
Epoch 456/500
1/1 - 0s - loss: 0.2784 - accuracy: 0.8571
Epoch 457/500
1/1 - 0s - loss: 0.2780 - accuracy: 0.8571
Epoch 458/500
1/1 - 0s - loss: 0.2775 - accuracy: 0.8571
Epoch 459/500
1/1 - 0s - loss: 0.2771 - accuracy: 0.8571
Epoch 460/500
1/1 - 0s - loss: 0.2767 - accuracy: 0.8571
Epoch 461/500
1/1 - 0s - loss: 0.2763 - accuracy: 0.8571
Epoch 462/500
1/1 - 0s - loss: 0.2759 - accuracy: 0.8571
Epoch 463/500
1/1 - 0s - loss: 0.2754 - accuracy: 0.8571
Epoch 464/500
1/1 - 0s - loss: 0.2750 - accuracy: 0.8571
Epoch 465/500
1/1 - 0s - loss: 0.2747 - accuracy: 0.8571
Epoch 466/500
1/1 - 0s - loss: 0.2743 - accuracy: 0.8571
Epoch 467/500
1/1 - 0s - loss: 0.2739 - accuracy: 0.8571
Epoch 468/500
1/1 - 0s - loss: 0.2735 - accuracy: 0.8571
Epoch 469/500
1/1 - 0s - loss: 0.2731 - accuracy: 0.8571
Epoch 470/500
1/1 - 0s - loss: 0.2727 - accuracy: 0.8571
Epoch 471/500
1/1 - 0s - loss: 0.2724 - accuracy: 0.8571
Epoch 472/500
1/1 - 0s - loss: 0.2720 - accuracy: 0.8571
Epoch 473/500
1/1 - 0s - loss: 0.2716 - accuracy: 0.8571
Epoch 474/500
1/1 - 0s - loss: 0.2713 - accuracy: 0.8571
Epoch 475/500
1/1 - 0s - loss: 0.2709 - accuracy: 0.8571
Epoch 476/500
1/1 - 0s - loss: 0.2706 - accuracy: 0.8571
Epoch 477/500
1/1 - 0s - loss: 0.2702 - accuracy: 0.8571
Epoch 478/500
1/1 - 0s - loss: 0.2699 - accuracy: 0.8571
Epoch 479/500
1/1 - 0s - loss: 0.2696 - accuracy: 0.8571
Epoch 480/500
1/1 - 0s - loss: 0.2692 - accuracy: 0.8571
Epoch 481/500
1/1 - 0s - loss: 0.2689 - accuracy: 0.8571
Epoch 482/500
1/1 - 0s - loss: 0.2686 - accuracy: 0.8571
Epoch 483/500
1/1 - 0s - loss: 0.2682 - accuracy: 0.8571
Epoch 484/500
1/1 - 0s - loss: 0.2679 - accuracy: 0.8571
Epoch 485/500
1/1 - 0s - loss: 0.2676 - accuracy: 0.8571
Epoch 486/500
1/1 - 0s - loss: 0.2673 - accuracy: 0.8571
Epoch 487/500
1/1 - 0s - loss: 0.2670 - accuracy: 0.8571
Epoch 488/500
1/1 - 0s - loss: 0.2667 - accuracy: 0.8571
Epoch 489/500
1/1 - 0s - loss: 0.2664 - accuracy: 0.8571
Epoch 490/500
1/1 - 0s - loss: 0.2661 - accuracy: 0.8571
Epoch 491/500
1/1 - 0s - loss: 0.2658 - accuracy: 0.8571
Epoch 492/500
1/1 - 0s - loss: 0.2655 - accuracy: 0.8571
Epoch 493/500
1/1 - 0s - loss: 0.2652 - accuracy: 0.8571
Epoch 494/500
1/1 - 0s - loss: 0.2649 - accuracy: 0.8571
Epoch 495/500
1/1 - 0s - loss: 0.2646 - accuracy: 0.8571
Epoch 496/500
1/1 - 0s - loss: 0.2643 - accuracy: 0.8571
Epoch 497/500
1/1 - 0s - loss: 0.2640 - accuracy: 0.8571
Epoch 498/500
1/1 - 0s - loss: 0.2638 - accuracy: 0.8571
Epoch 499/500
1/1 - 0s - loss: 0.2635 - accuracy: 0.8571
Epoch 500/500
1/1 - 0s - loss: 0.2632 - accuracy: 0.8571
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x7ff300c072e8&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-language-model-primer_24_0.png" src="../_images/dl-neural-language-model-primer_24_0.png" />
</div>
</div>
</div>
<div class="section" id="text-generation-using-the-model">
<h3>Text Generation Using the Model<a class="headerlink" href="#text-generation-using-the-model" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>After we trained the bigram-based LM, we can use the model for text generation.</p></li>
<li><p>we can implement a simple text generator: the model always outputs the next-word that has the highest predicted probability values.</p></li>
<li><p>At every timestep, the model will use the newly predicted word as the input for another next-word prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a sequence from the model</span>
<span class="k">def</span> <span class="nf">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
    <span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">seed_text</span>
    <span class="c1"># generate a fixed number of words</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="c1"># encode the text as integer</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">in_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        
        <span class="c1"># predict a word in the vocabulary</span>
        <span class="n">yhat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># map predicted word index to word</span>
        <span class="n">out_word</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">:</span>
                <span class="n">out_word</span> <span class="o">=</span> <span class="n">word</span>
                <span class="k">break</span>
        <span class="c1"># append to input</span>
        <span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">out_word</span><span class="p">,</span> <span class="n">result</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">out_word</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When we generate the output sequence, we use a <strong>greedy search</strong>, which selects the most likely word at each time step in the output sequence. While this approach features its efficiency, the quality of the final output sequences may not be necessarily optimal.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s1">&#39;Jill&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jill came tumbling after and jill came tumbling after and jill
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sampling-strategies-for-text-generation">
<h3>Sampling Strategies for Text Generation<a class="headerlink" href="#sampling-strategies-for-text-generation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Given a trained language model and a seed text chunk, we can generate new text by greedy-search like we’ve seen above.</p></li>
<li><p>But we may sometimes like to add a bit vibe to the robotic texts.</p></li>
<li><p>Possible alternatives:</p>
<ul>
<li><p>We can re-normalize the predicted probability distributions of all next-words to reduce probability differences between the highest and the lowest. (Please see Ch.8.1 Text Generation with LSTM in Chollet’s Deep Learning with Python. You will need this for the assignment.)</p></li>
<li><p>We can use non-greedy search by keeping the top <em>k</em> probable candidates in the list for next-word prediction. (cf. <strong>Beam Search</strong> below).</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="beam-search-skipped">
<h2><span class="section-number">2.3. </span>Beam Search (skipped)<a class="headerlink" href="#beam-search-skipped" title="Permalink to this headline">¶</a></h2>
<div class="section" id="searching-in-nlp">
<h3>Searching in NLP<a class="headerlink" href="#searching-in-nlp" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In the previous demonstration, when we generate the predicted next word, we adopt a naive approach, i.e., always choosing the word of the highest probability.</p></li>
<li><p>It is common in NLP for models to output a probability distribution over words in the vocabulary.</p></li>
<li><p>This step involves searching through all the possible output sequences based on their likelihood.</p></li>
<li><p>Choosing the next word of highest probability does not guarantee us the most optimal sequence.</p></li>
<li><p>The search problem is exponential in the length of the output sequence given the large size of vocabulary.</p></li>
</ul>
</div>
<div class="section" id="beam-search-decoding">
<h3>Beam Search Decoding<a class="headerlink" href="#beam-search-decoding" title="Permalink to this headline">¶</a></h3>
<p>The beam search expands all possible next steps and keeps the <strong><span class="math notranslate nohighlight">\(k\)</span></strong> most likely, where <strong><span class="math notranslate nohighlight">\(k\)</span></strong> is a researcher-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.</p>
<p>The search process can stop for each candidate independently either by:</p>
<ul class="simple">
<li><p>reaching a maximum length</p></li>
<li><p>reaching an end-of-sequence token</p></li>
<li><p>reaching a threshold likelihood</p></li>
</ul>
<div class="highlight-{note] notranslate"><div class="highlight"><pre><span></span>Please see Jason Brownlee&#39;s blog post [How to Implement a Beam Search Decoder for Natural Language Processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/) for the python implementation.

The following codes are based on Jason&#39;s code.
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The following codes may not work properly. In Beam Search, when the model predicts <code class="docutils literal notranslate"><span class="pre">None</span></code> as the next character, we should set it as a stopping condition. The following codes have not be optimized with respect to this.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a sequence from the model</span>
<span class="k">def</span> <span class="nf">generate_seq_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">in_text</span> <span class="o">=</span> <span class="n">seed_text</span> 
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[[[</span><span class="n">in_text</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">]]</span>
    <span class="c1"># prepare id_2_word map</span>
    <span class="n">id_2_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">([(</span><span class="n">i</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
    
    <span class="c1"># start next-word generating</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="n">all_candidates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>        
        <span class="c1">#print(&quot;Next word &quot;, _+1)</span>
        <span class="c1"># temp list to hold all possible candidates</span>
        <span class="c1"># `sequence + next words`</span>


        <span class="c1"># for each existing sequence</span>
        <span class="c1"># take the last word of the sequence</span>
        <span class="c1"># find probs of all words in the next position</span>
        <span class="c1"># save the top k</span>
        <span class="c1"># all_candidates should have 3 * 22 = 66 candidates</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)):</span>
            <span class="c1"># for the current sequence</span>
            <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>            
            <span class="c1"># next word probablity distribution</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
            <span class="n">model_pred_prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="c1"># compute all probabilities for `curent_sequence + all_possible_next_word`</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_pred_prob</span><span class="p">)):</span>
                <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">id_2_word</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">score</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_pred_prob</span><span class="p">[</span><span class="n">j</span><span class="p">])]</span>
                <span class="n">all_candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>

            <span class="n">all_candidates</span><span class="o">=</span> <span class="p">[[</span><span class="n">seq</span><span class="p">,</span> <span class="n">score</span><span class="p">]</span> <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">all_candidates</span> <span class="k">if</span> <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

            <span class="c1"># order all candidates (seqence + nextword) by score</span>
            <span class="c1">#print(&quot;all_condidates length:&quot;, len(all_candidates))</span>
            <span class="n">ordered</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_candidates</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># default ascending</span>
            <span class="c1"># select k best</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="n">ordered</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="c1">## choose top k</span>

    <span class="k">return</span> <span class="n">sequences</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_seq_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s1">&#39;Jill&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;broke&#39;], 5.70021602883935],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;pail&#39;], 9.975788600742817],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;pail&#39;, &#39;water&#39;],
  10.006022842600942],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;came&#39;], 10.572875507175922],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;came&#39;, &#39;after&#39;], 10.62351381778717],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;tumbling&#39;], 10.633405216038227],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;up&#39;], 10.789379127323627],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;up&#39;, &#39;hill&#39;], 10.838647928088903],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;of&#39;], 11.02272368222475],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;of&#39;, &#39;fell&#39;], 11.054580122232437]]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2><span class="section-number">2.4. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Chollet (2017): Ch 8.1</p></li>
<li><p>Check Jason Brownlee’s blog post <a class="reference external" href="https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/">How to Develop Word-Based Neural Language Models in Python with Keras</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="dl-sequence-models-intuition.html" title="previous page"><span class="section-number">1. </span>Sequence Models Intuition</a>
    <a class='right-next' id="next-link" href="../temp/text-vec-embedding.html" title="next page"><span class="section-number">3. </span>Word Embeddings</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>