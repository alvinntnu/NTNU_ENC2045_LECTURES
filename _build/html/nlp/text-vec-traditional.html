

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Text Vectorization Using Traditional Methods &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/text-vec-traditional';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Machine Learning: Overview" href="ml-overview.html" />
    <link rel="prev" title="Google Colab" href="google-colab.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-neural-network-from-scratch.html">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">Sequence Model with Attention for Addition Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-fine-tuning-bert.html">Transfer Learning Using BERT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/text-vec-traditional.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/text-vec-traditional.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Text Vectorization Using Traditional Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering">Feature Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-necessary-dependencies-and-settings">Import necessary dependencies and settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-corpus-of-text-documents">Sample Corpus of Text Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-preprocessing">Simple Text Preprocessing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words-model">Bag of Words Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#countvectorizer-from-sklearn"><code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-bag-of-words-text-representation">Improving Bag-of-Words Text Representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-bag-of-words-text-representation">N-gram Bag-of-Words Text Representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-model">TF-IDF Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tfidftransformer-from-sklearn"><code class="docutils literal notranslate"><span class="pre">TfidfTransformer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tfidfvectorizer-from-sklearn"><code class="docutils literal notranslate"><span class="pre">TfidfVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-tf-idf-self-study">Intuition of TF-IDF (Self-Study)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-vocabulary-dictionary-of-the-corpus">Create Vocabulary Dictionary of the Corpus</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-document-word-matrix-bag-of-word-frequencies">Create Document-Word Matrix (Bag-of-Word Frequencies)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-document-frequency-of-words">Compute Document Frequency of Words</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-inverse-document-frequency-of-words">Create Inverse Document Frequency of Words</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-raw-tf-idf-for-each-document">Compute Raw TF-IDF for Each Document</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#get-l2-norms-of-tf-idf">Get L2 Norms of TF-IDF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-normalized-tf-idf-for-each-document">Compute Normalized TF-IDF for Each Document</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-similarity">Document Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-distance-metrics-and-intuition">Similarity/Distance Metrics and Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-based-metrics">Distance-based Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-based-metrics">Similarity-based Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-metrics-to-use-then">Which Metrics to Use then?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pairwise-similarity-computation">Pairwise Similarity Computation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-documents-using-similarity-features">Clustering Documents Using Similarity Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-words-using-similarity-features">Clustering Words Using Similarity Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-word-vectors">Notes on Word Vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="text-vectorization-using-traditional-methods">
<h1><a class="toc-backref" href="#id1">Text Vectorization Using Traditional Methods</a><a class="headerlink" href="#text-vectorization-using-traditional-methods" title="Permalink to this headline">#</a></h1>
<p>Traditionally, in linguistic studies, experts would analyze text by <strong>manually labeling</strong> it with linguistic characteristics they deemed relevant. These labels are then converted into numeric values, making it easier to process the text computationally.</p>
<p>However, this approach requires a lot of manual effort. In statistical language processing, the focus is on automating this process by developing techniques to automatically convert text into numeric representations. One popular method for this is the <strong>bag-of-words</strong> approach, which we’ll explore in this tutorial.</p>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#text-vectorization-using-traditional-methods" id="id1">Text Vectorization Using Traditional Methods</a></p>
<ul>
<li><p><a class="reference internal" href="#feature-engineering" id="id2">Feature Engineering</a></p></li>
<li><p><a class="reference internal" href="#import-necessary-dependencies-and-settings" id="id3">Import necessary dependencies and settings</a></p></li>
<li><p><a class="reference internal" href="#sample-corpus-of-text-documents" id="id4">Sample Corpus of Text Documents</a></p></li>
<li><p><a class="reference internal" href="#simple-text-preprocessing" id="id5">Simple Text Preprocessing</a></p></li>
<li><p><a class="reference internal" href="#bag-of-words-model" id="id6">Bag of Words Model</a></p>
<ul>
<li><p><a class="reference internal" href="#countvectorizer-from-sklearn" id="id7"><code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#improving-bag-of-words-text-representation" id="id8">Improving Bag-of-Words Text Representation</a></p>
<ul>
<li><p><a class="reference internal" href="#n-gram-bag-of-words-text-representation" id="id9">N-gram Bag-of-Words Text Representation</a></p></li>
<li><p><a class="reference internal" href="#tf-idf-model" id="id10">TF-IDF Model</a></p></li>
<li><p><a class="reference internal" href="#tfidftransformer-from-sklearn" id="id11"><code class="docutils literal notranslate"><span class="pre">TfidfTransformer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></p></li>
<li><p><a class="reference internal" href="#tfidfvectorizer-from-sklearn" id="id12"><code class="docutils literal notranslate"><span class="pre">TfidfVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></p></li>
<li><p><a class="reference internal" href="#intuition-of-tf-idf-self-study" id="id13">Intuition of TF-IDF (Self-Study)</a></p>
<ul>
<li><p><a class="reference internal" href="#create-vocabulary-dictionary-of-the-corpus" id="id14">Create Vocabulary Dictionary of the Corpus</a></p></li>
<li><p><a class="reference internal" href="#create-document-word-matrix-bag-of-word-frequencies" id="id15">Create Document-Word Matrix (Bag-of-Word Frequencies)</a></p></li>
<li><p><a class="reference internal" href="#compute-document-frequency-of-words" id="id16">Compute Document Frequency of Words</a></p></li>
<li><p><a class="reference internal" href="#create-inverse-document-frequency-of-words" id="id17">Create Inverse Document Frequency of Words</a></p></li>
<li><p><a class="reference internal" href="#compute-raw-tf-idf-for-each-document" id="id18">Compute Raw TF-IDF for Each Document</a></p></li>
<li><p><a class="reference internal" href="#get-l2-norms-of-tf-idf" id="id19">Get L2 Norms of TF-IDF</a></p></li>
<li><p><a class="reference internal" href="#compute-normalized-tf-idf-for-each-document" id="id20">Compute Normalized TF-IDF for Each Document</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#document-similarity" id="id21">Document Similarity</a></p>
<ul>
<li><p><a class="reference internal" href="#similarity-distance-metrics-and-intuition" id="id22">Similarity/Distance Metrics and Intuition</a></p></li>
<li><p><a class="reference internal" href="#distance-based-metrics" id="id23">Distance-based Metrics</a></p></li>
<li><p><a class="reference internal" href="#similarity-based-metrics" id="id24">Similarity-based Metrics</a></p></li>
<li><p><a class="reference internal" href="#which-metrics-to-use-then" id="id25">Which Metrics to Use then?</a></p></li>
<li><p><a class="reference internal" href="#pairwise-similarity-computation" id="id26">Pairwise Similarity Computation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#clustering-documents-using-similarity-features" id="id27">Clustering Documents Using Similarity Features</a></p></li>
<li><p><a class="reference internal" href="#clustering-words-using-similarity-features" id="id28">Clustering Words Using Similarity Features</a></p></li>
<li><p><a class="reference internal" href="#notes-on-word-vectors" id="id29">Notes on Word Vectors</a></p></li>
<li><p><a class="reference internal" href="#references" id="id30">References</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="feature-engineering">
<h2><a class="toc-backref" href="#id2">Feature Engineering</a><a class="headerlink" href="#feature-engineering" title="Permalink to this headline">#</a></h2>
<p>Feature engineering is the process of selecting, transforming, or creating features from raw text data to improve the performance of machine learning models. In the context of natural language processing (NLP), feature engineering involves converting textual data into a <strong>numerical</strong> format that machine learning algorithms can understand and process effectively.</p>
<p>The bag-of-words (BoW) model is a common (yet naive) technique used in feature engineering for NLP. It represents text data as a collection of unique words (or tokens) present in the corpus, ignoring grammar and word order. Each document in the corpus is then represented as a vector, where each dimension corresponds to a unique word in the vocabulary, and the value in each dimension represents the frequency or presence of that word in the document.</p>
<p>So, the connection between feature engineering and bag of words lies in the fact that the bag-of-words model is a method used in feature engineering to convert textual data into a structured numerical format that can be used as input for machine learning algorithms. It allows us to extract meaningful features from text data, making it suitable for tasks such as classification, clustering, and sentiment analysis.</p>
</section>
<section id="import-necessary-dependencies-and-settings">
<h2><a class="toc-backref" href="#id3">Import necessary dependencies and settings</a><a class="headerlink" href="#import-necessary-dependencies-and-settings" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import warnings</span>
<span class="c1"># warnings.filterwarnings(&#39;ignore&#39;)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1">## Default Style Settings</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_colwidth</span> <span class="o">=</span> <span class="mi">200</span>
<span class="c1">#%matplotlib inline</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sample-corpus-of-text-documents">
<h2><a class="toc-backref" href="#id4">Sample Corpus of Text Documents</a><a class="headerlink" href="#sample-corpus-of-text-documents" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>To have a quick intuition of how bag-of-words work, we start with a naive corpus, which consists of only eight short documents. Each document is in fact a simple sentence.</p></li>
<li><p>Each document in the corpus has a label. Let’s assume that the label refers to the <strong>topic</strong> of each document.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## documents</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;The sky is blue and beautiful.&#39;</span><span class="p">,</span> <span class="s1">&#39;Love this blue and beautiful sky!&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The quick brown fox jumps over the lazy dog.&#39;</span><span class="p">,</span>
    <span class="s2">&quot;A king&#39;s breakfast has sausages, ham, bacon, eggs, toast and beans&quot;</span><span class="p">,</span>
    <span class="s1">&#39;I love green eggs, ham, sausages and bacon!&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The brown fox is quick and the blue dog is lazy!&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The sky is very blue and the sky is very beautiful today&#39;</span><span class="p">,</span>
    <span class="s1">&#39;The dog is lazy but the brown fox is quick!&#39;</span>
<span class="p">]</span>

<span class="c1">## labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">,</span> <span class="s1">&#39;weather&#39;</span><span class="p">,</span>
    <span class="s1">&#39;animals&#39;</span>
<span class="p">]</span>

<span class="c1">## DF</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> <span class="c1"># np.array better than list</span>
<span class="n">corpus_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Document&#39;</span><span class="p">:</span> <span class="n">corpus</span><span class="p">,</span> <span class="s1">&#39;Category&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">})</span>
<span class="n">corpus_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-bf418efe-9157-44d5-9c8d-543749184884" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Document</th>
      <th>Category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The sky is blue and beautiful.</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Love this blue and beautiful sky!</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>2</th>
      <td>The quick brown fox jumps over the lazy dog.</td>
      <td>animals</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>
      <td>food</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I love green eggs, ham, sausages and bacon!</td>
      <td>food</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The brown fox is quick and the blue dog is lazy!</td>
      <td>animals</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The sky is very blue and the sky is very beautiful today</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The dog is lazy but the brown fox is quick!</td>
      <td>animals</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-bf418efe-9157-44d5-9c8d-543749184884')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-bf418efe-9157-44d5-9c8d-543749184884 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-bf418efe-9157-44d5-9c8d-543749184884');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-4beb45a1-42fb-45d6-a6b9-ffd2057cac68">
  <button class="colab-df-quickchart" onclick="quickchart('df-4beb45a1-42fb-45d6-a6b9-ffd2057cac68')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-4beb45a1-42fb-45d6-a6b9-ffd2057cac68 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

  <div id="id_a3a5de35-87f6-4d5e-8e57-e679ac23d204">
    <style>
      .colab-df-generate {
        background-color: #E8F0FE;
        border: none;
        border-radius: 50%;
        cursor: pointer;
        display: none;
        fill: #1967D2;
        height: 32px;
        padding: 0 0 0 0;
        width: 32px;
      }

      .colab-df-generate:hover {
        background-color: #E2EBFA;
        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
        fill: #174EA6;
      }

      [theme=dark] .colab-df-generate {
        background-color: #3B4455;
        fill: #D2E3FC;
      }

      [theme=dark] .colab-df-generate:hover {
        background-color: #434B5C;
        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
        fill: #FFFFFF;
      }
    </style>
    <button class="colab-df-generate" onclick="generateWithVariable('corpus_df')"
            title="Generate code using this dataframe."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z"/>
  </svg>
    </button>
    <script>
      (() => {
      const buttonEl =
        document.querySelector('#id_a3a5de35-87f6-4d5e-8e57-e679ac23d204 button.colab-df-generate');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      buttonEl.onclick = () => {
        google.colab.notebook.generateWithVariable('corpus_df');
      }
      })();
    </script>
  </div>

    </div>
  </div>
</div></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In text processing, people often cast <code class="docutils literal notranslate"><span class="pre">list</span></code> into <code class="docutils literal notranslate"><span class="pre">np.array</span></code> for efficiency. A numpy array is a lot faster than the native <code class="docutils literal notranslate"><span class="pre">list</span></code> in Python.</p>
<p>If you are interested, please check this <a class="reference external" href="https://www.youtube.com/watch?v=9JUAPgtkKpI&amp;t=1868s">YouTube Numpy Crash Course</a>.</p>
</div>
</section>
<section id="simple-text-preprocessing">
<h2><a class="toc-backref" href="#id5">Simple Text Preprocessing</a><a class="headerlink" href="#simple-text-preprocessing" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>A few steps for text preprocessing</p>
<ul>
<li><p>Remove special characters</p></li>
<li><p>Normalize letter case</p></li>
<li><p>Remove redundant spaces</p></li>
<li><p>Tokenize each document into word-tokens</p></li>
<li><p>Remove stop words</p></li>
</ul>
</li>
<li><p>All these preprocessing steps are wrapped in one function, <code class="docutils literal notranslate"><span class="pre">normalize_document()</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Colab Only</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;stopwords&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wpt</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordPunctTokenizer</span><span class="p">()</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">normalize_document</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Normalize the document.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - doc (list): A list of documents</span>

<span class="sd">    Returns:</span>
<span class="sd">    list: a list of preprocessed documents</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># lower case and remove special characters\whitespaces</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-zA-Z\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">I</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">A</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="c1"># tokenize document</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">wpt</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="c1"># filter stopwords out of document</span>
    <span class="n">filtered_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="c1"># re-create document from filtered tokens</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filtered_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>


<span class="n">normalize_corpus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">normalize_document</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">norm_corpus</span> <span class="o">=</span> <span class="n">normalize_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">norm_corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;The sky is blue and beautiful.&#39; &#39;Love this blue and beautiful sky!&#39;
 &#39;The quick brown fox jumps over the lazy dog.&#39;
 &quot;A king&#39;s breakfast has sausages, ham, bacon, eggs, toast and beans&quot;
 &#39;I love green eggs, ham, sausages and bacon!&#39;
 &#39;The brown fox is quick and the blue dog is lazy!&#39;
 &#39;The sky is very blue and the sky is very beautiful today&#39;
 &#39;The dog is lazy but the brown fox is quick!&#39;]
==================================================
[&#39;sky blue beautiful&#39; &#39;love blue beautiful sky&#39;
 &#39;quick brown fox jumps lazy dog&#39;
 &#39;kings breakfast sausages ham bacon eggs toast beans&#39;
 &#39;love green eggs ham sausages bacon&#39; &#39;brown fox quick blue dog lazy&#39;
 &#39;sky blue sky beautiful today&#39; &#39;dog lazy brown fox quick&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="bag-of-words-model">
<h2><a class="toc-backref" href="#id6">Bag of Words Model</a><a class="headerlink" href="#bag-of-words-model" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Bag-of-words model is the simplest way (i.e., easy to be automated) to vectorize texts into numeric representations.</p></li>
<li><p>In short, it is a method to represent a text using its word frequency list.</p></li>
</ul>
<p><img alt="" src="../_images/text-representation-bow.gif" /></p>
<section id="countvectorizer-from-sklearn">
<h3><a class="toc-backref" href="#id7"><code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a><a class="headerlink" href="#countvectorizer-from-sklearn" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="c1"># get bag of words features in sparse format</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">cv_matrix</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">norm_corpus</span><span class="p">)</span>
<span class="n">cv_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;8x20 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 42 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># view non-zero feature positions in the sparse matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  (0, 17)	1
  (0, 3)	1
  (0, 2)	1
  (1, 17)	1
  (1, 3)	1
  (1, 2)	1
  (1, 14)	1
  (2, 15)	1
  (2, 5)	1
  (2, 8)	1
  (2, 11)	1
  (2, 13)	1
  (2, 6)	1
  (3, 12)	1
  (3, 4)	1
  (3, 16)	1
  (3, 10)	1
  (3, 0)	1
  (3, 7)	1
  (3, 18)	1
  (3, 1)	1
  (4, 14)	1
  (4, 16)	1
  (4, 10)	1
  (4, 0)	1
  (4, 7)	1
  (4, 9)	1
  (5, 3)	1
  (5, 15)	1
  (5, 5)	1
  (5, 8)	1
  (5, 13)	1
  (5, 6)	1
  (6, 17)	2
  (6, 3)	1
  (6, 2)	1
  (6, 19)	1
  (7, 15)	1
  (7, 5)	1
  (7, 8)	1
  (7, 13)	1
  (7, 6)	1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># view dense representation</span>
<span class="c1"># warning might give a memory error if data is too big</span>
<span class="n">cv_matrix</span> <span class="o">=</span> <span class="n">cv_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">cv_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],
       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],
       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],
       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get all unique words in the corpus</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="c1"># show document feature vectors</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_matrix</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bacon</th>
      <th>beans</th>
      <th>beautiful</th>
      <th>blue</th>
      <th>breakfast</th>
      <th>brown</th>
      <th>dog</th>
      <th>eggs</th>
      <th>fox</th>
      <th>green</th>
      <th>ham</th>
      <th>jumps</th>
      <th>kings</th>
      <th>lazy</th>
      <th>love</th>
      <th>quick</th>
      <th>sausages</th>
      <th>sky</th>
      <th>toast</th>
      <th>today</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Issues with Bag-of-Words Text Representation</p>
<ul>
<li><p><strong>Word order</strong> is ignored.</p></li>
<li><p><strong>Raw</strong> absolute frequency counts of words do not necessarily represent the meaning of the text properly.</p></li>
<li><p><strong>Marginal</strong> frequencies play important roles. (Row and Columns)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="improving-bag-of-words-text-representation">
<h2><a class="toc-backref" href="#id8">Improving Bag-of-Words Text Representation</a><a class="headerlink" href="#improving-bag-of-words-text-representation" title="Permalink to this headline">#</a></h2>
<p>To enhance the Bag of Words (BOW) representation of texts, we can consider the following approaches:</p>
<ol class="arabic simple">
<li><p><strong>Utilize n-grams</strong>: Instead of just using individual words (unigrams), we can include sequences of words (n-grams) in the BOW model. This helps capture partial word order information in the text, which can provide richer semantics.</p></li>
<li><p><strong>Filter words</strong>: We can filter out words based on certain criteria such as their distributional characteristics (e.g., term frequencies, document frqeuencies) or morphosyntactic patterns (e.g., morphological endings). This can help remove noisy or irrelevant words from the BOW representation, making it more focused on the essential semantic content.</p></li>
<li><p><strong>Weighting</strong>: Instead of just counting the frequency of each word in the text, we can apply weights to the raw frequency counts. Various weighting schemes such as Term Frequency-Inverse Document Frequency (TF-IDF) can be employed to assign higher weights to words that are more informative and less common across the entire corpus. This helps prioritize important words and downplay the significance of common words that may not carry much semantic meaning.</p></li>
</ol>
<p>In <code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code>, we can utilize its parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">max_df</span></code>: When building the vocabulary, the vectorizer will ignore terms that have a <strong>document frequency</strong> strictly higher than the given threshold (corpus-specific stop words). <code class="docutils literal notranslate"><span class="pre">float</span></code> = the parameter represents a proportion of documents; <code class="docutils literal notranslate"><span class="pre">integer</span></code> = absolute counts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_df</span></code>: When building the vocabulary, the vectorizer will ignore terms that have a <strong>document frequency</strong> strictly lower than the given threshold. <code class="docutils literal notranslate"><span class="pre">float</span></code> = the parameter represents a proportion of documents; <code class="docutils literal notranslate"><span class="pre">integer</span></code> = absolute counts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_features</span></code> : Build a vocabulary that only consider the top <code class="docutils literal notranslate"><span class="pre">max_features</span></code> ordered by term frequency across the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ngram_range</span></code> : The lower and upper boundary of the range of n-values for different word n-grams. <code class="docutils literal notranslate"><span class="pre">tuple</span></code> (min_n, max_n), default=(1, 1).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">token_pattern</span></code>: Regular expression denoting what constitutes a “token” in vocabulary. The default regexp select tokens of 2 or more alphanumeric characters (Note: <strong>punctuation</strong> is completely ignored and always treated as a token separator). If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">token_pattern</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> is crucial for specifying the pattern used to extract tokens (words) from the text data. By default, <code class="docutils literal notranslate"><span class="pre">token_pattern</span></code> is set to a regular expression that captures words consisting of 2 or more alphanumeric characters. This default setting works well for English text where words are typically separated by whitespace and punctuation.</p>
<p>However, when working with languages like Chinese, where words are not separated by whitespace, it becomes necessary to adjust the <code class="docutils literal notranslate"><span class="pre">token_pattern</span></code> parameter to ensure that the tokenizer recognizes the individual words correctly. In the case of Chinese text data that has been word-segmented (i.e., split into individual words), specifying the appropriate <code class="docutils literal notranslate"><span class="pre">token_pattern</span></code> is essential for preserving the integrity of the original word tokens.</p>
<p>Without specifying the <code class="docutils literal notranslate"><span class="pre">token_pattern</span></code>, the default behavior of <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> may not correctly identify the segmented words in Chinese text, leading to incorrect tokenization and potentially impacting the accuracy of the analysis or modeling tasks performed using the vectorized data.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jieba</span>

<span class="c1">## raw corpus (list)</span>
<span class="n">corpus_zh</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;對這個世界如果你有太多的抱怨&#39;</span><span class="p">,</span>
    <span class="s1">&#39;跌倒了就不敢繼續往前走&#39;</span><span class="p">,</span>
    <span class="s1">&#39;為什麼人要這麼的脆弱墮落&#39;</span><span class="p">,</span>
    <span class="s1">&#39;請你打開電視看看&#39;</span><span class="p">,</span>
    <span class="s1">&#39;多少人為生命在努力勇敢的走下去&#39;</span><span class="p">,</span>
    <span class="s1">&#39;我們是不是該知足&#39;</span><span class="p">,</span>
    <span class="s1">&#39;珍惜一切就算沒有擁有&#39;</span>
<span class="p">]</span>

<span class="c1">## np.array</span>
<span class="n">corpus_zh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corpus_zh</span><span class="p">)</span>

<span class="c1">## vectorize</span>
<span class="n">cv_zh</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">cv_zh_matrix</span> <span class="o">=</span> <span class="n">cv_zh</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus_zh</span><span class="p">)</span>

<span class="c1"># get all unique words in the corpus</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">cv_zh</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;多少人為生命在努力勇敢的走下去&#39; &#39;對這個世界如果你有太多的抱怨&#39; &#39;我們是不是該知足&#39; &#39;為什麼人要這麼的脆弱墮落&#39; &#39;珍惜一切就算沒有擁有&#39;
 &#39;請你打開電視看看&#39; &#39;跌倒了就不敢繼續往前走&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Define language/task-specific word tokenizer</span>
<span class="k">def</span> <span class="nf">jieba_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Use Jieba to tokenize the text</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>


<span class="c1">## vectorizer with self-defined tokenizer</span>
<span class="n">cv_zh</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                     <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">jieba_tokenizer</span><span class="p">)</span>
<span class="n">cv_zh_matrix</span> <span class="o">=</span> <span class="n">cv_zh</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus_zh</span><span class="p">)</span>
<span class="c1"># get all unique words in the corpus</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">cv_zh</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/alvinchen/anaconda3/envs/python-notes/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter &#39;token_pattern&#39; will not be used since &#39;tokenizer&#39; is not None&#39;
  warnings.warn(
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/70/qfdgs0k52qj24jtjcz7d0dkm0000gn/T/jieba.cache
Loading model cost 0.261 seconds.
Prefix dict has been built successfully.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;一切&#39; &#39;下去&#39; &#39;不敢&#39; &#39;世界&#39; &#39;了&#39; &#39;人為&#39; &#39;人要&#39; &#39;什麼&#39; &#39;你&#39; &#39;努力&#39; &#39;勇敢&#39; &#39;在&#39; &#39;墮落&#39; &#39;多&#39; &#39;多少&#39;
 &#39;太&#39; &#39;如果&#39; &#39;對&#39; &#39;就&#39; &#39;就算&#39; &#39;往前走&#39; &#39;我們&#39; &#39;打&#39; &#39;抱怨&#39; &#39;擁有&#39; &#39;是不是&#39; &#39;有&#39; &#39;沒有&#39; &#39;為&#39; &#39;珍惜&#39;
 &#39;生命&#39; &#39;的&#39; &#39;看看&#39; &#39;知足&#39; &#39;繼續&#39; &#39;脆弱&#39; &#39;該&#39; &#39;請&#39; &#39;走&#39; &#39;跌倒&#39; &#39;這個&#39; &#39;這麼&#39; &#39;開電視&#39;]
</pre></div>
</div>
</div>
</div>
<section id="n-gram-bag-of-words-text-representation">
<h3><a class="toc-backref" href="#id9">N-gram Bag-of-Words Text Representation</a><a class="headerlink" href="#n-gram-bag-of-words-text-representation" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># you can set the n-gram range to 1,2 to get unigrams as well as bigrams</span>
<span class="n">bv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">bv_matrix</span> <span class="o">=</span> <span class="n">bv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">norm_corpus</span><span class="p">)</span>

<span class="n">bv_matrix</span> <span class="o">=</span> <span class="n">bv_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">bv</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">bv_matrix</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bacon eggs</th>
      <th>beautiful sky</th>
      <th>beautiful today</th>
      <th>blue beautiful</th>
      <th>blue dog</th>
      <th>blue sky</th>
      <th>breakfast sausages</th>
      <th>brown fox</th>
      <th>dog lazy</th>
      <th>eggs ham</th>
      <th>...</th>
      <th>lazy dog</th>
      <th>love blue</th>
      <th>love green</th>
      <th>quick blue</th>
      <th>quick brown</th>
      <th>sausages bacon</th>
      <th>sausages ham</th>
      <th>sky beautiful</th>
      <th>sky blue</th>
      <th>toast beans</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 29 columns</p>
</div></div></div>
</div>
</section>
<section id="tf-idf-model">
<h3><a class="toc-backref" href="#id10">TF-IDF Model</a><a class="headerlink" href="#tf-idf-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>TF-IDF model is an extension of the bag-of-words model, whose main objective is to adjust the raw frequency counts of the lexical features by considering the <strong>dispersion</strong> of the words in the corpus.</p></li>
<li><p><strong>Disperson</strong> refers to how evenly each word/term is distributed across different documents of the corpus.</p></li>
</ul>
<ul class="simple">
<li><p>Interaction between Word Raw Frequency Counts and Dispersion:</p>
<ul>
<li><p>Given a <strong>high-frequency</strong> word:</p>
<ul>
<li><p>If the word is widely dispersed across different documents of the corpus (i.e., <strong>high dispersion</strong>)</p>
<ul>
<li><p>it is more likely to be semantically general.</p></li>
</ul>
</li>
<li><p>If the word is mostly centralized in a limited set of documents in the corpus (i.e., <strong>low dispersion</strong>)</p>
<ul>
<li><p>it is more likely to be topic-specific.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dispersion rates of words can be used as weights for the importance of word frequency counts.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Document Frequency</strong> (<strong>DF</strong>) is an intuitive metric for measuring word dispersion across the corpus. DF refers to the number of documents where the word occurs (at least once).</p></li>
<li><p>The inverse of the DF is referred to as <strong>Inverse Document Frequency</strong> (<strong>IDF</strong>). IDF is usually computed as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\textit{IDF} = 1 + log\frac{N}{1+df}
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All these plus-1’s in the above formula are to avoid potential division-by-zero errors.</p>
</div>
<ul class="simple">
<li><p>The raw absolute frequency counts of words in the BOW model are referred to as <strong>Term Frequency</strong> (<strong>TF</strong>).</p></li>
<li><p>The <strong>TF-IDF</strong> Weighting Scheme:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\textit{TF-IDF}_{normalized} = \frac{tf \times idf}{\sqrt{(tf\times idf)^2}}
\]</div>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">tfidf</span></code> is normalized using the L2 norm, i.e., the Euclidean norm (taking the square root of the sum of the square of <code class="docutils literal notranslate"><span class="pre">tfidf</span></code> metrics).</p></li>
</ul>
<div class="dropdown admonition seealso">
<p class="admonition-title">See also</p>
<p>The L1 and L2 norms are mathematical techniques used in machine learning for regularization, specifically in the context of linear models like linear regression or logistic regression. They are used to penalize the size of the coefficients (weights) of the model to prevent overfitting and improve generalization performance.</p>
<ol class="arabic simple">
<li><p><strong>L1 Norm (Lasso Regularization)</strong>:</p>
<ul class="simple">
<li><p>The L1 norm calculates the absolute values of the coefficients and sums them up. It is also known as the Manhattan distance or taxicab norm.</p></li>
<li><p>The L1 norm tends to drive some weights to exactly 0 during the optimization process. This induces sparsity in the weights, meaning that some features are completely ignored in the model. As a result, L1 regularization can be beneficial for memory efficiency and feature selection. It helps in reducing the model complexity by eliminating irrelevant features and focusing only on the most important ones.</p></li>
</ul>
</li>
<li><p><strong>L2 Norm (Ridge Regularization)</strong>:</p>
<ul class="simple">
<li><p>The L2 norm calculates the square of the coefficients, sums them up, and takes the square root of the result. It is also known as the Euclidean distance or the Frobenius norm.</p></li>
<li><p>Unlike the L1 norm, the L2 norm does not force the weights to be exactly 0 but instead reduces them towards 0. It penalizes large weights more severely than small ones, but it rarely forces them to be exactly 0.</p></li>
<li><p>The L2 norm regularization is less aggressive in inducing sparsity compared to L1 regularization. It is generally used when we want to retain all parameters and avoid overfitting by preventing any of the weights from becoming too large.</p></li>
</ul>
</li>
</ol>
<p>In summary, L1 regularization tends to produce sparse models by driving some coefficients to exactly 0, while L2 regularization reduces the magnitude of all coefficients without necessarily eliminating any of them. The choice between L1 and L2 regularization depends on the specific problem and the desired properties of the resulting model.</p>
<ul class="simple">
<li><p>The L1 norm will drive some weights to 0, inducing sparsity in the weights. This can be beneficial for memory efficiency or when feature selection is needed (i.e., we want to select only certain weights).</p></li>
<li><p>The L2 norm instead will reduce all weights but not all the way to 0. This is less memory efficient but can be useful if we want/need to retain all parameters.</p></li>
</ul>
</div>
</section>
<section id="tfidftransformer-from-sklearn">
<h3><a class="toc-backref" href="#id11"><code class="docutils literal notranslate"><span class="pre">TfidfTransformer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a><a class="headerlink" href="#tfidftransformer-from-sklearn" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfTransformer</span>

<span class="n">tt</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">smooth_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tt_matrix</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">cv_matrix</span><span class="p">)</span>

<span class="n">tt_matrix</span> <span class="o">=</span> <span class="n">tt_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tt_matrix</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bacon</th>
      <th>beans</th>
      <th>beautiful</th>
      <th>blue</th>
      <th>breakfast</th>
      <th>brown</th>
      <th>dog</th>
      <th>eggs</th>
      <th>fox</th>
      <th>green</th>
      <th>ham</th>
      <th>jumps</th>
      <th>kings</th>
      <th>lazy</th>
      <th>love</th>
      <th>quick</th>
      <th>sausages</th>
      <th>sky</th>
      <th>toast</th>
      <th>today</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.60</td>
      <td>0.53</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.60</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.49</td>
      <td>0.43</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.49</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.53</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.32</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.47</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.37</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.36</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.72</td>
      <td>0.00</td>
      <td>0.5</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="tfidfvectorizer-from-sklearn">
<h3><a class="toc-backref" href="#id12"><code class="docutils literal notranslate"><span class="pre">TfidfVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a><a class="headerlink" href="#tfidfvectorizer-from-sklearn" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">tv</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                     <span class="n">max_df</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                     <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span>
                     <span class="n">use_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">smooth_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tv_matrix</span> <span class="o">=</span> <span class="n">tv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">norm_corpus</span><span class="p">)</span>
<span class="n">tv_matrix</span> <span class="o">=</span> <span class="n">tv_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">tv</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tv_matrix</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bacon</th>
      <th>beans</th>
      <th>beautiful</th>
      <th>blue</th>
      <th>breakfast</th>
      <th>brown</th>
      <th>dog</th>
      <th>eggs</th>
      <th>fox</th>
      <th>green</th>
      <th>ham</th>
      <th>jumps</th>
      <th>kings</th>
      <th>lazy</th>
      <th>love</th>
      <th>quick</th>
      <th>sausages</th>
      <th>sky</th>
      <th>toast</th>
      <th>today</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.60</td>
      <td>0.53</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.60</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.49</td>
      <td>0.43</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.49</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.53</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.32</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.47</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.37</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.36</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.72</td>
      <td>0.00</td>
      <td>0.5</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="intuition-of-tf-idf-self-study">
<h3><a class="toc-backref" href="#id13">Intuition of TF-IDF (Self-Study)</a><a class="headerlink" href="#intuition-of-tf-idf-self-study" title="Permalink to this headline">#</a></h3>
<p>The following shows the creation and computation of the TFIDF matrix step by step. Please go over the codes on your own if you are interested.</p>
<section id="create-vocabulary-dictionary-of-the-corpus">
<h4><a class="toc-backref" href="#id14">Create Vocabulary Dictionary of the Corpus</a><a class="headerlink" href="#create-vocabulary-dictionary-of-the-corpus" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get unique words as feature names</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
    <span class="nb">set</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">norm_corpus</span><span class="p">]</span>
         <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]))</span>

<span class="c1"># default dict</span>
<span class="n">def_feature_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">unique_words</span><span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Feature Names:&#39;</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Default Feature Dict:&#39;</span><span class="p">,</span> <span class="n">def_feature_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature Names: [&#39;jumps&#39;, &#39;sky&#39;, &#39;dog&#39;, &#39;today&#39;, &#39;lazy&#39;, &#39;kings&#39;, &#39;breakfast&#39;, &#39;sausages&#39;, &#39;brown&#39;, &#39;blue&#39;, &#39;love&#39;, &#39;beautiful&#39;, &#39;beans&#39;, &#39;eggs&#39;, &#39;fox&#39;, &#39;toast&#39;, &#39;green&#39;, &#39;quick&#39;, &#39;ham&#39;, &#39;bacon&#39;]
Default Feature Dict: {&#39;jumps&#39;: 0, &#39;sky&#39;: 0, &#39;dog&#39;: 0, &#39;today&#39;: 0, &#39;lazy&#39;: 0, &#39;kings&#39;: 0, &#39;breakfast&#39;: 0, &#39;sausages&#39;: 0, &#39;brown&#39;: 0, &#39;blue&#39;: 0, &#39;love&#39;: 0, &#39;beautiful&#39;: 0, &#39;beans&#39;: 0, &#39;eggs&#39;: 0, &#39;fox&#39;: 0, &#39;toast&#39;: 0, &#39;green&#39;: 0, &#39;quick&#39;: 0, &#39;ham&#39;: 0, &#39;bacon&#39;: 0}
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-document-word-matrix-bag-of-word-frequencies">
<h4><a class="toc-backref" href="#id15">Create Document-Word Matrix (Bag-of-Word Frequencies)</a><a class="headerlink" href="#create-document-word-matrix-bag-of-word-frequencies" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="c1"># build bag of words features for each document - term frequencies</span>
<span class="n">bow_features</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">norm_corpus</span><span class="p">:</span>
    <span class="n">bow_feature_doc</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
    <span class="c1"># initialize default corpus dictionary</span>
    <span class="n">all_features</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">def_feature_dict</span><span class="p">)</span>

    <span class="c1"># update default dict with current doc words</span>
    <span class="n">bow_feature_doc</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">all_features</span><span class="p">)</span>

    <span class="c1"># append cur doc dict</span>
    <span class="n">bow_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bow_feature_doc</span><span class="p">)</span>

<span class="n">bow_features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">bow_features</span><span class="p">)</span>
<span class="n">bow_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sky</th>
      <th>blue</th>
      <th>beautiful</th>
      <th>jumps</th>
      <th>dog</th>
      <th>today</th>
      <th>lazy</th>
      <th>kings</th>
      <th>breakfast</th>
      <th>sausages</th>
      <th>brown</th>
      <th>love</th>
      <th>beans</th>
      <th>eggs</th>
      <th>fox</th>
      <th>toast</th>
      <th>green</th>
      <th>quick</th>
      <th>ham</th>
      <th>bacon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="compute-document-frequency-of-words">
<h4><a class="toc-backref" href="#id16">Compute Document Frequency of Words</a><a class="headerlink" href="#compute-document-frequency-of-words" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">bow_features</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># build the document frequency matrix</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">csc_matrix</span><span class="p">(</span><span class="n">bow_features</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">indptr</span><span class="p">)</span>
<span class="c1"># `csc_matrix()` compress `bow_features` into sparse matrix based on columns</span>
<span class="c1"># `csc_matrix.indices` stores the matrix value indices in each column</span>
<span class="c1"># `csc_matrix.indptr` stores the accumulative numbers of values from column-0 to the right-most column</span>

<span class="n">df</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">df</span>  <span class="c1"># adding 1 to smoothen idf later</span>

<span class="c1"># show smoothened document frequencies</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">df</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sky</th>
      <th>blue</th>
      <th>beautiful</th>
      <th>jumps</th>
      <th>dog</th>
      <th>today</th>
      <th>lazy</th>
      <th>kings</th>
      <th>breakfast</th>
      <th>sausages</th>
      <th>brown</th>
      <th>love</th>
      <th>beans</th>
      <th>eggs</th>
      <th>fox</th>
      <th>toast</th>
      <th>green</th>
      <th>quick</th>
      <th>ham</th>
      <th>bacon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4</td>
      <td>5</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>3</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="create-inverse-document-frequency-of-words">
<h4><a class="toc-backref" href="#id17">Create Inverse Document Frequency of Words</a><a class="headerlink" href="#create-inverse-document-frequency-of-words" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute inverse document frequencies for each term</span>
<span class="n">total_docs</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">norm_corpus</span><span class="p">)</span>
<span class="n">idf</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">total_docs</span><span class="p">)</span> <span class="o">/</span> <span class="n">df</span><span class="p">)</span>

<span class="c1"># show smoothened idfs</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">idf</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sky</th>
      <th>blue</th>
      <th>beautiful</th>
      <th>jumps</th>
      <th>dog</th>
      <th>today</th>
      <th>lazy</th>
      <th>kings</th>
      <th>breakfast</th>
      <th>sausages</th>
      <th>brown</th>
      <th>love</th>
      <th>beans</th>
      <th>eggs</th>
      <th>fox</th>
      <th>toast</th>
      <th>green</th>
      <th>quick</th>
      <th>ham</th>
      <th>bacon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.81</td>
      <td>1.59</td>
      <td>1.81</td>
      <td>2.5</td>
      <td>1.81</td>
      <td>2.5</td>
      <td>1.81</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>2.1</td>
      <td>1.81</td>
      <td>2.1</td>
      <td>2.5</td>
      <td>2.1</td>
      <td>1.81</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>1.81</td>
      <td>2.1</td>
      <td>2.1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="compute-raw-tf-idf-for-each-document">
<h4><a class="toc-backref" href="#id18">Compute Raw TF-IDF for Each Document</a><a class="headerlink" href="#compute-raw-tf-idf-for-each-document" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute tfidf feature matrix</span>
<span class="n">tf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bow_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">tf</span> <span class="o">*</span> <span class="n">idf</span>  <span class="c1">## `tf.shape` = (8,20), `idf.shape`=(20,)</span>
<span class="c1"># view raw tfidf feature matrix</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sky</th>
      <th>blue</th>
      <th>beautiful</th>
      <th>jumps</th>
      <th>dog</th>
      <th>today</th>
      <th>lazy</th>
      <th>kings</th>
      <th>breakfast</th>
      <th>sausages</th>
      <th>brown</th>
      <th>love</th>
      <th>beans</th>
      <th>eggs</th>
      <th>fox</th>
      <th>toast</th>
      <th>green</th>
      <th>quick</th>
      <th>ham</th>
      <th>bacon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.81</td>
      <td>1.59</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.81</td>
      <td>1.59</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>2.1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>2.5</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>2.1</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>2.5</td>
      <td>2.1</td>
      <td>0.00</td>
      <td>2.5</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>2.1</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.1</td>
      <td>0.00</td>
      <td>2.1</td>
      <td>0.0</td>
      <td>2.1</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>2.5</td>
      <td>0.00</td>
      <td>2.1</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.00</td>
      <td>1.59</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3.62</td>
      <td>1.59</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>2.5</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.81</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="get-l2-norms-of-tf-idf">
<h4><a class="toc-backref" href="#id19">Get L2 Norms of TF-IDF</a><a class="headerlink" href="#get-l2-norms-of-tf-idf" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># compute L2 norms</span>
<span class="n">norms</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># get the L2 forms of tfidf according to columns</span>

<span class="c1"># print norms for each document</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">norms</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[3.013 3.672 4.761 6.534 5.319 4.35  5.019 4.049]
</pre></div>
</div>
</div>
</div>
</section>
<section id="compute-normalized-tf-idf-for-each-document">
<h4><a class="toc-backref" href="#id20">Compute Normalized TF-IDF for Each Document</a><a class="headerlink" href="#compute-normalized-tf-idf-for-each-document" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute normalized tfidf</span>
<span class="n">norm_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span> <span class="o">/</span> <span class="n">norms</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># show final tfidf feature matrix</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">norm_tfidf</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sky</th>
      <th>blue</th>
      <th>beautiful</th>
      <th>jumps</th>
      <th>dog</th>
      <th>today</th>
      <th>lazy</th>
      <th>kings</th>
      <th>breakfast</th>
      <th>sausages</th>
      <th>brown</th>
      <th>love</th>
      <th>beans</th>
      <th>eggs</th>
      <th>fox</th>
      <th>toast</th>
      <th>green</th>
      <th>quick</th>
      <th>ham</th>
      <th>bacon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.60</td>
      <td>0.53</td>
      <td>0.60</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.49</td>
      <td>0.43</td>
      <td>0.49</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.53</td>
      <td>0.38</td>
      <td>0.0</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.38</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.32</td>
      <td>0.00</td>
      <td>0.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>0.32</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.47</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>0.39</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.00</td>
      <td>0.37</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.0</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.72</td>
      <td>0.32</td>
      <td>0.36</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.5</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.0</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_doc</span> <span class="o">=</span> <span class="s1">&#39;the sky is green today&#39;</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tv</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">new_doc</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="mi">2</span><span class="p">),</span>
             <span class="n">columns</span><span class="o">=</span><span class="n">tv</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bacon</th>
      <th>beans</th>
      <th>beautiful</th>
      <th>blue</th>
      <th>breakfast</th>
      <th>brown</th>
      <th>dog</th>
      <th>eggs</th>
      <th>fox</th>
      <th>green</th>
      <th>ham</th>
      <th>jumps</th>
      <th>kings</th>
      <th>lazy</th>
      <th>love</th>
      <th>quick</th>
      <th>sausages</th>
      <th>sky</th>
      <th>toast</th>
      <th>today</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.63</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.46</td>
      <td>0.0</td>
      <td>0.63</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
</section>
<section id="document-similarity">
<h2><a class="toc-backref" href="#id21">Document Similarity</a><a class="headerlink" href="#document-similarity" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Now each document in our corpus has been transformed into a <strong>vectorized</strong> representation using the naive Bag-of-Words method.</p></li>
<li><p>And we believe that these vectorized representations are indicators of textual <strong>semantics</strong>.</p></li>
<li><p>This vectorized text vectorization allows us to perform mathematical computation of the <strong>semantic relationships</strong> between documents.</p></li>
</ul>
<section id="similarity-distance-metrics-and-intuition">
<h3><a class="toc-backref" href="#id22">Similarity/Distance Metrics and Intuition</a><a class="headerlink" href="#similarity-distance-metrics-and-intuition" title="Permalink to this headline">#</a></h3>
<p>Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically:</p>
<p><img alt="" src="../_images/text-vec.001.jpeg" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">manhattan_distances</span><span class="p">,</span> <span class="n">euclidean_distances</span><span class="p">,</span> <span class="n">cosine_similarity</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xyz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">xyz</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 9],
       [1, 3],
       [5, 1]])
</pre></div>
</div>
</div>
</div>
<p>In Math, there are in general two types of metrics to measure the relationship between vectors: <strong>distance</strong>-based vs. <strong>similarity</strong>-based metrics.</p>
</section>
<section id="distance-based-metrics">
<h3><a class="toc-backref" href="#id23">Distance-based Metrics</a><a class="headerlink" href="#distance-based-metrics" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Many distance measures of vectors are based on the following formula and differ in individual parameter settings.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\big( \sum_{i = 1}^{n}{|x_i - y_i|^y}\big)^{\frac{1}{y}}
\]</div>
<ul class="simple">
<li><p>The <em>n</em> in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.)</p></li>
</ul>
<ul class="simple">
<li><p>When <em>y</em> is set to 2, it computes the famous <strong>Euclidean distance</strong> of two vectors, i.e., the direct spatial distance between two points on the <em>n</em>-dimensional space.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sqrt{\big( \sum_{i = 1}^{n}{|x_i - y_i|^2}\big)}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">xyz</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.        , 6.        , 8.94427191],
       [6.        , 0.        , 4.47213595],
       [8.94427191, 4.47213595, 0.        ]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The geometrical meanings of the Euclidean distance are easy to conceptualize.</p></li>
</ul>
<p><img alt="" src="../_images/text-vec-euclidean.gif" /></p>
</section>
<section id="similarity-based-metrics">
<h3><a class="toc-backref" href="#id24">Similarity-based Metrics</a><a class="headerlink" href="#similarity-based-metrics" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of <strong>correlations</strong>.</p></li>
<li><p>The most commonly used one is <strong>Cosine Similarity</strong>, which can be computed as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
cos(\vec{x},\vec{y}) = \frac{\sum_{i=1}^{n}{x_i\times y_i}}{\sqrt{\sum_{i=1}^{n}x_i^2}\times \sqrt{\sum_{i=1}^{n}y_i^2}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">xyz</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1.        , 0.97780241, 0.30320366],
       [0.97780241, 1.        , 0.49613894],
       [0.30320366, 0.49613894, 1.        ]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The geometric meanings of <strong>cosines</strong> of two vectors are connected to the <strong>arcs</strong> between the vectors.</p></li>
<li><p>The greater their cosine similarity, the smaller the arcs, the closer (i.e., the more similar) they are.</p></li>
</ul>
<p><img alt="" src="../_images/text-vec-similarity2.png" /></p>
<p><img alt="" src="../_images/text-vec-cosine.gif" /></p>
</section>
<section id="which-metrics-to-use-then">
<h3><a class="toc-backref" href="#id25">Which Metrics to Use then?</a><a class="headerlink" href="#which-metrics-to-use-then" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Please note that different metrics may lead to very different results.</p></li>
<li><p>In our earlier examples, if we adopt <strong>euclidean distance</strong>, then y is closer to z than is to x.</p></li>
<li><p>But if we adopt <strong>cosine similarity</strong>, then y is closer to x than is to z.</p></li>
<li><p>The choice of distance/similarity metrics depends on:</p>
<ul>
<li><p>Whether the magnitude of value differences on each dimension of the vectors matters (distance-based preferred)</p></li>
<li><p>Whether the values of each dimension of the vectors co-vary (cosine referred)</p></li>
</ul>
</li>
</ul>
</section>
<section id="pairwise-similarity-computation">
<h3><a class="toc-backref" href="#id26">Pairwise Similarity Computation</a><a class="headerlink" href="#pairwise-similarity-computation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">cosine_similarity</span></code> automatically computes the <strong>pairwise</strong> similarities between the <strong>rows</strong> of the input matrix.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">similarity_doc_matrix</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">tv_matrix</span><span class="p">)</span>
<span class="n">similarity_doc_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarity_doc_matrix</span><span class="p">)</span>
<span class="n">similarity_doc_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.000000</td>
      <td>0.820599</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.192353</td>
      <td>0.817246</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.820599</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.225489</td>
      <td>0.157845</td>
      <td>0.670631</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.791821</td>
      <td>0.000000</td>
      <td>0.850516</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.506866</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.225489</td>
      <td>0.000000</td>
      <td>0.506866</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.192353</td>
      <td>0.157845</td>
      <td>0.791821</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.115488</td>
      <td>0.930989</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.817246</td>
      <td>0.670631</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.115488</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.850516</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.930989</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="clustering-documents-using-similarity-features">
<h2><a class="toc-backref" href="#id27">Clustering Documents Using Similarity Features</a><a class="headerlink" href="#clustering-documents-using-similarity-features" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">similarity_doc_matrix</span><span class="p">,</span> <span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="c1"># pd.DataFrame(Z,</span>
<span class="c1">#              columns=[</span>
<span class="c1">#                  &#39;Document\Cluster 1&#39;, &#39;Document\Cluster 2&#39;, &#39;Distance&#39;,</span>
<span class="c1">#                  &#39;Cluster Size&#39;</span>
<span class="c1">#              ],</span>
<span class="c1">#              dtype=&#39;object&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Hierarchical Clustering Dendrogram&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Data point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">)</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x177c963d0&gt;
</pre></div>
</div>
<img alt="../_images/6f3c36be77005dc3c7e17092d15f69268e959361beed96783f1f9f8e97c468a6.png" src="../_images/6f3c36be77005dc3c7e17092d15f69268e959361beed96783f1f9f8e97c468a6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Convert hierarchical cluster into a flat cluster structure</span>

<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">fcluster</span>
<span class="n">max_dist</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">max_dist</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cluster_labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ClusterLabel&#39;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">corpus_df</span><span class="p">,</span> <span class="n">cluster_labels</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Document</th>
      <th>Category</th>
      <th>ClusterLabel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The sky is blue and beautiful.</td>
      <td>weather</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Love this blue and beautiful sky!</td>
      <td>weather</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>The quick brown fox jumps over the lazy dog.</td>
      <td>animals</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>
      <td>food</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I love green eggs, ham, sausages and bacon!</td>
      <td>food</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The brown fox is quick and the blue dog is lazy!</td>
      <td>animals</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The sky is very blue and the sky is very beautiful today</td>
      <td>weather</td>
      <td>2</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The dog is lazy but the brown fox is quick!</td>
      <td>animals</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="clustering-words-using-similarity-features">
<h2><a class="toc-backref" href="#id28">Clustering Words Using Similarity Features</a><a class="headerlink" href="#clustering-words-using-similarity-features" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>We can also transpose the <code class="docutils literal notranslate"><span class="pre">tv_matrix</span></code> to get a Word-Document matrix.</p></li>
<li><p>Each word can be represented as vectors based on their document distributions.</p></li>
<li><p>Words that are semantically similar tend to show similar distributions.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">similarity_term_matrix</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tv_matrix</span><span class="p">))</span>
<span class="n">similarity_term_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarity_term_matrix</span><span class="p">,</span>
                                  <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
                                  <span class="n">index</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">similarity_term_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sky</th>
      <th>blue</th>
      <th>beautiful</th>
      <th>jumps</th>
      <th>dog</th>
      <th>today</th>
      <th>lazy</th>
      <th>kings</th>
      <th>breakfast</th>
      <th>sausages</th>
      <th>brown</th>
      <th>love</th>
      <th>beans</th>
      <th>eggs</th>
      <th>fox</th>
      <th>toast</th>
      <th>green</th>
      <th>quick</th>
      <th>ham</th>
      <th>bacon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sky</th>
      <td>1.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.775547</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.440615</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>blue</th>
      <td>0.631290</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>beautiful</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>0.899485</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.473517</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.951208</td>
      <td>0.00000</td>
      <td>0.420997</td>
    </tr>
    <tr>
      <th>jumps</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.899485</td>
      <td>1.000000</td>
      <td>0.00000</td>
      <td>0.252766</td>
      <td>0.252766</td>
      <td>0.000000</td>
      <td>0.252766</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.252766</td>
      <td>0.425922</td>
      <td>0.252766</td>
      <td>0.000000</td>
      <td>0.855597</td>
      <td>0.00000</td>
      <td>0.378680</td>
    </tr>
    <tr>
      <th>dog</th>
      <td>0.631290</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>today</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.252766</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>lazy</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.252766</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>kings</th>
      <td>1.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.775547</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.440615</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>breakfast</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.252766</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>sausages</th>
      <td>0.775547</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.775547</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.775547</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.568135</td>
      <td>0.000000</td>
      <td>0.775547</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>brown</th>
      <td>1.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.775547</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.440615</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>love</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.528473</td>
      <td>0.528473</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.00000</td>
      <td>0.528473</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>beans</th>
      <td>0.631290</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>eggs</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.252766</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>fox</th>
      <td>0.440615</td>
      <td>0.00000</td>
      <td>0.473517</td>
      <td>0.425922</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.440615</td>
      <td>0.000000</td>
      <td>0.568135</td>
      <td>0.440615</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.440615</td>
      <td>0.382602</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>toast</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.252766</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.528473</td>
      <td>0.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>green</th>
      <td>1.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.775547</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
      <td>0.440615</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.63129</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>quick</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.951208</td>
      <td>0.855597</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.382602</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.00000</td>
      <td>0.680330</td>
    </tr>
    <tr>
      <th>ham</th>
      <td>0.631290</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.631290</td>
      <td>0.000000</td>
      <td>1.00000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>bacon</th>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.420997</td>
      <td>0.378680</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.680330</td>
      <td>0.00000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z2</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">similarity_term_matrix</span><span class="p">,</span> <span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="c1"># pd.DataFrame(Z2,</span>
<span class="c1">#              columns=[</span>
<span class="c1">#                  &#39;Document\Cluster 1&#39;, &#39;Document\Cluster 2&#39;, &#39;Distance&#39;,</span>
<span class="c1">#                  &#39;Cluster Size&#39;</span>
<span class="c1">#              ],</span>
<span class="c1">#              dtype=&#39;object&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Hierarchical Clustering Dendrogram&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Data point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">)</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">Z2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x177da9040&gt;
</pre></div>
</div>
<img alt="../_images/a7da5d167e9b0c342bcb639611131d9edfc885685e664517ee75de68480fbb3e.png" src="../_images/a7da5d167e9b0c342bcb639611131d9edfc885685e664517ee75de68480fbb3e.png" />
</div>
</div>
</section>
<section id="notes-on-word-vectors">
<h2><a class="toc-backref" href="#id29">Notes on Word Vectors</a><a class="headerlink" href="#notes-on-word-vectors" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>In the previous section, we talk about how we can utilize the <strong>Word-Document Matrix</strong> to create vectorized representations of words for a corpus.</p></li>
<li><p>This initial effort of representing words using their frequency distributions is referred to as a traditional <strong>count-based</strong> approach to word representations.</p></li>
<li><p>This <strong>count-base</strong>d feature engineering strategy can be further sophisticated in several ways:</p>
<ul>
<li><p>We can further limit the Word-Document Matrix to a Word-Word Co-occurrence Matrix, where the counts refer to the number of times when the two words co-occur within a specific window frame.</p></li>
<li><p>We can transform the sparse word vectors in the Word-Document Matrix or Word-Word Co-occurrence Matrix using statistical methods (e.g., Latent Semantic Analysis) and build the dense word vectors.</p></li>
</ul>
</li>
<li><p>It should be noted that the count-based approach relies on the creation of the word distribution for the entire corpus in the first place. This can be difficult when we deal with a large corpus.</p></li>
<li><p>In contrast to the traditional count-based approach, <strong>predicative methods</strong> like neural network based language models try to predict words from their neighboring words by looking at word sequences in the corpus in a piecemeal fashion. Through this process the model learns the distributed representations of words, i.e, word embeddings.</p></li>
<li><p>We will come back to “word embeddings” when we work on the deep learning NLP.</p></li>
<li><p>Recommended Reading: <a class="reference external" href="https://www.aclweb.org/anthology/P14-1023.pdf">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a> by Baroni et al.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Latent Semantic Analysis (aka. Latent Semantic Indexing) learns latent topics by performing a matrix decomposition on the document-term matrix using Singular value decomposition. LSA is typically used as a dimension reduction or noise reducing technique.</p>
</div>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id30">References</a><a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Based on Sarkar (2020), Ch 4 Feature Engineering and Text Representation</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="google-colab.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Google Colab</p>
      </div>
    </a>
    <a class="right-next"
       href="ml-overview.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning: Overview</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering">Feature Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-necessary-dependencies-and-settings">Import necessary dependencies and settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-corpus-of-text-documents">Sample Corpus of Text Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-preprocessing">Simple Text Preprocessing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words-model">Bag of Words Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#countvectorizer-from-sklearn"><code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-bag-of-words-text-representation">Improving Bag-of-Words Text Representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-bag-of-words-text-representation">N-gram Bag-of-Words Text Representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-model">TF-IDF Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tfidftransformer-from-sklearn"><code class="docutils literal notranslate"><span class="pre">TfidfTransformer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tfidfvectorizer-from-sklearn"><code class="docutils literal notranslate"><span class="pre">TfidfVectorizer()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-tf-idf-self-study">Intuition of TF-IDF (Self-Study)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-vocabulary-dictionary-of-the-corpus">Create Vocabulary Dictionary of the Corpus</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-document-word-matrix-bag-of-word-frequencies">Create Document-Word Matrix (Bag-of-Word Frequencies)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-document-frequency-of-words">Compute Document Frequency of Words</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-inverse-document-frequency-of-words">Create Inverse Document Frequency of Words</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-raw-tf-idf-for-each-document">Compute Raw TF-IDF for Each Document</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#get-l2-norms-of-tf-idf">Get L2 Norms of TF-IDF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-normalized-tf-idf-for-each-document">Compute Normalized TF-IDF for Each Document</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-similarity">Document Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-distance-metrics-and-intuition">Similarity/Distance Metrics and Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-based-metrics">Distance-based Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-based-metrics">Similarity-based Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-metrics-to-use-then">Which Metrics to Use then?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pairwise-similarity-computation">Pairwise Similarity Computation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-documents-using-similarity-features">Clustering Documents Using Similarity Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-words-using-similarity-features">Clustering Words Using Similarity Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-word-vectors">Notes on Word Vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>