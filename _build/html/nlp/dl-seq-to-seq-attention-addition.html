

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Sequence Model with Attention for Addition Learning &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/dl-seq-to-seq-attention-addition';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sentiment Classification with Transformer (Self-Study)" href="dl-transformers-keras.html" />
    <link rel="prev" title="Attention and Transformers: Intuitions" href="dl-attention-transformer-intuition.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-common-nlp-tasks.html">Common NLP Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-neural-network-from-scratch.html">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Sequence Model with Attention for Addition Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-transformers-keras.html">Sentiment Classification with Transformer (Self-Study)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
<li class="toctree-l1"><a class="reference internal" href="langchain-introduction.html">Langchain: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="langchain-youtube.html">LangChain Application: YouTube Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-machine-learning.html">5. Assignment V: Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6-topic-modeling.html">6. Assignment VI: Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7-dl-chinese-name-gender.html">7. Assignment VII: Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/8-sentiment-analysis-dl.html">8. Assignment VIII: Sentiment Analysis Using Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/9-neural-language-model.html">9. Assignment IV: Neural Language Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exams</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/final-exam-112.html">Final Exam</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-seq-to-seq-attention-addition.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/dl-seq-to-seq-attention-addition.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sequence Model with Attention for Addition Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-dependencies">Set up Dependencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-hyperparameters">Deep Learning Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split">Train-Test Split</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data Preprocessing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-sequences">Text to Sequences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-considerations-for-decoder-s-input-and-output">Special Considerations for Decoder’s Input and Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequences-to-one-hot-encoding">Sequences to One-Hot Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-indices">Token Indices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1-vanilla-rnn">Model 1 (Vanilla RNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-model">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2-gru">Model 2 (GRU)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-3-birdirectional">Model 3 (Birdirectional)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-4-peeky-decoder">Model 4 (Peeky Decoder)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-5-attention">Model 5 (Attention)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#save-models">Save Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-comparison">Interim Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-model-analysis">Attention Model Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-encoder">Inference Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-decoder">Inference Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-input-sequences">Decoding Input Sequences</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-attention">Plotting Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-on-testing-data">Evaluation on Testing Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sequence-model-with-attention-for-addition-learning">
<h1><a class="toc-backref" href="#id9">Sequence Model with Attention for Addition Learning</a><a class="headerlink" href="#sequence-model-with-attention-for-addition-learning" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>In this unit, we will practice on the sequence-to-sequence model using a naive example of numbers addition.</p></li>
<li><p>The inputs are sequences of two numbers adding together (e.g., 123+23); the outputs are the correct answers, i.e., the sum of the two numbers (i.e., 125).</p></li>
<li><p>This type of sequence model is also referred to as Encoder-Decoder Models.</p></li>
<li><p>This task is to simulate the machine translation task (i.e, the sequence to the left of the equation is the source language while the sequence to the right of the equation is the target language).</p></li>
<li><p>In particular, we will implement not only a vanilla RNN-based sequence-to-sequence model but also a few extended variants of the RNN, including:</p>
<ul>
<li><p>GRU</p></li>
<li><p>Bidirectional GRU</p></li>
<li><p>Peeky Decoder</p></li>
<li><p>Attention-based Decoder</p></li>
</ul>
</li>
</ul>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#sequence-model-with-attention-for-addition-learning" id="id9">Sequence Model with Attention for Addition Learning</a></p>
<ul>
<li><p><a class="reference internal" href="#set-up-dependencies" id="id10">Set up Dependencies</a></p></li>
<li><p><a class="reference internal" href="#deep-learning-hyperparameters" id="id11">Deep Learning Hyperparameters</a></p></li>
<li><p><a class="reference internal" href="#data" id="id12">Data</a></p></li>
<li><p><a class="reference internal" href="#train-test-split" id="id13">Train-Test Split</a></p></li>
<li><p><a class="reference internal" href="#data-preprocessing" id="id14">Data Preprocessing</a></p>
<ul>
<li><p><a class="reference internal" href="#text-to-sequences" id="id15">Text to Sequences</a></p></li>
<li><p><a class="reference internal" href="#special-considerations-for-decoder-s-input-and-output" id="id16">Special Considerations for Decoder’s Input and Output</a></p></li>
<li><p><a class="reference internal" href="#sequences-to-one-hot-encoding" id="id17">Sequences to One-Hot Encoding</a></p></li>
<li><p><a class="reference internal" href="#token-indices" id="id18">Token Indices</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-training" id="id19">Model Training</a></p></li>
<li><p><a class="reference internal" href="#model-1-vanilla-rnn" id="id20">Model 1 (Vanilla RNN)</a></p>
<ul>
<li><p><a class="reference internal" href="#define-model" id="id21">Define Model</a></p></li>
<li><p><a class="reference internal" href="#training" id="id22">Training</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-2-gru" id="id23">Model 2 (GRU)</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id24">Define Model</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id25">Training</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-3-birdirectional" id="id26">Model 3 (Birdirectional)</a></p>
<ul>
<li><p><a class="reference internal" href="#id3" id="id27">Define Model</a></p></li>
<li><p><a class="reference internal" href="#id4" id="id28">Training</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-4-peeky-decoder" id="id29">Model 4 (Peeky Decoder)</a></p>
<ul>
<li><p><a class="reference internal" href="#id5" id="id30">Define Model</a></p></li>
<li><p><a class="reference internal" href="#id6" id="id31">Training</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-5-attention" id="id32">Model 5 (Attention)</a></p>
<ul>
<li><p><a class="reference internal" href="#id7" id="id33">Define Model</a></p></li>
<li><p><a class="reference internal" href="#id8" id="id34">Training</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#save-models" id="id35">Save Models</a></p></li>
<li><p><a class="reference internal" href="#interim-comparison" id="id36">Interim Comparison</a></p></li>
<li><p><a class="reference internal" href="#attention-model-analysis" id="id37">Attention Model Analysis</a></p></li>
<li><p><a class="reference internal" href="#inference" id="id38">Inference</a></p>
<ul>
<li><p><a class="reference internal" href="#inference-encoder" id="id39">Inference Encoder</a></p></li>
<li><p><a class="reference internal" href="#inference-decoder" id="id40">Inference Decoder</a></p></li>
<li><p><a class="reference internal" href="#decoding-input-sequences" id="id41">Decoding Input Sequences</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#plotting-attention" id="id42">Plotting Attention</a></p></li>
<li><p><a class="reference internal" href="#evaluation-on-testing-data" id="id43">Evaluation on Testing Data</a></p></li>
<li><p><a class="reference internal" href="#references" id="id44">References</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="set-up-dependencies">
<h2><a class="toc-backref" href="#id10">Set up Dependencies</a><a class="headerlink" href="#set-up-dependencies" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">tensorflow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">GRU</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">SimpleRNN</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Bidirectional</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Concatenate</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">TimeDistributed</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">RepeatVector</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Attention</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">AdditiveAttention</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">GlobalAveragePooling1D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tensorflow Version: &#39;</span><span class="p">,</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensorflow Version:  2.13.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-learning-hyperparameters">
<h2><a class="toc-backref" href="#id11">Deep Learning Hyperparameters</a><a class="headerlink" href="#deep-learning-hyperparameters" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># Batch size for training.</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># Epochs for training</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># Encoder-Decoder latent dimensions</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="data">
<h2><a class="toc-backref" href="#id12">Data</a><a class="headerlink" href="#data" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Please download the data set from <code class="docutils literal notranslate"><span class="pre">demo_data/addition-student-version.csv</span></code>, where each line is a training sample, consisting of the input sequence (e.g., <code class="docutils literal notranslate"><span class="pre">16+75</span></code>) and the target sequence (e.g., <code class="docutils literal notranslate"><span class="pre">91</span></code>) separated by a comma.</p></li>
<li><p>We load the data and add initial and ending token to all the target sequences (<code class="docutils literal notranslate"><span class="pre">_</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## download the dataset</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;../../../RepositoryData/data/deep-learning-2/addition-student-version.csv&#39;</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">## cleaning</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span> <span class="k">if</span> <span class="n">l</span><span class="o">!=</span><span class="s1">&#39;&#39;</span><span class="p">]</span>
    
<span class="c1">## split input and target</span>
<span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
<span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>


<span class="c1">## add special tokens for targets</span>
<span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">sent</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">target_texts</span><span class="p">]</span>


<span class="c1">## randomize</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_texts</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">inds</span><span class="p">)</span>

<span class="c1">## checking</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Data Size:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_texts</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;16+75&#39;, &#39;52+607&#39;, &#39;75+22&#39;, &#39;63+22&#39;, &#39;795+3&#39;]
[&#39;_91_&#39;, &#39;_659_&#39;, &#39;_97_&#39;, &#39;_85_&#39;, &#39;_798_&#39;]
Data Size: 50000
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-test-split">
<h2><a class="toc-backref" href="#id13">Train-Test Split</a><a class="headerlink" href="#train-test-split" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_test_ratio</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_test_ratio</span><span class="p">))</span>
<span class="n">train_inds</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>
<span class="n">test_inds</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>

<span class="n">tr_input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">train_inds</span><span class="p">]</span>
<span class="n">tr_target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">train_inds</span><span class="p">]</span>

<span class="n">ts_input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">test_inds</span><span class="p">]</span>
<span class="n">ts_target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">test_inds</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr_input_texts</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;27+673&#39;,
 &#39;153+27&#39;,
 &#39;93+901&#39;,
 &#39;243+678&#39;,
 &#39;269+46&#39;,
 &#39;235+891&#39;,
 &#39;46+290&#39;,
 &#39;324+947&#39;,
 &#39;721+49&#39;,
 &#39;535+7&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr_target_texts</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;_700_&#39;,
 &#39;_180_&#39;,
 &#39;_994_&#39;,
 &#39;_921_&#39;,
 &#39;_315_&#39;,
 &#39;_1126_&#39;,
 &#39;_336_&#39;,
 &#39;_1271_&#39;,
 &#39;_770_&#39;,
 &#39;_542_&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of Samples:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of Samples in Training:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of Samples in Testing:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ts_input_texts</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of Samples: 50000
Number of Samples in Training: 45000
Number of Samples in Testing: 5000
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-preprocessing">
<h2><a class="toc-backref" href="#id14">Data Preprocessing</a><a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/text-seq-onehot-embedding.jpeg" /></p>
<section id="text-to-sequences">
<h3><a class="toc-backref" href="#id15">Text to Sequences</a><a class="headerlink" href="#text-to-sequences" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Tokenization of input and target texts invovles the following important steps:</p>
<ul>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code></p></li>
<li><p>Fit the <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> on the training sets</p></li>
<li><p>Tokenize input and target texts of the training set into sequences</p></li>
<li><p>Identify the maxlen of the input and target sequences</p></li>
<li><p>Pad input and target sequences to uniform lengths</p></li>
</ul>
</li>
<li><p>Note that we need to create a <strong>character-based</strong> <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code>.</p></li>
<li><p>There will be two Tokenizers, one for input texts and the other for target texts.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Tokenizers for inputs</span>
<span class="n">input_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">char_level</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">input_tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">)</span>
<span class="n">encoder_input_sequences</span> <span class="o">=</span> <span class="n">input_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">)</span>
<span class="n">input_maxlen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">encoder_input_sequences</span><span class="p">])</span>
<span class="n">encoder_input_sequences</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span>
                                        <span class="n">maxlen</span><span class="o">=</span><span class="n">input_maxlen</span><span class="p">)</span>

<span class="c1">## Tokenizer for targets</span>
<span class="n">target_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">char_level</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target_tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">tr_target_texts</span><span class="p">)</span>
<span class="n">target_sequences</span> <span class="o">=</span> <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">tr_target_texts</span><span class="p">)</span>
<span class="n">target_maxlen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">target_sequences</span><span class="p">])</span>
<span class="n">target_sequences</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">,</span>
                                 <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span>
                                 <span class="n">maxlen</span><span class="o">=</span><span class="n">target_maxlen</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Shapes of Input and Target Sequences</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(45000, 7)
(45000, 6)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## vocab size</span>
<span class="n">input_vsize</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">input_tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">target_vsize</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">target_tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The plus 1 for vocabulary size is to include the padding character, whose index is the reserved <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">input_vsize</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12
12
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;27+673&#39;, &#39;153+27&#39;, &#39;93+901&#39;]
[[ 9 10  1  7 10  5  0]
 [ 8  6  5  1  9 10  0]
 [ 2  5  1  2 11  8  0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_tokenizer</span><span class="o">.</span><span class="n">word_index</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;+&#39;: 1,
 &#39;9&#39;: 2,
 &#39;4&#39;: 3,
 &#39;8&#39;: 4,
 &#39;3&#39;: 5,
 &#39;5&#39;: 6,
 &#39;6&#39;: 7,
 &#39;1&#39;: 8,
 &#39;2&#39;: 9,
 &#39;7&#39;: 10,
 &#39;0&#39;: 11}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tr_target_texts</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;_700_&#39;, &#39;_180_&#39;, &#39;_994_&#39;]
[[ 1  4 11 11  1  0]
 [ 1  2 10 11  1  0]
 [ 1  7  7  8  1  0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_tokenizer</span><span class="o">.</span><span class="n">word_index</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;_&#39;: 1,
 &#39;1&#39;: 2,
 &#39;2&#39;: 3,
 &#39;7&#39;: 4,
 &#39;6&#39;: 5,
 &#39;3&#39;: 6,
 &#39;9&#39;: 7,
 &#39;4&#39;: 8,
 &#39;5&#39;: 9,
 &#39;8&#39;: 10,
 &#39;0&#39;: 11}
</pre></div>
</div>
</div>
</div>
</section>
<section id="special-considerations-for-decoder-s-input-and-output">
<h3><a class="toc-backref" href="#id16">Special Considerations for Decoder’s Input and Output</a><a class="headerlink" href="#special-considerations-for-decoder-s-input-and-output" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/seq2seq-vanilla-rnn.jpeg" /></p>
<ul class="simple">
<li><p>In the training stage, we give the Decoder the correct target sequences for <strong>teacher forcing</strong>.</p></li>
<li><p>Input and Output Sequences for Decoder</p>
<ul>
<li><p>Decoder input and output sequences have one time-step difference (i.e., the decoder’s output at <span class="math notranslate nohighlight">\(t-1\)</span> is the decoder’s input at <span class="math notranslate nohighlight">\(t\)</span>)</p></li>
<li><p>We create decoder input and output sequences as different sets of data.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">decoder_input_sequences</span> <span class="o">=</span> <span class="n">target_sequences</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">decoder_output_sequences</span> <span class="o">=</span> <span class="n">target_sequences</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_input_sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_output_sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1  4 11 11  1]
 [ 1  2 10 11  1]
 [ 1  7  7  8  1]
 [ 1  7  3  2  1]
 [ 1  6  2  9  1]]
[[ 4 11 11  1  0]
 [ 2 10 11  1  0]
 [ 7  7  8  1  0]
 [ 7  3  2  1  0]
 [ 6  2  9  1  0]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="sequences-to-one-hot-encoding">
<h3><a class="toc-backref" href="#id17">Sequences to One-Hot Encoding</a><a class="headerlink" href="#sequences-to-one-hot-encoding" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>To simplify the matter, we convert each sequence/integer token into one-hot encoding, which will be the input of the Encoder directly.</p></li>
<li><p>Normally we would add an Embedding layer to convert sequence tokens to embeddings before sending them to the Encoder.</p></li>
<li><p>Please note that this step renders the text representation of the entire training data from 2D (<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="docutils literal notranslate"><span class="pre">max_length</span></code>) to 3D tensors (<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="docutils literal notranslate"><span class="pre">max_length</span></code>, <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>).</p></li>
</ul>
<p><img alt="" src="../_images/text-seq-onehot-embedding.jpeg" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For neural machine translations, the vocabulary sizes of the input and target languages are usually very large. It is more effective to implement an <strong>Embedding</strong> layer to convert sequences (integers) into embeddings, rather than one-hot encodings.</p>
<p>For this tutorial, we have a limited vocabulary size (only digits and math symbols). One-hot encodings should be fine.</p>
<p>However, in the assignment, you will practice on how to add embedding layers for both Encoder and Decoder.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_input_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_output_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(45000, 7)
(45000, 5)
(45000, 5)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoder_input_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="p">,</span>
                                      <span class="n">num_classes</span><span class="o">=</span><span class="n">input_vsize</span><span class="p">)</span>
<span class="n">decoder_input_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">decoder_input_sequences</span><span class="p">,</span>
                                      <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">)</span>
<span class="n">decoder_output_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">decoder_output_sequences</span><span class="p">,</span>
                                       <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_input_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_output_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(45000, 7, 12)
(45000, 5, 12)
(45000, 5, 12)
</pre></div>
</div>
</div>
</div>
</section>
<section id="token-indices">
<h3><a class="toc-backref" href="#id18">Token Indices</a><a class="headerlink" href="#token-indices" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>We create the integer-to-character dictionaries for later use.</p></li>
<li><p>Two dictionaries, one for the input sequence and one for the target sequence.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Index2word &quot;&quot;&quot;</span>
<span class="n">enc_index2word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">input_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
        <span class="n">input_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">dec_index2word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">target_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
        <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">enc_index2word</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;+&#39;,
 2: &#39;9&#39;,
 3: &#39;4&#39;,
 4: &#39;8&#39;,
 5: &#39;3&#39;,
 6: &#39;5&#39;,
 7: &#39;6&#39;,
 8: &#39;1&#39;,
 9: &#39;2&#39;,
 10: &#39;7&#39;,
 11: &#39;0&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dec_index2word</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;_&#39;,
 2: &#39;1&#39;,
 3: &#39;2&#39;,
 4: &#39;7&#39;,
 5: &#39;6&#39;,
 6: &#39;3&#39;,
 7: &#39;9&#39;,
 8: &#39;4&#39;,
 9: &#39;5&#39;,
 10: &#39;8&#39;,
 11: &#39;0&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">150</span>


<span class="c1"># Plotting results</span>
<span class="k">def</span> <span class="nf">plot1</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">## Accuracy plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training acc&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation acc&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="c1">## Loss plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot2</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#plt.gca().set_ylim(0,1)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model-training">
<h2><a class="toc-backref" href="#id19">Model Training</a><a class="headerlink" href="#model-training" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Define the model architecture</p></li>
<li><p>Train the model</p></li>
</ul>
<ul class="simple">
<li><p>Sequence-to-Sequence can go from simple RNNs to complex models with attention mechanisms.</p></li>
<li><p>In this tutorial, we will try the following:</p>
<ul>
<li><p>Sequence-to-sequence model with <strong>vanilla RNN</strong> Encoder and Decoder</p></li>
<li><p>Sequence-to-sequence model with <strong>GRU</strong> Encoder and Decoder</p></li>
<li><p>Sequence-to-sequence model with <strong>bidirectional</strong> RNN Encoder</p></li>
<li><p>Sequence-to-sequence model with <strong>peeky</strong> Decoder</p></li>
<li><p>Sequence-to-sequence model with <strong>attention-based</strong> Decoder</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Sequential vs. Functional API in <code class="docutils literal notranslate"><span class="pre">keras</span></code></p>
<ul>
<li><p>We have been using the Sequential API to create the network models, where each layer’s output is the input of the subsequent layer.</p></li>
<li><p>However, for Encoder-Decoder Models, sometimes not all the outputs of the previous layer are the inputs of the subsequent layer.</p></li>
<li><p>We need more flexibility in the ways of connecting the inputs and outputs of the model layers.</p></li>
<li><p>Therefore, here we will use the Functional API for model definition.</p></li>
</ul>
</li>
</ul>
</section>
<section id="model-1-vanilla-rnn">
<h2><a class="toc-backref" href="#id20">Model 1 (Vanilla RNN)</a><a class="headerlink" href="#model-1-vanilla-rnn" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/seq2seq-vanilla-rnn.jpeg" /></p>
<section id="define-model">
<h3><a class="toc-backref" href="#id21">Define Model</a><a class="headerlink" href="#define-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Important Highlights:</p>
<ul>
<li><p>In the training stage, we feed the decoder the correct answer at each time step as the input sequence.</p></li>
<li><p>In the testing stage, the decoder will take the hidden state from the previous time step as the input sequence.</p></li>
<li><p>This type of training is referred to as <strong>teacher forcing</strong> learning strategy. This can help the model converge more effectively.</p></li>
<li><p>The decoder uses encoder’s last hidden state as the initial hidden state.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder RNN</span>
    <span class="c1">## first return is the hidden states of all timesteps of encoder</span>
    <span class="c1">## second return is the last hidden state of encoder</span>
<span class="n">encoder_rnn</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_rnn&#39;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state</span> <span class="o">=</span> <span class="n">encoder_rnn</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>


<span class="c1"># Decoder RNN</span>
    <span class="c1">## using `encoder_state` (last h) as initial state.</span>
    <span class="c1">## using `decoder_inputs` for teacher forcing learning</span>
<span class="n">decoder_rnn</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_rnn&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_rnn</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model1</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 encoder_inputs (InputLayer  [(None, 7, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 decoder_inputs (InputLayer  [(None, 5, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 encoder_rnn (SimpleRNN)     [(None, 7, 256),             68864     [&#39;encoder_inputs[0][0]&#39;]      
                              (None, 256)]                                                        
                                                                                                  
 decoder_rnn (SimpleRNN)     [(None, 5, 256),             68864     [&#39;decoder_inputs[0][0]&#39;,      
                              (None, 256)]                           &#39;encoder_rnn[0][1]&#39;]         
                                                                                                  
 time_distributed_layer (Ti  (None, 5, 12)                3084      [&#39;decoder_rnn[0][0]&#39;]         
 meDistributed)                                                                                   
                                                                                                  
==================================================================================================
Total params: 140812 (550.05 KB)
Trainable params: 140812 (550.05 KB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is crucial to wrap the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer with <code class="docutils literal notranslate"><span class="pre">TimeDistributed</span></code> layer. The <code class="docutils literal notranslate"><span class="pre">TimeDistributed</span></code> wrapper ensures that the Dense layer is applied independently to each time step in the input sequence. This is useful in models that deal with sequences, such as RNNs, LSTMs, and GRUs, where you need to apply the same layer to each element of the sequence.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model1</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f3eece3c02f9bc6b41e9891c131ebf612d40eb857aeb32c5dea2a8564afd9732.png" src="../_images/f3eece3c02f9bc6b41e9891c131ebf612d40eb857aeb32c5dea2a8564afd9732.png" />
</div>
</div>
</section>
<section id="training">
<h3><a class="toc-backref" href="#id22">Training</a><a class="headerlink" href="#training" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model1</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history1</span> <span class="o">=</span> <span class="n">full_model1</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
282/282 [==============================] - 3s 9ms/step - loss: 1.3127 - accuracy: 0.5142 - val_loss: 1.1931 - val_accuracy: 0.5534
Epoch 2/30
282/282 [==============================] - 2s 8ms/step - loss: 1.1009 - accuracy: 0.5870 - val_loss: 1.0211 - val_accuracy: 0.6139
Epoch 3/30
282/282 [==============================] - 2s 9ms/step - loss: 0.9173 - accuracy: 0.6500 - val_loss: 0.8363 - val_accuracy: 0.6737
Epoch 4/30
282/282 [==============================] - 2s 8ms/step - loss: 0.7440 - accuracy: 0.7129 - val_loss: 0.6818 - val_accuracy: 0.7317
Epoch 5/30
282/282 [==============================] - 2s 9ms/step - loss: 0.6040 - accuracy: 0.7679 - val_loss: 0.5688 - val_accuracy: 0.7736
Epoch 6/30
282/282 [==============================] - 2s 9ms/step - loss: 0.4908 - accuracy: 0.8138 - val_loss: 0.4671 - val_accuracy: 0.8196
Epoch 7/30
282/282 [==============================] - 2s 9ms/step - loss: 0.4137 - accuracy: 0.8431 - val_loss: 0.4192 - val_accuracy: 0.8309
Epoch 8/30
282/282 [==============================] - 2s 9ms/step - loss: 0.3442 - accuracy: 0.8698 - val_loss: 0.3372 - val_accuracy: 0.8660
Epoch 9/30
282/282 [==============================] - 2s 8ms/step - loss: 0.2847 - accuracy: 0.8935 - val_loss: 0.2892 - val_accuracy: 0.8892
Epoch 10/30
282/282 [==============================] - 2s 9ms/step - loss: 0.2376 - accuracy: 0.9132 - val_loss: 0.2696 - val_accuracy: 0.8958
Epoch 11/30
282/282 [==============================] - 2s 8ms/step - loss: 0.2146 - accuracy: 0.9217 - val_loss: 0.2328 - val_accuracy: 0.9116
Epoch 12/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1927 - accuracy: 0.9297 - val_loss: 0.2322 - val_accuracy: 0.9084
Epoch 13/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1678 - accuracy: 0.9413 - val_loss: 0.1838 - val_accuracy: 0.9302
Epoch 14/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1561 - accuracy: 0.9441 - val_loss: 0.2071 - val_accuracy: 0.9191
Epoch 15/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1451 - accuracy: 0.9475 - val_loss: 0.1814 - val_accuracy: 0.9309
Epoch 16/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1431 - accuracy: 0.9479 - val_loss: 0.1869 - val_accuracy: 0.9290
Epoch 17/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1262 - accuracy: 0.9548 - val_loss: 0.1534 - val_accuracy: 0.9424
Epoch 18/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1185 - accuracy: 0.9584 - val_loss: 0.1880 - val_accuracy: 0.9298
Epoch 19/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1123 - accuracy: 0.9603 - val_loss: 0.1497 - val_accuracy: 0.9433
Epoch 20/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1094 - accuracy: 0.9608 - val_loss: 0.1479 - val_accuracy: 0.9422
Epoch 21/30
282/282 [==============================] - 2s 9ms/step - loss: 0.0959 - accuracy: 0.9669 - val_loss: 0.1568 - val_accuracy: 0.9399
Epoch 22/30
282/282 [==============================] - 2s 9ms/step - loss: 0.1089 - accuracy: 0.9607 - val_loss: 0.1576 - val_accuracy: 0.9422
Epoch 23/30
282/282 [==============================] - 2s 9ms/step - loss: 0.0907 - accuracy: 0.9678 - val_loss: 0.1418 - val_accuracy: 0.9453
Epoch 24/30
282/282 [==============================] - 2s 9ms/step - loss: 0.0892 - accuracy: 0.9685 - val_loss: 0.1580 - val_accuracy: 0.9417
Epoch 25/30
282/282 [==============================] - 2s 9ms/step - loss: 0.0906 - accuracy: 0.9672 - val_loss: 0.1567 - val_accuracy: 0.9409
Epoch 26/30
282/282 [==============================] - 3s 9ms/step - loss: 0.0897 - accuracy: 0.9678 - val_loss: 0.1396 - val_accuracy: 0.9479
Epoch 27/30
282/282 [==============================] - 3s 9ms/step - loss: 0.0855 - accuracy: 0.9698 - val_loss: 0.1460 - val_accuracy: 0.9464
Epoch 28/30
282/282 [==============================] - 2s 9ms/step - loss: 0.0842 - accuracy: 0.9704 - val_loss: 0.1445 - val_accuracy: 0.9467
Epoch 29/30
282/282 [==============================] - 3s 9ms/step - loss: 0.0825 - accuracy: 0.9708 - val_loss: 0.1409 - val_accuracy: 0.9473
Epoch 30/30
282/282 [==============================] - 3s 9ms/step - loss: 0.0694 - accuracy: 0.9759 - val_loss: 0.1590 - val_accuracy: 0.9417
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot1</span><span class="p">(</span><span class="n">history1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/472651b99b1bcbac1f1df8d996d0d431bc94af1664d123b2bfda0a62d442411a.png" src="../_images/472651b99b1bcbac1f1df8d996d0d431bc94af1664d123b2bfda0a62d442411a.png" />
<img alt="../_images/976cb7891a2f1a9c0baa86c9aa83b88993361c9245d13ee21a2e69beedcdb98c.png" src="../_images/976cb7891a2f1a9c0baa86c9aa83b88993361c9245d13ee21a2e69beedcdb98c.png" />
</div>
</div>
</section>
</section>
<section id="model-2-gru">
<h2><a class="toc-backref" href="#id23">Model 2 (GRU)</a><a class="headerlink" href="#model-2-gru" title="Permalink to this headline">#</a></h2>
<section id="id1">
<h3><a class="toc-backref" href="#id24">Define Model</a><a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Important highlights:</p>
<ul>
<li><p>In Model 2, we replace vanilla RNN with GRU, which deals with the issue of long-distance dependencies between sequences.</p></li>
<li><p>You can try LSTM as well.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder GRU</span>
    <span class="c1">## first return is the hidden states of all timesteps of encoder</span>
    <span class="c1">## second return is the last hidden state of encoder</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Decoder RNN</span>
    <span class="c1">## using `encoder_state` (last h) as initial state.</span>
    <span class="c1">## using `decoder_inputs` for teacher forcing learning</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model2</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 encoder_inputs (InputLayer  [(None, 7, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 decoder_inputs (InputLayer  [(None, 5, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 encoder_gru (GRU)           [(None, 7, 256),             207360    [&#39;encoder_inputs[0][0]&#39;]      
                              (None, 256)]                                                        
                                                                                                  
 decoder_gru (GRU)           [(None, 5, 256),             207360    [&#39;decoder_inputs[0][0]&#39;,      
                              (None, 256)]                           &#39;encoder_gru[0][1]&#39;]         
                                                                                                  
 time_distributed_layer (Ti  (None, 5, 12)                3084      [&#39;decoder_gru[0][0]&#39;]         
 meDistributed)                                                                                   
                                                                                                  
==================================================================================================
Total params: 417804 (1.59 MB)
Trainable params: 417804 (1.59 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model2</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/610d25afae6224930b9b7a78a6aee1cb157b2152806b0f4b21d070f720e087b8.png" src="../_images/610d25afae6224930b9b7a78a6aee1cb157b2152806b0f4b21d070f720e087b8.png" />
</div>
</div>
</section>
<section id="id2">
<h3><a class="toc-backref" href="#id25">Training</a><a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history2</span> <span class="o">=</span> <span class="n">full_model2</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
282/282 [==============================] - 7s 22ms/step - loss: 1.4847 - accuracy: 0.4599 - val_loss: 1.3745 - val_accuracy: 0.4835
Epoch 2/30
282/282 [==============================] - 6s 22ms/step - loss: 1.3170 - accuracy: 0.5044 - val_loss: 1.2046 - val_accuracy: 0.5423
Epoch 3/30
282/282 [==============================] - 6s 22ms/step - loss: 1.1033 - accuracy: 0.5820 - val_loss: 1.0287 - val_accuracy: 0.6080
Epoch 4/30
282/282 [==============================] - 6s 23ms/step - loss: 0.9732 - accuracy: 0.6299 - val_loss: 0.9163 - val_accuracy: 0.6503
Epoch 5/30
282/282 [==============================] - 6s 22ms/step - loss: 0.8589 - accuracy: 0.6723 - val_loss: 0.8084 - val_accuracy: 0.6925
Epoch 6/30
282/282 [==============================] - 6s 22ms/step - loss: 0.7667 - accuracy: 0.7077 - val_loss: 0.7415 - val_accuracy: 0.7166
Epoch 7/30
282/282 [==============================] - 6s 23ms/step - loss: 0.7039 - accuracy: 0.7360 - val_loss: 0.6964 - val_accuracy: 0.7313
Epoch 8/30
282/282 [==============================] - 7s 23ms/step - loss: 0.6590 - accuracy: 0.7528 - val_loss: 0.6635 - val_accuracy: 0.7428
Epoch 9/30
282/282 [==============================] - 7s 25ms/step - loss: 0.6170 - accuracy: 0.7666 - val_loss: 0.6115 - val_accuracy: 0.7636
Epoch 10/30
282/282 [==============================] - 7s 24ms/step - loss: 0.5770 - accuracy: 0.7796 - val_loss: 0.5821 - val_accuracy: 0.7709
Epoch 11/30
282/282 [==============================] - 7s 24ms/step - loss: 0.5386 - accuracy: 0.7918 - val_loss: 0.5455 - val_accuracy: 0.7836
Epoch 12/30
282/282 [==============================] - 6s 23ms/step - loss: 0.5118 - accuracy: 0.8005 - val_loss: 0.5117 - val_accuracy: 0.7965
Epoch 13/30
282/282 [==============================] - 7s 23ms/step - loss: 0.4765 - accuracy: 0.8132 - val_loss: 0.4848 - val_accuracy: 0.8067
Epoch 14/30
282/282 [==============================] - 7s 23ms/step - loss: 0.4444 - accuracy: 0.8262 - val_loss: 0.4512 - val_accuracy: 0.8171
Epoch 15/30
282/282 [==============================] - 7s 23ms/step - loss: 0.4040 - accuracy: 0.8414 - val_loss: 0.4349 - val_accuracy: 0.8220
Epoch 16/30
282/282 [==============================] - 6s 23ms/step - loss: 0.3529 - accuracy: 0.8625 - val_loss: 0.3531 - val_accuracy: 0.8574
Epoch 17/30
282/282 [==============================] - 7s 23ms/step - loss: 0.3035 - accuracy: 0.8842 - val_loss: 0.3054 - val_accuracy: 0.8799
Epoch 18/30
282/282 [==============================] - 7s 24ms/step - loss: 0.2529 - accuracy: 0.9064 - val_loss: 0.2599 - val_accuracy: 0.8991
Epoch 19/30
282/282 [==============================] - 7s 25ms/step - loss: 0.2105 - accuracy: 0.9242 - val_loss: 0.2181 - val_accuracy: 0.9173
Epoch 20/30
282/282 [==============================] - 7s 24ms/step - loss: 0.1728 - accuracy: 0.9386 - val_loss: 0.1745 - val_accuracy: 0.9346
Epoch 21/30
282/282 [==============================] - 7s 24ms/step - loss: 0.1399 - accuracy: 0.9509 - val_loss: 0.1543 - val_accuracy: 0.9406
Epoch 22/30
282/282 [==============================] - 7s 25ms/step - loss: 0.1172 - accuracy: 0.9592 - val_loss: 0.1319 - val_accuracy: 0.9505
Epoch 23/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0895 - accuracy: 0.9708 - val_loss: 0.0991 - val_accuracy: 0.9650
Epoch 24/30
282/282 [==============================] - 7s 24ms/step - loss: 0.0755 - accuracy: 0.9761 - val_loss: 0.0893 - val_accuracy: 0.9668
Epoch 25/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0552 - accuracy: 0.9845 - val_loss: 0.0738 - val_accuracy: 0.9743
Epoch 26/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0499 - accuracy: 0.9858 - val_loss: 0.1067 - val_accuracy: 0.9602
Epoch 27/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0521 - accuracy: 0.9837 - val_loss: 0.0708 - val_accuracy: 0.9749
Epoch 28/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0381 - accuracy: 0.9895 - val_loss: 0.0543 - val_accuracy: 0.9811
Epoch 29/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0338 - accuracy: 0.9907 - val_loss: 0.0495 - val_accuracy: 0.9829
Epoch 30/30
282/282 [==============================] - 7s 25ms/step - loss: 0.0360 - accuracy: 0.9895 - val_loss: 0.0582 - val_accuracy: 0.9796
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot1</span><span class="p">(</span><span class="n">history2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4c446f029c7ab578953879cb4a2c91a61ded1708828ef998901e416a331b87cb.png" src="../_images/4c446f029c7ab578953879cb4a2c91a61ded1708828ef998901e416a331b87cb.png" />
<img alt="../_images/d6258726a76fc1d265a5452812dd22a05b1df070a6ddcf3f9f5f131cbcf3fb53.png" src="../_images/d6258726a76fc1d265a5452812dd22a05b1df070a6ddcf3f9f5f131cbcf3fb53.png" />
</div>
</div>
</section>
</section>
<section id="model-3-birdirectional">
<h2><a class="toc-backref" href="#id26">Model 3 (Birdirectional)</a><a class="headerlink" href="#model-3-birdirectional" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/seq2seq-bidirectional.jpeg" /></p>
<section id="id3">
<h3><a class="toc-backref" href="#id27">Define Model</a><a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Important highlights:</p>
<ul>
<li><p>In Model 3, we implement a bi-directional Encoder.</p></li>
<li><p>At each encoding step, there will be two hidden states (i.e., forward and backward passes)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span>
    <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Combine forward and backward state (last h&#39;s) from encoder</span>
<span class="n">encoder_state</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)([</span><span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span><span class="p">])</span>

<span class="c1"># Decoder GRU</span>
    <span class="c1"># using `encoder_state` as initial state</span>
    <span class="c1"># the latent_dim *2 because we use two last states from the bidirectional encoder</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model3</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model3</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_2&quot;
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 encoder_inputs (InputLayer  [(None, 7, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 bidirectional (Bidirection  [(None, 7, 512),             414720    [&#39;encoder_inputs[0][0]&#39;]      
 al)                          (None, 256),                                                        
                              (None, 256)]                                                        
                                                                                                  
 decoder_inputs (InputLayer  [(None, 5, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 concatenate (Concatenate)   (None, 512)                  0         [&#39;bidirectional[0][1]&#39;,       
                                                                     &#39;bidirectional[0][2]&#39;]       
                                                                                                  
 decoder_gru (GRU)           [(None, 5, 512),             807936    [&#39;decoder_inputs[0][0]&#39;,      
                              (None, 512)]                           &#39;concatenate[0][0]&#39;]         
                                                                                                  
 time_distributed_layer (Ti  (None, 5, 12)                6156      [&#39;decoder_gru[0][0]&#39;]         
 meDistributed)                                                                                   
                                                                                                  
==================================================================================================
Total params: 1228812 (4.69 MB)
Trainable params: 1228812 (4.69 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model3</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/41425a11b103541015f51467b56e6524817165a07e140d34871975c8d7f3cdac.png" src="../_images/41425a11b103541015f51467b56e6524817165a07e140d34871975c8d7f3cdac.png" />
</div>
</div>
</section>
<section id="id4">
<h3><a class="toc-backref" href="#id28">Training</a><a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model3</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history3</span> <span class="o">=</span> <span class="n">full_model3</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
282/282 [==============================] - 12s 37ms/step - loss: 1.4408 - accuracy: 0.4702 - val_loss: 1.3021 - val_accuracy: 0.5137
Epoch 2/30
282/282 [==============================] - 11s 39ms/step - loss: 1.1192 - accuracy: 0.5754 - val_loss: 0.9751 - val_accuracy: 0.6264
Epoch 3/30
282/282 [==============================] - 12s 43ms/step - loss: 0.8618 - accuracy: 0.6683 - val_loss: 0.8019 - val_accuracy: 0.6840
Epoch 4/30
282/282 [==============================] - 11s 39ms/step - loss: 0.7199 - accuracy: 0.7256 - val_loss: 0.6826 - val_accuracy: 0.7352
Epoch 5/30
282/282 [==============================] - 12s 43ms/step - loss: 0.6149 - accuracy: 0.7667 - val_loss: 0.5581 - val_accuracy: 0.7875
Epoch 6/30
282/282 [==============================] - 12s 41ms/step - loss: 0.4916 - accuracy: 0.8153 - val_loss: 0.4018 - val_accuracy: 0.8436
Epoch 7/30
282/282 [==============================] - 12s 44ms/step - loss: 0.2687 - accuracy: 0.9028 - val_loss: 0.2229 - val_accuracy: 0.9060
Epoch 8/30
282/282 [==============================] - 12s 42ms/step - loss: 0.1327 - accuracy: 0.9587 - val_loss: 0.1096 - val_accuracy: 0.9638
Epoch 9/30
282/282 [==============================] - 11s 40ms/step - loss: 0.0713 - accuracy: 0.9826 - val_loss: 0.0769 - val_accuracy: 0.9760
Epoch 10/30
282/282 [==============================] - 12s 43ms/step - loss: 0.0663 - accuracy: 0.9815 - val_loss: 0.0558 - val_accuracy: 0.9846
Epoch 11/30
282/282 [==============================] - 12s 41ms/step - loss: 0.0350 - accuracy: 0.9928 - val_loss: 0.0443 - val_accuracy: 0.9868
Epoch 12/30
282/282 [==============================] - 12s 43ms/step - loss: 0.0573 - accuracy: 0.9843 - val_loss: 0.0260 - val_accuracy: 0.9943
Epoch 13/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 0.0277 - val_accuracy: 0.9920
Epoch 14/30
282/282 [==============================] - 12s 41ms/step - loss: 0.0425 - accuracy: 0.9869 - val_loss: 0.0343 - val_accuracy: 0.9892
Epoch 15/30
282/282 [==============================] - 11s 40ms/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 0.0151 - val_accuracy: 0.9962
Epoch 16/30
282/282 [==============================] - 12s 43ms/step - loss: 0.0129 - accuracy: 0.9973 - val_loss: 0.0655 - val_accuracy: 0.9763
Epoch 17/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0511 - accuracy: 0.9833 - val_loss: 0.0160 - val_accuracy: 0.9962
Epoch 18/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0073 - accuracy: 0.9991 - val_loss: 0.0101 - val_accuracy: 0.9976
Epoch 19/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.0170 - val_accuracy: 0.9947
Epoch 20/30
282/282 [==============================] - 12s 43ms/step - loss: 0.0527 - accuracy: 0.9833 - val_loss: 0.0827 - val_accuracy: 0.9717
Epoch 21/30
282/282 [==============================] - 11s 40ms/step - loss: 0.0148 - accuracy: 0.9968 - val_loss: 0.0102 - val_accuracy: 0.9976
Epoch 22/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 0.0073 - val_accuracy: 0.9980
Epoch 23/30
282/282 [==============================] - 12s 43ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.0243 - val_accuracy: 0.9920
Epoch 24/30
282/282 [==============================] - 12s 41ms/step - loss: 0.0441 - accuracy: 0.9853 - val_loss: 0.0170 - val_accuracy: 0.9952
Epoch 25/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0096 - val_accuracy: 0.9974
Epoch 26/30
282/282 [==============================] - 11s 41ms/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.0449 - val_accuracy: 0.9837
Epoch 27/30
282/282 [==============================] - 12s 41ms/step - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.0142 - val_accuracy: 0.9957
Epoch 28/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.0114 - val_accuracy: 0.9963
Epoch 29/30
282/282 [==============================] - 12s 41ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0408 - val_accuracy: 0.9855
Epoch 30/30
282/282 [==============================] - 12s 42ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.0476 - val_accuracy: 0.9848
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot1</span><span class="p">(</span><span class="n">history3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/421e05c545ebc5e8f6e99bf4af04006bc081f192da5b73e1e189c17930d4617d.png" src="../_images/421e05c545ebc5e8f6e99bf4af04006bc081f192da5b73e1e189c17930d4617d.png" />
<img alt="../_images/dd859949aaa15831cca70dd7b79b1d57b40026d31c7e2ed2243832ae9dd95cee.png" src="../_images/dd859949aaa15831cca70dd7b79b1d57b40026d31c7e2ed2243832ae9dd95cee.png" />
</div>
</div>
</section>
</section>
<section id="model-4-peeky-decoder">
<h2><a class="toc-backref" href="#id29">Model 4 (Peeky Decoder)</a><a class="headerlink" href="#model-4-peeky-decoder" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/seq2seq-peeky.jpeg" /></p>
<section id="id5">
<h3><a class="toc-backref" href="#id30">Define Model</a><a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Important highlights:</p>
<ul>
<li><p>In the previous models, Decoder only utilizes Encoder’s last hidden state for the decoding of the <strong>first</strong> output. As for the subsequent decoding time steps, Decoder does not have any information from Encoder.</p></li>
<li><p>In Model 4, we implement a <strong>peeky</strong> Decoder. This strategy allows Decoder to access the information (last hidden state) of the Encoder in every decoding time step.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">target_maxlen</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.int64&#39;&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span>
    <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Combine forward and backward state (last h&#39;s) from encoder</span>
<span class="n">encoder_state</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)([</span><span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span><span class="p">])</span>


<span class="c1"># Repeat the last-hidden-state of Encoder</span>
<span class="n">encoder_state_repeated</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">target_maxlen</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)(</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1">## Concatenate every decoder input with the encoder_state (last hidden state from encoder)</span>
<span class="n">decoder_inputs_peeky</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span>
    <span class="p">[</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">encoder_state_repeated</span><span class="p">])</span>

<span class="c1"># Decoder GRU</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs_peeky</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model4</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>

<span class="n">full_model4</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_4&quot;
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 encoder_inputs (InputLayer  [(None, 7, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 bidirectional_3 (Bidirecti  [(None, 7, 512),             414720    [&#39;encoder_inputs[0][0]&#39;]      
 onal)                        (None, 256),                                                        
                              (None, 256)]                                                        
                                                                                                  
 concatenate_4 (Concatenate  (None, 512)                  0         [&#39;bidirectional_3[0][1]&#39;,     
 )                                                                   &#39;bidirectional_3[0][2]&#39;]     
                                                                                                  
 decoder_inputs (InputLayer  [(None, 5, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 repeat_vector_2 (RepeatVec  (None, 5, 512)               0         [&#39;concatenate_4[0][0]&#39;]       
 tor)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 5, 524)               0         [&#39;decoder_inputs[0][0]&#39;,      
 )                                                                   &#39;repeat_vector_2[0][0]&#39;]     
                                                                                                  
 decoder_gru (GRU)           [(None, 5, 512),             1594368   [&#39;concatenate_5[0][0]&#39;,       
                              (None, 512)]                           &#39;concatenate_4[0][0]&#39;]       
                                                                                                  
 time_distributed_layer (Ti  (None, 5, 12)                6156      [&#39;decoder_gru[0][0]&#39;]         
 meDistributed)                                                                                   
                                                                                                  
==================================================================================================
Total params: 2015244 (7.69 MB)
Trainable params: 2015244 (7.69 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model4</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a45b016b1fa501aba97dd26001a9f9ed46d4a9da8f58370347de20a84e123c20.png" src="../_images/a45b016b1fa501aba97dd26001a9f9ed46d4a9da8f58370347de20a84e123c20.png" />
</div>
</div>
</section>
<section id="id6">
<h3><a class="toc-backref" href="#id31">Training</a><a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model4</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history4</span> <span class="o">=</span> <span class="n">full_model4</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
282/282 [==============================] - 15s 49ms/step - loss: 1.4575 - accuracy: 0.4671 - val_loss: 1.3298 - val_accuracy: 0.5042
Epoch 2/30
282/282 [==============================] - 14s 50ms/step - loss: 1.1431 - accuracy: 0.5682 - val_loss: 0.9817 - val_accuracy: 0.6284
Epoch 3/30
282/282 [==============================] - 14s 50ms/step - loss: 0.8454 - accuracy: 0.6782 - val_loss: 0.7315 - val_accuracy: 0.7163
Epoch 4/30
282/282 [==============================] - 14s 49ms/step - loss: 0.6416 - accuracy: 0.7585 - val_loss: 0.5794 - val_accuracy: 0.7819
Epoch 5/30
282/282 [==============================] - 15s 52ms/step - loss: 0.5134 - accuracy: 0.8079 - val_loss: 0.4452 - val_accuracy: 0.8253
Epoch 6/30
282/282 [==============================] - 14s 49ms/step - loss: 0.2866 - accuracy: 0.8976 - val_loss: 0.2098 - val_accuracy: 0.9229
Epoch 7/30
282/282 [==============================] - 14s 48ms/step - loss: 0.1302 - accuracy: 0.9635 - val_loss: 0.0975 - val_accuracy: 0.9742
Epoch 8/30
282/282 [==============================] - 15s 52ms/step - loss: 0.0755 - accuracy: 0.9811 - val_loss: 0.0536 - val_accuracy: 0.9876
Epoch 9/30
282/282 [==============================] - 14s 49ms/step - loss: 0.0632 - accuracy: 0.9826 - val_loss: 0.0853 - val_accuracy: 0.9716
Epoch 10/30
282/282 [==============================] - 14s 50ms/step - loss: 0.0330 - accuracy: 0.9936 - val_loss: 0.0264 - val_accuracy: 0.9943
Epoch 11/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0532 - accuracy: 0.9834 - val_loss: 0.0453 - val_accuracy: 0.9882
Epoch 12/30
282/282 [==============================] - 14s 50ms/step - loss: 0.0177 - accuracy: 0.9973 - val_loss: 0.0142 - val_accuracy: 0.9971
Epoch 13/30
282/282 [==============================] - 14s 51ms/step - loss: 0.0082 - accuracy: 0.9991 - val_loss: 0.0187 - val_accuracy: 0.9950
Epoch 14/30
282/282 [==============================] - 14s 50ms/step - loss: 0.0462 - accuracy: 0.9860 - val_loss: 0.0176 - val_accuracy: 0.9958
Epoch 15/30
282/282 [==============================] - 14s 51ms/step - loss: 0.0098 - accuracy: 0.9984 - val_loss: 0.0138 - val_accuracy: 0.9967
Epoch 16/30
282/282 [==============================] - 14s 49ms/step - loss: 0.0464 - accuracy: 0.9844 - val_loss: 0.0208 - val_accuracy: 0.9943
Epoch 17/30
282/282 [==============================] - 14s 49ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0078 - val_accuracy: 0.9984
Epoch 18/30
282/282 [==============================] - 14s 49ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0075 - val_accuracy: 0.9980
Epoch 19/30
282/282 [==============================] - 14s 49ms/step - loss: 0.0409 - accuracy: 0.9861 - val_loss: 0.0515 - val_accuracy: 0.9824
Epoch 20/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0175 - accuracy: 0.9953 - val_loss: 0.0129 - val_accuracy: 0.9967
Epoch 21/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0343 - accuracy: 0.9888 - val_loss: 0.0282 - val_accuracy: 0.9910
Epoch 22/30
282/282 [==============================] - 14s 49ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0063 - val_accuracy: 0.9986
Epoch 23/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0058 - val_accuracy: 0.9984
Epoch 24/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0153 - val_accuracy: 0.9951
Epoch 25/30
282/282 [==============================] - 13s 47ms/step - loss: 0.0448 - accuracy: 0.9854 - val_loss: 0.0111 - val_accuracy: 0.9968
Epoch 26/30
282/282 [==============================] - 14s 50ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.0045 - val_accuracy: 0.9989
Epoch 27/30
282/282 [==============================] - 13s 48ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0093 - val_accuracy: 0.9969
Epoch 28/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0450 - accuracy: 0.9848 - val_loss: 0.0246 - val_accuracy: 0.9920
Epoch 29/30
282/282 [==============================] - 14s 50ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.0059 - val_accuracy: 0.9985
Epoch 30/30
282/282 [==============================] - 14s 48ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0055 - val_accuracy: 0.9986
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot1</span><span class="p">(</span><span class="n">history4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6afb7a1e7582ddbeae67a57e8ed4a7308be45ec8eee15aa7040b330fa6049b6c.png" src="../_images/6afb7a1e7582ddbeae67a57e8ed4a7308be45ec8eee15aa7040b330fa6049b6c.png" />
<img alt="../_images/68d529c085e12ec6b28b306d70086042e003ef9d36217f798266856c961c39fd.png" src="../_images/68d529c085e12ec6b28b306d70086042e003ef9d36217f798266856c961c39fd.png" />
</div>
</div>
</section>
</section>
<section id="model-5-attention">
<h2><a class="toc-backref" href="#id32">Model 5 (Attention)</a><a class="headerlink" href="#model-5-attention" title="Permalink to this headline">#</a></h2>
<section id="id7">
<h3><a class="toc-backref" href="#id33">Define Model</a><a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Important highlights:</p>
<ul>
<li><p>In Model 5, we implement an Attention-based Decoder.</p></li>
<li><p>This Attention mechanism allows the Decoder to use Encoder’s all hidden states.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/seq2seq-attention-weights.jpeg" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define an input sequence and process it.</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>
<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">)</span>
<span class="n">encoder_out</span><span class="p">,</span> <span class="n">encoder_state</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Decoder GRU</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">decoder_state</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span>
                                         <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Attention layer</span>
<span class="n">attn_layer</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;attention_layer&quot;</span><span class="p">)</span>

<span class="c1">## The inputs for Attention:</span>
<span class="c1">##  `query`: the `decoder_out` = decoder&#39;s hidden state at the decoding step</span>
<span class="c1">##  `value` &amp; `key`: the `encoder_out` = encoder&#39;s all hidden states</span>
<span class="c1">## It returns a tensor of shape as `query`, i.e., context tensor</span>

<span class="n">attn_out</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_layer</span><span class="p">([</span><span class="n">decoder_out</span><span class="p">,</span> <span class="n">encoder_out</span><span class="p">],</span>
                                    <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Concat context tensor + decoder_out</span>
<span class="n">decoder_concat_input</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span>
    <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat_layer&#39;</span><span class="p">)([</span><span class="n">decoder_out</span><span class="p">,</span> <span class="n">attn_out</span><span class="p">])</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_concat_input</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model5</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model5</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_5&quot;
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 encoder_inputs (InputLayer  [(None, 7, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 decoder_inputs (InputLayer  [(None, 5, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 encoder_gru (GRU)           [(None, 7, 256),             207360    [&#39;encoder_inputs[0][0]&#39;]      
                              (None, 256)]                                                        
                                                                                                  
 decoder_gru (GRU)           [(None, 5, 256),             207360    [&#39;decoder_inputs[0][0]&#39;,      
                              (None, 256)]                           &#39;encoder_gru[0][1]&#39;]         
                                                                                                  
 attention_layer (Attention  ((None, 5, 256),             0         [&#39;decoder_gru[0][0]&#39;,         
 )                            (None, 5, 7))                          &#39;encoder_gru[0][0]&#39;]         
                                                                                                  
 concat_layer (Concatenate)  (None, 5, 512)               0         [&#39;decoder_gru[0][0]&#39;,         
                                                                     &#39;attention_layer[0][0]&#39;]     
                                                                                                  
 time_distributed_layer (Ti  (None, 5, 12)                6156      [&#39;concat_layer[0][0]&#39;]        
 meDistributed)                                                                                   
                                                                                                  
==================================================================================================
Total params: 420876 (1.61 MB)
Trainable params: 420876 (1.61 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model5</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/081155ed1d8cbabb3fbe18aa26c325c29b682e37a04f14ae9aab6c7e557f9638.png" src="../_images/081155ed1d8cbabb3fbe18aa26c325c29b682e37a04f14ae9aab6c7e557f9638.png" />
</div>
</div>
</section>
<section id="id8">
<h3><a class="toc-backref" href="#id34">Training</a><a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model5</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history5</span> <span class="o">=</span> <span class="n">full_model5</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
282/282 [==============================] - 10s 32ms/step - loss: 1.4844 - accuracy: 0.4648 - val_loss: 1.3643 - val_accuracy: 0.4886
Epoch 2/30
282/282 [==============================] - 9s 32ms/step - loss: 1.2823 - accuracy: 0.5179 - val_loss: 1.1667 - val_accuracy: 0.5568
Epoch 3/30
282/282 [==============================] - 8s 30ms/step - loss: 1.0821 - accuracy: 0.5861 - val_loss: 1.0221 - val_accuracy: 0.6080
Epoch 4/30
282/282 [==============================] - 9s 30ms/step - loss: 0.9666 - accuracy: 0.6296 - val_loss: 0.9237 - val_accuracy: 0.6430
Epoch 5/30
282/282 [==============================] - 9s 30ms/step - loss: 0.8499 - accuracy: 0.6719 - val_loss: 0.8047 - val_accuracy: 0.6857
Epoch 6/30
282/282 [==============================] - 9s 31ms/step - loss: 0.7705 - accuracy: 0.7048 - val_loss: 0.7387 - val_accuracy: 0.7198
Epoch 7/30
282/282 [==============================] - 9s 33ms/step - loss: 0.7143 - accuracy: 0.7299 - val_loss: 0.7108 - val_accuracy: 0.7278
Epoch 8/30
282/282 [==============================] - 9s 32ms/step - loss: 0.6750 - accuracy: 0.7437 - val_loss: 0.6666 - val_accuracy: 0.7470
Epoch 9/30
282/282 [==============================] - 9s 33ms/step - loss: 0.6342 - accuracy: 0.7589 - val_loss: 0.6429 - val_accuracy: 0.7490
Epoch 10/30
282/282 [==============================] - 9s 31ms/step - loss: 0.5980 - accuracy: 0.7687 - val_loss: 0.5968 - val_accuracy: 0.7645
Epoch 11/30
282/282 [==============================] - 9s 32ms/step - loss: 0.5564 - accuracy: 0.7843 - val_loss: 0.5790 - val_accuracy: 0.7686
Epoch 12/30
282/282 [==============================] - 9s 33ms/step - loss: 0.5242 - accuracy: 0.7949 - val_loss: 0.5206 - val_accuracy: 0.7924
Epoch 13/30
282/282 [==============================] - 10s 34ms/step - loss: 0.4812 - accuracy: 0.8103 - val_loss: 0.4803 - val_accuracy: 0.8039
Epoch 14/30
282/282 [==============================] - 9s 32ms/step - loss: 0.4412 - accuracy: 0.8260 - val_loss: 0.4275 - val_accuracy: 0.8300
Epoch 15/30
282/282 [==============================] - 9s 31ms/step - loss: 0.3969 - accuracy: 0.8440 - val_loss: 0.3974 - val_accuracy: 0.8426
Epoch 16/30
282/282 [==============================] - 9s 32ms/step - loss: 0.3513 - accuracy: 0.8644 - val_loss: 0.3523 - val_accuracy: 0.8597
Epoch 17/30
282/282 [==============================] - 9s 34ms/step - loss: 0.3163 - accuracy: 0.8781 - val_loss: 0.3121 - val_accuracy: 0.8742
Epoch 18/30
282/282 [==============================] - 9s 33ms/step - loss: 0.2777 - accuracy: 0.8944 - val_loss: 0.2802 - val_accuracy: 0.8890
Epoch 19/30
282/282 [==============================] - 9s 33ms/step - loss: 0.2508 - accuracy: 0.9050 - val_loss: 0.2504 - val_accuracy: 0.9047
Epoch 20/30
282/282 [==============================] - 9s 31ms/step - loss: 0.2191 - accuracy: 0.9190 - val_loss: 0.2302 - val_accuracy: 0.9100
Epoch 21/30
282/282 [==============================] - 9s 33ms/step - loss: 0.1837 - accuracy: 0.9333 - val_loss: 0.1870 - val_accuracy: 0.9284
Epoch 22/30
282/282 [==============================] - 9s 33ms/step - loss: 0.1473 - accuracy: 0.9484 - val_loss: 0.1635 - val_accuracy: 0.9383
Epoch 23/30
282/282 [==============================] - 9s 32ms/step - loss: 0.1230 - accuracy: 0.9582 - val_loss: 0.1351 - val_accuracy: 0.9504
Epoch 24/30
282/282 [==============================] - 10s 34ms/step - loss: 0.1030 - accuracy: 0.9662 - val_loss: 0.1084 - val_accuracy: 0.9624
Epoch 25/30
282/282 [==============================] - 9s 33ms/step - loss: 0.0806 - accuracy: 0.9757 - val_loss: 0.0940 - val_accuracy: 0.9693
Epoch 26/30
282/282 [==============================] - 9s 33ms/step - loss: 0.0699 - accuracy: 0.9796 - val_loss: 0.0734 - val_accuracy: 0.9766
Epoch 27/30
282/282 [==============================] - 9s 33ms/step - loss: 0.0691 - accuracy: 0.9785 - val_loss: 0.1062 - val_accuracy: 0.9625
Epoch 28/30
282/282 [==============================] - 10s 36ms/step - loss: 0.0515 - accuracy: 0.9866 - val_loss: 0.0605 - val_accuracy: 0.9807
Epoch 29/30
282/282 [==============================] - 9s 33ms/step - loss: 0.0431 - accuracy: 0.9885 - val_loss: 0.0599 - val_accuracy: 0.9801
Epoch 30/30
282/282 [==============================] - 10s 36ms/step - loss: 0.0449 - accuracy: 0.9872 - val_loss: 0.0770 - val_accuracy: 0.9740
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model5</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/081155ed1d8cbabb3fbe18aa26c325c29b682e37a04f14ae9aab6c7e557f9638.png" src="../_images/081155ed1d8cbabb3fbe18aa26c325c29b682e37a04f14ae9aab6c7e557f9638.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot1</span><span class="p">(</span><span class="n">history5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8c5ead26641e8e2765f81aadc00150b287dff295502a74f9232e034ea28c6600.png" src="../_images/8c5ead26641e8e2765f81aadc00150b287dff295502a74f9232e034ea28c6600.png" />
<img alt="../_images/68583be24aef86d4aea40f2a5cfc86635d96141ccf2a23f0569c899842993355.png" src="../_images/68583be24aef86d4aea40f2a5cfc86635d96141ccf2a23f0569c899842993355.png" />
</div>
</div>
</section>
</section>
<section id="save-models">
<h2><a class="toc-backref" href="#id35">Save Models</a><a class="headerlink" href="#save-models" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Save model</span>
<span class="n">full_model5</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;keras_models/s2s-addition-attention.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/alvinchen/anaconda3/envs/python-notes/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save(&#39;my_model.keras&#39;)`.
  saving_api.save_model(
</pre></div>
</div>
</div>
</div>
</section>
<section id="interim-comparison">
<h2><a class="toc-backref" href="#id36">Interim Comparison</a><a class="headerlink" href="#interim-comparison" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">history1</span><span class="p">,</span> <span class="n">history2</span><span class="p">,</span> <span class="n">history3</span><span class="p">,</span> <span class="n">history4</span><span class="p">,</span> <span class="n">history5</span><span class="p">]</span>
<span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">history</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VanilaRNN&#39;</span><span class="p">,</span> <span class="s1">&#39;GRU&#39;</span><span class="p">,</span> <span class="s1">&#39;Birdirectional&#39;</span><span class="p">,</span> <span class="s1">&#39;Peeky&#39;</span><span class="p">,</span> <span class="s1">&#39;Attention&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ## Saving all training histories</span>
<span class="c1"># import pickle</span>
<span class="c1"># with open(&#39;keras_models/s2s-attention-addition-history&#39;, &#39;wb&#39;) as f:</span>
<span class="c1">#     pickle.dump(history, f)</span>
<span class="c1"># with open(&#39;keras_models/s2s-attention-addition-history&#39;, &#39;rb&#39;) as f:</span>
<span class="c1">#     history = pickle.load(f)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">acc</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">,</span>
             <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
             <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparing Different Sequence Models&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7902bbefc733dc1fff45e7073822e8609c023fbd5616f5efdd2fdb3ed35fae33.png" src="../_images/7902bbefc733dc1fff45e7073822e8609c023fbd5616f5efdd2fdb3ed35fae33.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span>
             <span class="n">a</span><span class="p">,</span>
             <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
             <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparing Different Sequence Models&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cf5ce07563c0fb22c88baa5e04609770053dc1d45a37bea093ffe562f02af237.png" src="../_images/cf5ce07563c0fb22c88baa5e04609770053dc1d45a37bea093ffe562f02af237.png" />
</div>
</div>
</section>
<section id="attention-model-analysis">
<h2><a class="toc-backref" href="#id37">Attention Model Analysis</a><a class="headerlink" href="#attention-model-analysis" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ## If the model is loaded via external files</span>
<span class="c1"># ## Load the encoder_model, decoder_model this way</span>
<span class="c1"># from keras.models import load_model</span>
<span class="c1"># full_model5.load_weights(&#39;keras_models/s2s-addition-attention.h5&#39;)</span>
<span class="c1"># full_model5.compile(optimizer=&#39;adam&#39;,</span>
<span class="c1">#                     loss=&#39;categorical_crossentropy&#39;,</span>
<span class="c1">#                     metrics=[&#39;accuracy&#39;])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Let&#39;s look at the attention-based model</span>
<span class="n">full_model</span> <span class="o">=</span> <span class="n">full_model5</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="inference">
<h2><a class="toc-backref" href="#id38">Inference</a><a class="headerlink" href="#inference" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>At the inference stage, we use the trained model to decode input sequences.</p></li>
<li><p>In decoding, it should be noted that Decoder would decode one word at a time.</p></li>
<li><p>We set up <strong>Inference-Encoder</strong> and <strong>Inference-Decoder</strong> based on the trained model. We need to identify the right layer from the trained model for the use in inference.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_5&quot;
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 encoder_inputs (InputLayer  [(None, 7, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 decoder_inputs (InputLayer  [(None, 5, 12)]              0         []                            
 )                                                                                                
                                                                                                  
 encoder_gru (GRU)           [(None, 7, 256),             207360    [&#39;encoder_inputs[0][0]&#39;]      
                              (None, 256)]                                                        
                                                                                                  
 decoder_gru (GRU)           [(None, 5, 256),             207360    [&#39;decoder_inputs[0][0]&#39;,      
                              (None, 256)]                           &#39;encoder_gru[0][1]&#39;]         
                                                                                                  
 attention_layer (Attention  ((None, 5, 256),             0         [&#39;decoder_gru[0][0]&#39;,         
 )                            (None, 5, 7))                          &#39;encoder_gru[0][0]&#39;]         
                                                                                                  
 concat_layer (Concatenate)  (None, 5, 512)               0         [&#39;decoder_gru[0][0]&#39;,         
                                                                     &#39;attention_layer[0][0]&#39;]     
                                                                                                  
 time_distributed_layer (Ti  (None, 5, 12)                6156      [&#39;concat_layer[0][0]&#39;]        
 meDistributed)                                                                                   
                                                                                                  
==================================================================================================
Total params: 420876 (1.61 MB)
Trainable params: 420876 (1.61 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<section id="inference-encoder">
<h3><a class="toc-backref" href="#id39">Inference Encoder</a><a class="headerlink" href="#inference-encoder" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Inference-Encoder</span>
<span class="n">encoder_inf_inputs</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">encoder_inf_out</span><span class="p">,</span> <span class="n">encoder_inf_state</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>
<span class="n">encoder_inf_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">encoder_inf_inputs</span><span class="p">,</span>
                          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inf_out</span><span class="p">,</span> <span class="n">encoder_inf_state</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">encoder_inf_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/22fa37eb7b0e70295f73816a4bd869772aa4f3c5e186faf30d3241af62c38154.png" src="../_images/22fa37eb7b0e70295f73816a4bd869772aa4f3c5e186faf30d3241af62c38154.png" />
</div>
</div>
</section>
<section id="inference-decoder">
<h3><a class="toc-backref" href="#id40">Inference Decoder</a><a class="headerlink" href="#inference-decoder" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Inference-Decoder requires two inputs:</p>
<ul>
<li><p>Encoder’s last hidden state as its initial hidden state</p></li>
<li><p>The input token of the target sequence (default start token: <code class="docutils literal notranslate"><span class="pre">_</span></code>)</p></li>
</ul>
</li>
<li><p>Inference-Attention requires two inputs:</p>
<ul>
<li><p>Encoder’s all hidden states as the <strong>values</strong> and <strong>keys</strong></p></li>
<li><p>Inference-Decoder’s hidden state as the <strong>query</strong></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Inference-Decoder Input (1): The input token from the target sequence </span>
    <span class="c1">## one token at each time</span>
    <span class="c1">## the default is the start token &#39;_&#39;</span>
<span class="n">decoder_inf_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inf_inputs&#39;</span><span class="p">)</span>  <span class="c1">## Initial Decoder&#39;s Output Token &#39;_&#39;</span>

<span class="c1">## Inference-Decoder Input (2): All hidden states from Inference-Encoder</span>
<span class="n">encoder_inf_states</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inf_states&#39;</span><span class="p">)</span>

<span class="c1">## Inference-Decoder Initial Hidden State = Inference-Encoder&#39;s last h</span>
<span class="n">decoder_init_state</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">),</span>
                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_init&#39;</span><span class="p">)</span>  <span class="c1">## initial c from encoder</span>

<span class="c1">## Inference-Decoder</span>
<span class="n">decoder_inf_gru</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">decoder_inf_out</span><span class="p">,</span> <span class="n">decoder_inf_state</span> <span class="o">=</span> <span class="n">decoder_inf_gru</span><span class="p">(</span>
    <span class="n">decoder_inf_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">decoder_init_state</span><span class="p">)</span>


<span class="c1">## Inference-Attention</span>
<span class="n">decoder_inf_attention</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="n">attn_inf_out</span><span class="p">,</span> <span class="n">attn_inf_weights</span> <span class="o">=</span> <span class="n">decoder_inf_attention</span><span class="p">(</span>
    <span class="p">[</span><span class="n">decoder_inf_out</span><span class="p">,</span> <span class="n">encoder_inf_states</span><span class="p">],</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">## Inference-Concatenate</span>
<span class="n">decoder_inf_concat</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span>
    <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat&#39;</span><span class="p">)([</span><span class="n">decoder_inf_out</span><span class="p">,</span> <span class="n">attn_inf_out</span><span class="p">])</span>

<span class="c1">## Inference-Dense</span>
<span class="n">decoder_inf_timedense</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
<span class="n">decoder_inf_pred</span> <span class="o">=</span> <span class="n">decoder_inf_timedense</span><span class="p">(</span><span class="n">decoder_inf_concat</span><span class="p">)</span>

<span class="c1">## Inference-Model</span>
<span class="n">decoder_inf_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inf_states</span><span class="p">,</span> <span class="n">decoder_init_state</span><span class="p">,</span> <span class="n">decoder_inf_inputs</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">decoder_inf_pred</span><span class="p">,</span> <span class="n">attn_inf_weights</span><span class="p">,</span> <span class="n">decoder_inf_state</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">decoder_inf_model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/60b99c385b046363f1fe71d506bb8620233a2868632b9287e312269c4979c30c.png" src="../_images/60b99c385b046363f1fe71d506bb8620233a2868632b9287e312269c4979c30c.png" />
</div>
</div>
</section>
<section id="decoding-input-sequences">
<h3><a class="toc-backref" href="#id41">Decoding Input Sequences</a><a class="headerlink" href="#decoding-input-sequences" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The Inference-Encoder processes the tokens of input sequences to get (a) all hidden states, and (b) the last hidden state.</p></li>
<li><p>The Inference-Decoder uses the Inference-Encoder’s last hidden state as the initial hidden state.</p></li>
<li><p>The Inference-Decoder uses the token <code class="docutils literal notranslate"><span class="pre">_</span></code> as the initial token of the target sequence for decoding.</p></li>
<li><p>At the subsequent decoding steps, Inference-Decoder updates its hidden state and the decoded token as inputs for the next-round decoding.</p></li>
</ul>
<ul class="simple">
<li><p>Inference-Decoder is different from the training Decoder in that:</p>
<ul>
<li><p>The latter takes in <strong>all the correct target sequences</strong> (i.e., <code class="docutils literal notranslate"><span class="pre">decoder_input_onehot</span></code>) for <strong>teacher forcing</strong>.</p></li>
<li><p>The former takes in <strong>one</strong> target token, which is predicted by the Inference-Decoder at the previous decoding step.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decode_sequence</span><span class="p">(</span><span class="n">input_seq</span><span class="p">):</span>

    <span class="c1">## Initialize target output character &quot;_&quot;</span>
    <span class="n">initial_text</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span>
    <span class="n">initial_seq</span> <span class="o">=</span> <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">initial_text</span><span class="p">)</span>
    <span class="n">test_dec_onehot_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="n">to_categorical</span><span class="p">(</span><span class="n">initial_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1">## Inference-Encoder processes input sequence</span>
    <span class="n">enc_outs</span><span class="p">,</span> <span class="n">enc_last_state</span> <span class="o">=</span> <span class="n">encoder_inf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>

    <span class="c1">## Update Inference-Decoder initial hidden state</span>
    <span class="n">dec_state</span> <span class="o">=</span> <span class="n">enc_last_state</span>

    
    <span class="c1">## Holder for attention weights and decoded texts</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dec_text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="c1">## Inference-Decoder decoding step-by-step</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_maxlen</span><span class="p">):</span>
        <span class="n">dec_out</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dec_state</span> <span class="o">=</span> <span class="n">decoder_inf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="p">[</span><span class="n">enc_outs</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">test_dec_onehot_seq</span><span class="p">])</span>
         
        <span class="c1">## Decoded Output (one-hot to integer)</span>
        <span class="n">dec_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="c1">## Stopping Condition</span>
        <span class="k">if</span> <span class="n">dec_ind</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1">## Decoded Output (integer to char)</span>
        <span class="n">initial_text</span> <span class="o">=</span> <span class="n">dec_index2word</span><span class="p">[</span><span class="n">dec_ind</span><span class="p">]</span>
        
        <span class="c1">## Decoded Output for next-round decoding</span>
        <span class="n">initial_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">dec_ind</span><span class="p">]</span> <span class="c1">#target_tokenizer.texts_to_sequences(initial_text)</span>
        <span class="n">test_dec_onehot_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">to_categorical</span><span class="p">(</span><span class="n">initial_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1">## Keep track of attention weights for current decoded output</span>
        <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">dec_ind</span><span class="p">,</span> <span class="n">attention</span><span class="p">))</span>

        <span class="c1">## Append the predicted char</span>
        <span class="n">dec_text</span> <span class="o">+=</span> <span class="n">dec_index2word</span><span class="p">[</span><span class="n">dec_ind</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">dec_text</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Test sequence-decoding function</span>
<span class="c1">## on first 20 training samples</span>

<span class="k">for</span> <span class="n">seq_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">decoded_sentence</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decode_sequence</span><span class="p">(</span>
        <span class="n">encoder_input_onehot</span><span class="p">[</span><span class="n">seq_index</span><span class="p">:</span><span class="n">seq_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Input sentence:&#39;</span><span class="p">,</span> <span class="n">tr_input_texts</span><span class="p">[</span><span class="n">seq_index</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Decoded sentence:&#39;</span><span class="p">,</span> <span class="n">decoded_sentence</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 122ms/step
1/1 [==============================] - 0s 118ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 27+673
Decoded sentence: 700_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 153+27
Decoded sentence: 180_
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 93+901
Decoded sentence: 994_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 243+678
Decoded sentence: 921_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 269+46
Decoded sentence: 315_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 10ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 235+891
Decoded sentence: 1125_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 10ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 46+290
Decoded sentence: 336_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 324+947
Decoded sentence: 1271_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 721+49
Decoded sentence: 770_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 535+7
Decoded sentence: 542_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 10ms/step
1/1 [==============================] - 0s 9ms/step
-
Input sentence: 45+117
Decoded sentence: 162_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 669+174
Decoded sentence: 843_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 904+7
Decoded sentence: 911_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 22+731
Decoded sentence: 753_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 83+742
Decoded sentence: 825_
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 10ms/step
1/1 [==============================] - 0s 9ms/step
-
Input sentence: 678+983
Decoded sentence: 1661_
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 10ms/step
1/1 [==============================] - 0s 11ms/step
1/1 [==============================] - 0s 9ms/step
-
Input sentence: 240+42
Decoded sentence: 282_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 18+44
Decoded sentence: 62_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 10ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 4+166
Decoded sentence: 170_
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
-
Input sentence: 731+13
Decoded sentence: 744_
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="plotting-attention">
<h2><a class="toc-backref" href="#id42">Plotting Attention</a><a class="headerlink" href="#plotting-attention" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ind</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">doc_inputs</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">decode_sequence</span><span class="p">(</span><span class="n">encoder_input_onehot</span><span class="p">[</span><span class="n">ind</span><span class="p">:</span><span class="n">ind</span> <span class="o">+</span>
                                                                     <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
<span class="n">mats</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dec_inputs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">dec_ind</span><span class="p">,</span> <span class="n">attn</span> <span class="ow">in</span> <span class="n">attention_weights</span><span class="p">:</span>
    <span class="n">mats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">dec_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dec_ind</span><span class="p">)</span>

<span class="n">attention_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mats</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_mat</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">attention_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">attention_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span>
    <span class="p">[</span><span class="n">dec_index2word</span><span class="p">[</span><span class="n">inp</span><span class="p">]</span> <span class="k">if</span> <span class="n">inp</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;&lt;PAD&gt;&quot;</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">dec_inputs</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span>
    <span class="n">enc_index2word</span><span class="p">[</span><span class="n">inp</span><span class="p">]</span> <span class="k">if</span> <span class="n">inp</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;&lt;PAD&gt;&quot;</span>
    <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">encoder_input_sequences</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
</pre></div>
</div>
<img alt="../_images/2517baa358eede428ed657d490e03c09b910c49250e368e81c3e49d76d5e591c.png" src="../_images/2517baa358eede428ed657d490e03c09b910c49250e368e81c3e49d76d5e591c.png" />
</div>
</div>
</section>
<section id="evaluation-on-testing-data">
<h2><a class="toc-backref" href="#id43">Evaluation on Testing Data</a><a class="headerlink" href="#evaluation-on-testing-data" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Wrap text vectorization in a function.</p></li>
<li><p>Vectorize the testing data in the same way as the training data</p>
<ul>
<li><p>Texts to sequences</p></li>
<li><p>Pad sequences</p></li>
<li><p>One-hot encode sequences</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_data</span><span class="p">(</span><span class="n">enc_tokenizer</span><span class="p">,</span> <span class="n">dec_tokenizer</span><span class="p">,</span> <span class="n">enc_text</span><span class="p">,</span> <span class="n">dec_text</span><span class="p">,</span>
                    <span class="n">enc_maxlen</span><span class="p">,</span> <span class="n">dec_maxlen</span><span class="p">,</span> <span class="n">enc_vsize</span><span class="p">,</span> <span class="n">dec_vsize</span><span class="p">):</span>
    <span class="n">enc_seq</span> <span class="o">=</span> <span class="n">enc_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">enc_text</span><span class="p">)</span>
    <span class="n">enc_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">enc_seq</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">enc_maxlen</span><span class="p">)</span>
    <span class="n">enc_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">enc_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">enc_vsize</span><span class="p">)</span>

    <span class="n">dec_seq</span> <span class="o">=</span> <span class="n">dec_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">dec_text</span><span class="p">)</span>
    <span class="n">dec_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">dec_seq</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">dec_maxlen</span><span class="p">)</span>
    <span class="n">dec_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">dec_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">dec_vsize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">enc_onehot</span><span class="p">,</span> <span class="n">dec_onehot</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_encoder_input_onehot</span><span class="p">,</span> <span class="n">ts_decoder_target_onehot</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span>
    <span class="n">input_tokenizer</span><span class="p">,</span> <span class="n">target_tokenizer</span><span class="p">,</span> <span class="n">ts_input_texts</span><span class="p">,</span> <span class="n">ts_target_texts</span><span class="p">,</span>
    <span class="n">input_maxlen</span><span class="p">,</span> <span class="n">target_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">ts_encoder_input_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ts_decoder_target_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 7, 12)
(5000, 6, 12)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_model5</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">ts_encoder_input_onehot</span><span class="p">,</span> <span class="n">ts_decoder_target_onehot</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]],</span>
    <span class="n">ts_decoder_target_onehot</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>40/40 - 1s - loss: 0.0752 - accuracy: 0.9746 - 522ms/epoch - 13ms/step
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.07519256323575974, 0.9746000170707703]
</pre></div>
</div>
</div>
</div>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id44">References</a><a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>This unit is based on Chapter 7 of <a class="reference external" href="https://www.tenlong.com.tw/products/9789865020675">Deep Learning 2用 Python 進行自然語言處理的基礎理論實作</a> and the original dataset is provided in the book and modified for this unit.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="dl-attention-transformer-intuition.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Attention and Transformers: Intuitions</p>
      </div>
    </a>
    <a class="right-next"
       href="dl-transformers-keras.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sentiment Classification with Transformer (Self-Study)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-dependencies">Set up Dependencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-hyperparameters">Deep Learning Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split">Train-Test Split</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data Preprocessing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-sequences">Text to Sequences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-considerations-for-decoder-s-input-and-output">Special Considerations for Decoder’s Input and Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequences-to-one-hot-encoding">Sequences to One-Hot Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-indices">Token Indices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1-vanilla-rnn">Model 1 (Vanilla RNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-model">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2-gru">Model 2 (GRU)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-3-birdirectional">Model 3 (Birdirectional)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-4-peeky-decoder">Model 4 (Peeky Decoder)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-5-attention">Model 5 (Attention)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Define Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#save-models">Save Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-comparison">Interim Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-model-analysis">Attention Model Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-encoder">Inference Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-decoder">Inference Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-input-sequences">Decoding Input Sequences</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-attention">Plotting Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-on-testing-data">Evaluation on Testing Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>