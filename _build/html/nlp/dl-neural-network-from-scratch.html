

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1. Neural Network From Scratch &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/dl-neural-network-from-scratch';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Deep Learning: A Simple Example" href="dl-simple-case.html" />
    <link rel="prev" title="3. Emsemble Learning" href="ml-emsemble-learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">1. Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">2. Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">3. Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">1. Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">2. Topic Modeling: A Naive Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">3. Emsemble Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">2. Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">3. Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">1. Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">2. Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">3. Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Seq2Seq, Attention, Transformers, and Transfer Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">1. Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">2. Sequence Model with Attention for Addition Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-transformers-keras.html">3. Sentiment Classification with Transformer (Self-Study)</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment-analysis-using-bert-keras-movie-reviews.html">4. Transfer Learning With BERT (Self-Study)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2-journal-review.html">Assignment II: Journal Articles Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-preprocessing.html">Assignment III: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-chinese-nlp.html">Assignment IV: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-text-vectorization.html">Assignment V: Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6-machine-learning.html">Assignment VI: Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7-topic-modeling.html">Assignment VII: Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/8-dl-chinese-name-gender.html">Assignment VIII: Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/9-sentiment-analysis-dl.html">Assignment IX: Sentiment Analysis Using Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/10-neural-language-model.html">Assignment X: Neural Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/11-word2vec.html">Assignment XI: Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/12-encoder-decoder.html">Assignment XII: Encoder-Decoder Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/13-attention.html">Assignment XIII: Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../temp/final-project.html">Final Exam</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-neural-network-from-scratch.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/dl-neural-network-from-scratch.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural Network From Scratch</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-of-neural-network">1.1. Workflow of Neural Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-overview">1.2. Neural Network Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation">Backward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons">Neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions-in-python">Activation Functions in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-nodes-to-layers">From Nodes to Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-parameters-and-matrix-mutiplication">Layer, Parameters, and Matrix Mutiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-neural-networks">Types of Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-neural-network-model-in-python">1.3. Building a Neural Network Model in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-in-focus">Packages in Focus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model">Create Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-modules-in-keras">Useful Modules in keras</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-utils"><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-layers"><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-modules">Other modules</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-example-of-model-training">1.4. A Quick Example of Model Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-and-prediction">Evaluation and Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-and-training">1.5. Learning and Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-neural-network-learn-the-parameters">How does the neural network learn the parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principles-for-loss-functions">Principles for Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-variables-encoding">1.6. Categorical Variables Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-wrong-with-categorical-variables">What’s wrong with categorical variables?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-ways-of-encodings">Three Ways of Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-and-loss-function">One-hot Encoding and Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-loss-errors-skipped">1.7. Examples of Loss Errors (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-based-on-one-sample">Error based on One Sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#errors-based-on-batch-samples">Errors based on Batch Samples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">1.8. Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-adjust-the-model-parameters">How do we adjust the model parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-gradients-an-example-skipped">Intuition of Gradients: An Example (skipped)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent">Types of Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-in-python-skipped">Gradients in Python (skipped)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting">1.9. Overfitting and Underfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-vs-generalization">Optimization vs. Generalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-underfitting">Overfitting/Underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-overfitting">How to Deal With Overfitting?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristics-for-regularization">Heuristics for Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization-and-layer-normalization">Batch Normalization and Layer Normalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-math-skipped">1.10. Some Math (Skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-and-back-propagation">Chain Rule and Back Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementwise-operations-of-matrix">Elementwise Operations of Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcast">Broadcast</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production">Matrices Dot Production</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production-and-forward-propagation">Matrices Dot Production and Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives">Derivatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives">Partial Derivatives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">1.11. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-network-from-scratch">
<h1><span class="section-number">1. </span>Neural Network From Scratch<a class="headerlink" href="#neural-network-from-scratch" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>This lecture note introduces fundamentals of the mechanism for neural network, or deep learning.</p></li>
</ul>
<ul class="simple">
<li><p>We will break our discussions into three main parts:</p>
<ul>
<li><p>Building A Neural Network (How does the network work?)</p>
<ul>
<li><p>Forward Propagation</p></li>
<li><p>Weights, Biases, and Activation functions</p></li>
<li><p>Matrix multiplication</p></li>
</ul>
</li>
<li><p>Learning and Training (How does it learn?)</p>
<ul>
<li><p>Loss Function</p></li>
<li><p>Gradients</p></li>
<li><p>Derivatives and Partial Derivatives</p></li>
<li><p>Gradient Descent</p></li>
</ul>
</li>
<li><p>Gradient Descent (More on how does it learn.)</p>
<ul>
<li><p>Batch</p></li>
<li><p>Mini-batch</p></li>
<li><p>Stochastic gradient descent</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Loading Dependencies</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<section id="workflow-of-neural-network">
<h2><span class="section-number">1.1. </span>Workflow of Neural Network<a class="headerlink" href="#workflow-of-neural-network" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/nn-flowchart.png" /></p>
</section>
<section id="neural-network-overview">
<h2><span class="section-number">1.2. </span>Neural Network Overview<a class="headerlink" href="#neural-network-overview" title="Permalink to this headline">#</a></h2>
<section id="deep-learning">
<h3>Deep Learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/neural-network-propagation.gif" /></p>
</section>
<section id="forward-propagation">
<h3>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural network is a type of machine learning algorithm modeled on human brains and nervous system.</p></li>
<li><p>The model is believed to process information in a similar way to the human brain:</p>
<ul>
<li><p>inputs and weights as the <strong>dendrites</strong></p></li>
<li><p>neuron operations of weighted sums and activation as <strong>neurons</strong></p></li>
<li><p>outputs as <strong>axons</strong></p></li>
</ul>
</li>
<li><p>A neural network often consists of a large number of elements, known as <strong>nodes</strong>, working in parallel to solve a specific problem. These nodes are often organized into different <strong>layers</strong>.</p></li>
<li><p>Each layer of the network transforms the input values into the output values based on the weights (parameters) of the nodes.</p></li>
<li><p>The data transformation from the input to the output is in general referred to as <strong>forward propagation</strong> of the network.</p></li>
</ul>
</section>
<section id="backward-propagation">
<h3>Backward Propagation<a class="headerlink" href="#backward-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>When the predicted output is compared with the true label, we can evaluate the network performance by computing the <strong>loss</strong> of the network.</p></li>
<li><p>Then we determine the proportion of the losses that may be attributed to each model parameter. This process goes from the losses of the predicted output backward to the original inputs. This step is referred to as the <strong>back propagation</strong> of the network.</p></li>
</ul>
</section>
<section id="neurons">
<h3>Neurons<a class="headerlink" href="#neurons" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural network consists of neurons, which allow us to model non-linear relationships between input and output data.</p></li>
<li><p>Given an input vector, traditional linear transformation can only model a linear relationship between X and y:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\hat{y} = w_0 + w_1 x_1 + w_2x_2 + w_3x_3 +...+w_nx_n
\]</div>
<ul class="simple">
<li><p>A neron is like a linear transformation but with an extra <strong>activation function</strong>.</p></li>
<li><p>This mechanism of activation function in each neuron will ultimately determine the output of the neuron.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\textit{Neuron Output Value} &amp; =  h(\hat{y}) \\
&amp; = h(w_0 + w_1 x_1 + w_2x_2 + w_3x_3 +...+w_nx_n)
\end{align}\end{split}\]</div>
<p><img alt="" src="../_images/neuron.png" /></p>
</section>
<section id="activation-functions">
<h3>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In neural network, the activation function of a node determines whether the node would activate the output given the <strong>weighted sum of the input values</strong>.</p></li>
<li><p>Different types of activation functions may determine the cut-offs for output activation in different ways.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Sigmoid</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into values within the range of 0 and 1 (i.e., a probability-like value).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ h(y) = \frac{1}{1 + \exp(-y)}\]</div>
<ul class="simple">
<li><p><strong>Step</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into binary ones, with only the positive values activated.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(y)= \left\{ 
     \begin{array}\\
     0 &amp; (y \leq 0) \\
     1 &amp; (y &gt; 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p><strong>ReLU</strong> (Rectified Linear Unit) function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values by passing only positive values and zero for negative <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(y)= \left\{ 
     \begin{array}\\
     y &amp; (y &gt; 0) \\
     0 &amp; (y \leq 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Softmax</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into normalized probability values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h(y_k) = \frac{\exp(y_k)}{\sum_{i = 1}^{n} \exp({y_i})}
\]</div>
</section>
<section id="activation-functions-in-python">
<h3>Activation Functions in Python<a class="headerlink" href="#activation-functions-in-python" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="c1"># def softmax(x):</span>
<span class="c1">#     exp_x = np.exp(x)</span>
<span class="c1">#     sum_exp_x = np.sum(exp_x)</span>
<span class="c1">#     y = exp_x/sum_exp_x</span>
<span class="c1">#     return y</span>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>  <span class="c1"># avoid overflow issues</span>
    <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">sum_exp_x</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/249f766bf4d74a963a5e3cc97a966451cb4c2fd7384220862a299df80f9bc51e.png" src="../_images/249f766bf4d74a963a5e3cc97a966451cb4c2fd7384220862a299df80f9bc51e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## sigmoid function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/33a74037b6fb32335a1886afc2a94757a9179fbc9ce89fc187b8f9aaf4c3ce46.png" src="../_images/33a74037b6fb32335a1886afc2a94757a9179fbc9ce89fc187b8f9aaf4c3ce46.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ReLU</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/28fe0dc3c9d5d2cea82040e9656db0a1d85cb6d654a9100bba3709101c15ebe1.png" src="../_images/28fe0dc3c9d5d2cea82040e9656db0a1d85cb6d654a9100bba3709101c15ebe1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.   0.01 0.05 0.95]
</pre></div>
</div>
</div>
</div>
</section>
<section id="from-nodes-to-layers">
<h3>From Nodes to Layers<a class="headerlink" href="#from-nodes-to-layers" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A neural network can be defined in terms of <strong>depths</strong> and <strong>widths</strong> of its layers.</p>
<ul>
<li><p><strong>Depth</strong>: How many layers does the network have?</p></li>
<li><p><strong>Width</strong>: How many neurons does each layer have?</p></li>
</ul>
</li>
<li><p>A network can consist of several layers.</p></li>
<li><p>Each layer can have various numbers of neurons.</p></li>
<li><p>For each layer, the shape of the input tensor, the number of its neurons, and the shape of its output are inter-connected. These settings will determine the number of parameters (i.e., <strong>weights</strong>) needed to train.</p></li>
</ul>
<p><img alt="" src="../_images/neural-network-dense-layer.gif" /></p>
</section>
<section id="layer-parameters-and-matrix-mutiplication">
<h3>Layer, Parameters, and Matrix Mutiplication<a class="headerlink" href="#layer-parameters-and-matrix-mutiplication" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Each layer transforms the input values into the output values based on its layer parameters.</p></li>
<li><p>Mathematically, these values transformation is a matrix multiplication, running in parallel for all nodes of the layer.</p></li>
<li><p>In Deep Learning, the input and output values are represented as a multi-dimensional tensor.</p>
<ul>
<li><p>A 1D tensor is a vector.</p></li>
<li><p>A 2D tensor is a two-dimensional array.</p></li>
<li><p>A 3D tensor is a three-dimensional array.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/neural-network-dense-layer-1.gif" /></p>
<p><img alt="" src="../_images/neural-network-dense-layer-2.gif" /></p>
</section>
<section id="types-of-neural-networks">
<h3>Types of Neural Networks<a class="headerlink" href="#types-of-neural-networks" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Multi-Layer Perceptron</strong> (Fully Connected Network)</p>
<ul>
<li><p>Input Layer, one or more hidden layers, and output layer.</p></li>
<li><p>A hidden layer consists of neurons (perceptrons) which process certain aspect of the features and send the processed information into the next hidden layer.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Convolutional Neural Network (CNN)</strong></p>
<ul>
<li><p>Mainly for image and audio processing</p></li>
<li><p>Convolution Layer, Pooling Layer, Fully Connected Layer</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Recurrent Neural Network (RNN)</strong></p>
<ul>
<li><p>Preferred methods in NLP</p></li>
<li><p>Most fully-connected networks do not remember the steps from previous situations and therefore do not learn to make decisions based on <strong>context</strong> in training.</p></li>
<li><p>RNN stores the past information and all its decisions are taken from what it has learned from the past.</p></li>
<li><p>RNN is effective in dealing with time-series data (e.g., text, speech).</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/s2s-rnn.jpeg" /></p>
</section>
</section>
<section id="building-a-neural-network-model-in-python">
<h2><span class="section-number">1.3. </span>Building a Neural Network Model in Python<a class="headerlink" href="#building-a-neural-network-model-in-python" title="Permalink to this headline">#</a></h2>
<section id="packages-in-focus">
<h3>Packages in Focus<a class="headerlink" href="#packages-in-focus" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>: It is an open source machine learning library used for numerical computational tasks developed by Google.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code>: It is a high level API built on top of Tensorflow. It originated as an independent library and now has been incorporated as part of Tensorflow 2+.</p></li>
</ul>
<ul class="simple">
<li><p>Usually we need to define the architecture of the neural network model in terms of <strong>depths</strong> and <strong>widths</strong> of the layers.</p></li>
<li><p>After we define the structure of the network and initialize the values for all parameters, the training requires an iterative processing involving:</p>
<ul>
<li><p><strong>Forward Propagation</strong>: It refers to the process of transforming the data values by moving the input data through the network to get output.</p></li>
<li><p>Define your <strong>loss function</strong>.</p></li>
<li><p>Calculate <strong>Total Error</strong> based on the loss function.</p></li>
<li><p>Calculate <strong>Gradients</strong> via <strong>Back Propagation</strong></p></li>
<li><p>Update the <strong>weights</strong> based on gradients.</p></li>
<li><p>Iterate the process until the stop-condition is reached.</p></li>
</ul>
</li>
</ul>
</section>
<section id="create-model">
<h3>Create Model<a class="headerlink" href="#create-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code>, we can create models in two ways:</p>
<ul>
<li><p>Sequential API (<code class="docutils literal notranslate"><span class="pre">keras.Sequential</span></code>)</p></li>
<li><p>Functional API (<code class="docutils literal notranslate"><span class="pre">keras.model</span></code>)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="c1">## Sequential API to create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_1&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_2&quot;</span><span class="p">))</span>

<span class="c1"># Sequential API (Another alternative)</span>
<span class="c1"># model = keras.Sequential(</span>
<span class="c1">#     [</span>
<span class="c1">#         keras.Input(shape=(2)),</span>
<span class="c1">#         layers.Dense(4, activation=&quot;relu&quot;),</span>
<span class="c1">#         layers.Dense(2, activation=&quot;relu&quot;)</span>
<span class="c1">#     ]</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Functional API (A bit more flexible)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_1&quot;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_2&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you create the model using the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> method, you will not be able to check the <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> or <code class="docutils literal notranslate"><span class="pre">plot_model()</span></code> until the model has been compiled and fitted.</p>
</div>
<ul class="simple">
<li><p>Two ways to inspect the model architecture:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model.summary()</span></code>: A printed summary of the model structure</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow.keras.utils.plot_model(model)</span></code>: A visual representation of the model structure</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
dense_layer_1 (Dense)        (None, 4)                 12        
_________________________________________________________________
dense_layer_2 (Dense)        (None, 2)                 10        
=================================================================
Total params: 22
Trainable params: 22
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_layer_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/70b7b70ede4b5e4dfd733c47ecc0256b99adee4f2e8c9f15f9d95900ff44d1b5.png" src="../_images/70b7b70ede4b5e4dfd733c47ecc0256b99adee4f2e8c9f15f9d95900ff44d1b5.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you see error messages when running the <code class="docutils literal notranslate"><span class="pre">plot_model()</span></code>, please follow the instructions provided in the error messages and install:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pydot</span></code></p></li>
<li><p>Install <a class="reference external" href="https://graphviz.gitlab.io/download/">graphviz</a> (choose the installation methods depending on your OS).</p></li>
</ul>
</div>
</section>
<section id="useful-modules-in-keras">
<h3>Useful Modules in keras<a class="headerlink" href="#useful-modules-in-keras" title="Permalink to this headline">#</a></h3>
<section id="keras-utils">
<h4><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code>:<a class="headerlink" href="#keras-utils" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">to_categorical()</span></code>: To convert a class/label list into a one-hot encoding matrix</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plot_model()</span></code>: Plot the model structure</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Text Labels to Sequences</span>
<span class="n">labels_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 2, 1, 1, 0, 0]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Sequences to One-Hot Encoding</span>
<span class="n">to_categorical</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 0., 1.],
       [0., 0., 1.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [1., 0., 0.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="keras-layers">
<h4><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code><a class="headerlink" href="#keras-layers" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Dense</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SimpleRNN</span></code>, <code class="docutils literal notranslate"><span class="pre">LSTM</span></code>, <code class="docutils literal notranslate"><span class="pre">GRU</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">Flatten</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Attention</span></code>, <code class="docutils literal notranslate"><span class="pre">AdditiveAttention</span></code></p></li>
</ul>
</section>
<section id="other-modules">
<h4>Other modules<a class="headerlink" href="#other-modules" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">keras.losses</span></code>: Loss functions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers</span></code>: Optimization is an important process which optimizes the input weights by comparing the prediction and the loss function.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers.RMSprop</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers.Adam</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimziers.SGD</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.metrics</span></code>: Metrics for model evaluation</p></li>
</ul>
</section>
</section>
</section>
<section id="a-quick-example-of-model-training">
<h2><span class="section-number">1.4. </span>A Quick Example of Model Training<a class="headerlink" href="#a-quick-example-of-model-training" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/keras-workflow.png" /></p>
<ul class="simple">
<li><p>Steps for Model Training</p>
<ul>
<li><p>Create model</p></li>
<li><p>Compile model</p></li>
<li><p>Fit model</p></li>
<li><p>Evaluate model</p></li>
<li><p>Predict</p></li>
</ul>
</li>
</ul>
<section id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>We create random samples, where each sample is characterized by two random numbers (e.g., grades in subjects)</p></li>
<li><p>Each sample is also labeled with a binary class label (e.g., fail or pass)</p></li>
<li><p>We create three datasets: <strong>training</strong>, <strong>validation</strong>, and <strong>testing</strong> sets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># train set</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># val set</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># test set</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100, 2)
(100,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.69646919 0.28613933]
 [0.22685145 0.55131477]
 [0.71946897 0.42310646]
 [0.9807642  0.68482974]
 [0.4809319  0.39211752]]
[1 1 0 1 0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-training">
<h3>Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compile</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>epochs</strong>: the number of times that the entire training set is passed forward and backward through the neural network.</p></li>
<li><p><strong>batch_size</strong>: the number of samples that are used for parameter updates each time (i.e., passed through the network forward and backward)</p></li>
</ul>
<p>If we specify <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">=</span> <span class="pre">10</span></code> and <code class="docutils literal notranslate"><span class="pre">epochs=50</span></code>, the model will try to train the model parameters (i.e., all the weights in the layers) by running through the entire training set <strong>50</strong> times (epochs, or iterations).</p>
<p>And in each epoch, the model will update the model parameters according to the average loss values based on a batch of <strong>10</strong> samples.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
10/10 [==============================] - 2s 140ms/step - loss: 0.7009 - accuracy: 0.5001 - val_loss: 0.6997 - val_accuracy: 0.4600
Epoch 2/50
10/10 [==============================] - 0s 5ms/step - loss: 0.7011 - accuracy: 0.5143 - val_loss: 0.6942 - val_accuracy: 0.4700
Epoch 3/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.4545 - val_loss: 0.6908 - val_accuracy: 0.5400
Epoch 4/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5198 - val_loss: 0.6904 - val_accuracy: 0.5400
Epoch 5/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6823 - accuracy: 0.5806 - val_loss: 0.6905 - val_accuracy: 0.5400
Epoch 6/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6837 - accuracy: 0.5361 - val_loss: 0.6913 - val_accuracy: 0.5400
Epoch 7/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6836 - accuracy: 0.4996 - val_loss: 0.6927 - val_accuracy: 0.5400
Epoch 8/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6824 - accuracy: 0.5652 - val_loss: 0.6934 - val_accuracy: 0.5200
Epoch 9/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6745 - accuracy: 0.6086 - val_loss: 0.6948 - val_accuracy: 0.5100
Epoch 10/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5183 - val_loss: 0.6962 - val_accuracy: 0.4700
Epoch 11/50
10/10 [==============================] - 0s 15ms/step - loss: 0.6739 - accuracy: 0.6345 - val_loss: 0.6977 - val_accuracy: 0.4900
Epoch 12/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5676 - val_loss: 0.6985 - val_accuracy: 0.4900
Epoch 13/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6720 - accuracy: 0.6536 - val_loss: 0.7001 - val_accuracy: 0.4900
Epoch 14/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.5924 - val_loss: 0.7015 - val_accuracy: 0.4900
Epoch 15/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6748 - accuracy: 0.6337 - val_loss: 0.7029 - val_accuracy: 0.4800
Epoch 16/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6884 - accuracy: 0.6205 - val_loss: 0.7033 - val_accuracy: 0.4900
Epoch 17/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6656 - accuracy: 0.6724 - val_loss: 0.7046 - val_accuracy: 0.4900
Epoch 18/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6613 - accuracy: 0.6567 - val_loss: 0.7063 - val_accuracy: 0.4900
Epoch 19/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.6101 - val_loss: 0.7072 - val_accuracy: 0.5000
Epoch 20/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6560 - accuracy: 0.6835 - val_loss: 0.7093 - val_accuracy: 0.5300
Epoch 21/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6715 - accuracy: 0.6667 - val_loss: 0.7110 - val_accuracy: 0.5200
Epoch 22/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6739 - accuracy: 0.6297 - val_loss: 0.7116 - val_accuracy: 0.5200
Epoch 23/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6617 - accuracy: 0.6690 - val_loss: 0.7125 - val_accuracy: 0.5200
Epoch 24/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6435 - accuracy: 0.7028 - val_loss: 0.7143 - val_accuracy: 0.5200
Epoch 25/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6786 - accuracy: 0.6061 - val_loss: 0.7144 - val_accuracy: 0.5200
Epoch 26/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6720 - accuracy: 0.6481 - val_loss: 0.7162 - val_accuracy: 0.5000
Epoch 27/50
10/10 [==============================] - 0s 5ms/step - loss: 0.6599 - accuracy: 0.6950 - val_loss: 0.7167 - val_accuracy: 0.5000
Epoch 28/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6623 - accuracy: 0.6825 - val_loss: 0.7178 - val_accuracy: 0.4900
Epoch 29/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6684 - accuracy: 0.5951 - val_loss: 0.7186 - val_accuracy: 0.4900
Epoch 30/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6583 - accuracy: 0.6817 - val_loss: 0.7202 - val_accuracy: 0.4900
Epoch 31/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6302 - accuracy: 0.7278 - val_loss: 0.7208 - val_accuracy: 0.5000
Epoch 32/50
10/10 [==============================] - 0s 5ms/step - loss: 0.6475 - accuracy: 0.6859 - val_loss: 0.7222 - val_accuracy: 0.4700
Epoch 33/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6624 - accuracy: 0.6414 - val_loss: 0.7225 - val_accuracy: 0.4700
Epoch 34/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6601 - accuracy: 0.6306 - val_loss: 0.7233 - val_accuracy: 0.4800
Epoch 35/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6463 - accuracy: 0.6397 - val_loss: 0.7240 - val_accuracy: 0.4700
Epoch 36/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6374 - accuracy: 0.6736 - val_loss: 0.7249 - val_accuracy: 0.4800
Epoch 37/50
10/10 [==============================] - 0s 5ms/step - loss: 0.6628 - accuracy: 0.6368 - val_loss: 0.7253 - val_accuracy: 0.4800
Epoch 38/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6616 - accuracy: 0.6376 - val_loss: 0.7265 - val_accuracy: 0.4700
Epoch 39/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6545 - accuracy: 0.6191 - val_loss: 0.7264 - val_accuracy: 0.4800
Epoch 40/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6353 - accuracy: 0.6840 - val_loss: 0.7278 - val_accuracy: 0.4700
Epoch 41/50
10/10 [==============================] - 0s 14ms/step - loss: 0.6664 - accuracy: 0.6262 - val_loss: 0.7285 - val_accuracy: 0.4500
Epoch 42/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6342 - accuracy: 0.6593 - val_loss: 0.7294 - val_accuracy: 0.4700
Epoch 43/50
10/10 [==============================] - 0s 5ms/step - loss: 0.6708 - accuracy: 0.6036 - val_loss: 0.7287 - val_accuracy: 0.4600
Epoch 44/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6566 - accuracy: 0.6025 - val_loss: 0.7300 - val_accuracy: 0.4600
Epoch 45/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6457 - accuracy: 0.6659 - val_loss: 0.7306 - val_accuracy: 0.4600
Epoch 46/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6344 - accuracy: 0.6353 - val_loss: 0.7313 - val_accuracy: 0.4500
Epoch 47/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6351 - accuracy: 0.7117 - val_loss: 0.7323 - val_accuracy: 0.4500
Epoch 48/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6444 - accuracy: 0.6601 - val_loss: 0.7326 - val_accuracy: 0.4500
Epoch 49/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6775 - accuracy: 0.5798 - val_loss: 0.7324 - val_accuracy: 0.4400
Epoch 50/50
10/10 [==============================] - 0s 6ms/step - loss: 0.6404 - accuracy: 0.6756 - val_loss: 0.7335 - val_accuracy: 0.4500
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (10, 16)                  48        
_________________________________________________________________
dense_1 (Dense)              (10, 16)                  272       
_________________________________________________________________
dense_2 (Dense)              (10, 1)                   17        
=================================================================
Total params: 337
Trainable params: 337
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1d571a238e524f533eb7c192731fccbfa2afb27278c837e5c0a29cbd7b18596d.png" src="../_images/1d571a238e524f533eb7c192731fccbfa2afb27278c837e5c0a29cbd7b18596d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot accuracy changes from model.fit()</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8b0cbc89f0b99b1f12ac0d1f3c040f6d228b4d7a6cdc4664ccfc4b0c94a4651f.png" src="../_images/8b0cbc89f0b99b1f12ac0d1f3c040f6d228b4d7a6cdc4664ccfc4b0c94a4651f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot loss changes from model.fit()</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0bf6f492bee3b6703e450ec998330ddec304ba60957f890a47c5d9ce3aaeb59a.png" src="../_images/0bf6f492bee3b6703e450ec998330ddec304ba60957f890a47c5d9ce3aaeb59a.png" />
</div>
</div>
</section>
<section id="evaluation-and-prediction">
<h3>Evaluation and Prediction<a class="headerlink" href="#evaluation-and-prediction" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Evaludate</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 2ms/step - loss: 0.7588 - accuracy: 0.4800
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.7587524652481079, 0.47999998927116394]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Predict</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.42253184],
       [0.43321115],
       [0.45118868],
       [0.3607703 ],
       [0.56583214]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="learning-and-training">
<h2><span class="section-number">1.5. </span>Learning and Training<a class="headerlink" href="#learning-and-training" title="Permalink to this headline">#</a></h2>
<section id="how-does-the-neural-network-learn-the-parameters">
<h3>How does the neural network learn the parameters?<a class="headerlink" href="#how-does-the-neural-network-learn-the-parameters" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Forward propagation shows how the network takes the input values, transforms them, and produces the predicted output values based on the network parameters (i.e., weights).</p></li>
<li><p>The network needs to learn the weights that best produce the output values according to some <strong>loss function</strong>, i.e., how different is the prediction from the true label?</p></li>
<li><p>Crucially, we need to compute the differences between the <strong>predicted</strong> outputs of the network and the <strong>true</strong> target outputs.</p></li>
<li><p>The model should aim to minimize these differences, which are commonly referred to as <strong>errors</strong> of the model.</p></li>
</ul>
</section>
<section id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>If the target ouputs are numeric values, we can evaluate the errors (i.e., the differences between the actual model outputs and the target outputs) using the <strong>mean square error</strong> function.</p>
<ul>
<li><p>Mean Square Error: <span class="math notranslate nohighlight">\(E = \frac{1}{2}\sum(y_k - t_k)^2\)</span></p></li>
</ul>
</li>
<li><p>If the target outputs are labels, we can evaluate the errors (i.e., the differences between the actual model labels and the target labels) using the <strong>cross entory error</strong> function.</p>
<ul>
<li><p>Cross Entropy Error: <span class="math notranslate nohighlight">\(E= -\sum_{k}t_k\log(y_k)\)</span></p></li>
</ul>
</li>
<li><p>The function used to compute the errors of the model is referred to as the <strong>loss function</strong>.</p></li>
</ul>
</section>
<section id="principles-for-loss-functions">
<h3>Principles for Loss Functions<a class="headerlink" href="#principles-for-loss-functions" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem Type</p></th>
<th class="head"><p>Last-Layer Activation</p></th>
<th class="head"><p>Loss Function in Keras</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">y</span></code> Encoding</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary Classification</p></td>
<td><p>sigmoid</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">binary_crossentropy()</span></code></p></td>
<td><p>Integer Encoding</p></td>
</tr>
<tr class="row-odd"><td><p>Binary/Multiclass Classification</p></td>
<td><p>softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy(from_logits=False)</span></code></p></td>
<td><p>Integer Encoding</p></td>
</tr>
<tr class="row-even"><td><p>Multiclass Classification</p></td>
<td><p>softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">categorical_crossentropy()</span></code></p></td>
<td><p>One-hot Encoding</p></td>
</tr>
<tr class="row-odd"><td><p>Regression</p></td>
<td><p>None</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mes</span></code></p></td>
<td><p>Floating-point</p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For multi-class classification, as long as the label is in <strong>integer</strong> encodings, then we use the <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code> for loss; if the label is in <strong>one-hot</strong> encodings, then we use the <code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code> for loss.</p>
</div>
</section>
</section>
<section id="categorical-variables-encoding">
<h2><span class="section-number">1.6. </span>Categorical Variables Encoding<a class="headerlink" href="#categorical-variables-encoding" title="Permalink to this headline">#</a></h2>
<section id="what-s-wrong-with-categorical-variables">
<h3>What’s wrong with categorical variables?<a class="headerlink" href="#what-s-wrong-with-categorical-variables" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Before we look at the examples of loss computation, we need to talk about the ways of encoding <strong>categorical</strong> labels.</p></li>
<li><p>Remember, machine doesn’t understand characters. If we have class labels like male/female, positive/negative, or each word tokens, we may need to convert these strings into machine-readable numerical values.</p></li>
<li><p>This step is called <strong>encoding</strong>.</p></li>
<li><p>Most importantly, machine learning and deep learning both require input and output variables to be <strong>numbers</strong>.</p></li>
</ul>
</section>
<section id="three-ways-of-encodings">
<h3>Three Ways of Encodings<a class="headerlink" href="#three-ways-of-encodings" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Integer</strong> (Sequence) Encoding: Each unique label is mapped to an integer</p></li>
<li><p><strong>One-hot</strong> Encoding: Each unique label is mapped to a binary vector</p></li>
<li><p><strong>Embeddings</strong> Encoding: Each unique label is mapped to a learned vectorized representation (i.e., embeddings)</p></li>
</ul>
</section>
<section id="steps">
<h3>Steps<a class="headerlink" href="#steps" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Given a corpus, we can create its <strong>vocabulary</strong> (at the word-token level, or sometimes at the character level).</p></li>
<li><p>We can associate every word type with a <strong>unique integer index</strong> (i.e., integer encoding).</p></li>
<li><p>We can further turn this integer index <span class="math notranslate nohighlight">\(i\)</span> into a <strong>binary vector</strong> of size <span class="math notranslate nohighlight">\(N\)</span> (the size of vocabulary). The vector is all zeros except for the <span class="math notranslate nohighlight">\(i\)</span>th entry, which is 1 (i.e., one-hot encoding).</p></li>
<li><p>We can turn the integer index into a <strong>dense, low-dimensional floating-point vectors</strong> (i.e., embedding encoding).</p>
<ul>
<li><p>Learn embeddings jointly with the main task (i.e., <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer)</p></li>
<li><p>Use pretrained word embeddings that were precomputed using a different machine learning task.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Given Label</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span>

<span class="c1">## Create Dictionary</span>
<span class="n">labels_dict</span> <span class="o">=</span> <span class="p">{</span> <span class="n">x</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_dict</span><span class="p">)</span>

<span class="c1">## Integer Encoding</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>

<span class="c1">## One-hot Encoding</span>
<span class="n">labels_oh</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_oh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;female&#39;: 0, &#39;male&#39;: 1}
[1, 0, 0, 1]
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###########################################</span>
<span class="c1">## Integer Encoding and One-hot Encoding ##</span>
<span class="c1">## Using sklearn                         ##</span>
<span class="c1">###########################################</span>

<span class="c1">## Integer Encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">oe</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int16&#39;</span><span class="p">)</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="n">oe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>

<span class="c1">## One-hot Encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">ohe</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">labels_ohe</span> <span class="o">=</span> <span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">labels_ohe</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1]
 [0]
 [0]
 [1]]
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="one-hot-encoding-and-loss-function">
<h3>One-hot Encoding and Loss Function<a class="headerlink" href="#one-hot-encoding-and-loss-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>With one-hot encoding, we can convert any categorical variable into a numeric vector.</p></li>
<li><p>And if we specify our network output to be a vector of the same size, we can compute the differences between the output vector and the true numeric vector.</p></li>
</ul>
<ul class="simple">
<li><p>When the target class label is a binary one (e.g., name-gender prediction):</p></li>
</ul>
<p><img alt="" src="../_images/neural-network-loss-binary.jpeg" /></p>
<ul class="simple">
<li><p>When the target class label is a multi-level one (e.g., next-word prediction):
<img alt="" src="../_images/neural-network-loss-multicat.jpeg" /></p></li>
</ul>
</section>
</section>
<section id="examples-of-loss-errors-skipped">
<h2><span class="section-number">1.7. </span>Examples of Loss Errors (skipped)<a class="headerlink" href="#examples-of-loss-errors-skipped" title="Permalink to this headline">#</a></h2>
<section id="error-based-on-one-sample">
<h3>Error based on One Sample<a class="headerlink" href="#error-based-on-one-sample" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The following is a simple example showing how to compute the loss for a case of prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_square_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c1"># avoid log(0)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## mean square error</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>  <span class="c1"># predicted values</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># true label</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mean_square_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.40625
2.302584092994546
</pre></div>
</div>
</div>
</div>
</section>
<section id="errors-based-on-batch-samples">
<h3>Errors based on Batch Samples<a class="headerlink" href="#errors-based-on-batch-samples" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The following is a simple example showing how to compute the average loss of a sample of batch size N cases.</p></li>
</ul>
<ul class="simple">
<li><p>If the training is based on a sample of batch size <em>N</em>, we can compute the average loss (or total errors) of the batch sample:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ E = - \frac{1}{N}\sum_n\sum_k t_{nk}\log y_{nk}\]</div>
<ul class="simple">
<li><p>We can revise the <code class="docutils literal notranslate"><span class="pre">cross_entropy_error()</span></code> function to work with outputs from a min-batch sample.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># adjust the function to for batch sample outputs</span>
<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When the labels uses one-hot encoding, the function can be simplified as follows:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># because for one-hot labels</span>
    <span class="c1"># cross-entropy sums only the values of the true labels `1`</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="gradient-descent">
<h2><span class="section-number">1.8. </span>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h2>
<section id="how-do-we-adjust-the-model-parameters">
<h3>How do we adjust the model parameters?<a class="headerlink" href="#how-do-we-adjust-the-model-parameters" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>With the <strong>Loss Function</strong>, we can now perform the most important step in model training — adjusting the <strong>weights</strong> (i.e., <strong>parameters</strong>) of the model.</p></li>
<li><p>The mechanism behind the neural network training steps is that we need to figure out:</p>
<ul>
<li><p>how the change of a specific <strong>parameter</strong> (i.e., weight) in the model may lead to the change (i.e., decrease) of the values from the <strong>Loss Function</strong>? (i.e., How much does a change in a specific weight affect the total error?)</p></li>
</ul>
</li>
<li><p>Then we would know how much of the total error each parameter in the model is responsible for.</p></li>
<li><p>These are the bases for parameter adjustments.</p></li>
<li><p>All we need to do is the adjust the weights <strong>in proportion to</strong> the changes that the parameter is responsible for.</p></li>
<li><p>This optimization method to finding a combination of weights that minimize the loss function is called <strong>Gradient Descent</strong>.</p></li>
</ul>
</section>
<section id="gradients">
<h3>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The error that a specific weight is responsible for is referred to as the <strong>gradient</strong> of the parameter.</p></li>
<li><p>Mathematically, the gradient of a weight is the <strong>partial derivative</strong> of a weight in relation to the <strong>loss function</strong>.</p></li>
<li><p>Then we adjust the weight in proportion to its gradient:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
{W}_{new} = W_{original} + \eta \times \textit{Gradient}_{W_{original}}
\]</div>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is a hyperparameter in deep learning. This parameter controls how fast the model learns. This <span class="math notranslate nohighlight">\(\eta\)</span> is referred to as the <strong>learning rates</strong>.</p></li>
</ul>
</section>
<section id="intuition-of-gradients-an-example-skipped">
<h3>Intuition of Gradients: An Example (skipped)<a class="headerlink" href="#intuition-of-gradients-an-example-skipped" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Let’s assume that we are training a model with two parameters, <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, and the loss function is <span class="math notranslate nohighlight">\(f(x_0, x_1)\)</span>.</p>
<ul>
<li><p>When a function includes more than one parameters, we can compute the partial derivative of the function with respect to each parameter.</p></li>
<li><p>When all partial derivatives are concatenated into a vector, this vector is called the <strong>gradient</strong>.</p></li>
<li><p>That is, for the above loss function with two parameters (e.g., <span class="math notranslate nohighlight">\(f(x_0, x_1) = \beta x_0 + \beta x_1\)</span>), we can calculate the partial derivatives of each parameter all at once,and represent them in a vector, which is referred to as <strong>gradient</strong>, i.e:
$<span class="math notranslate nohighlight">\(
(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1})
\)</span>$</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Each parameter estimation value pair, <span class="math notranslate nohighlight">\((x_0,x_1)\)</span>, should correspond to a gradient.</p></li>
<li><p>Intuitive understanding of the gradient:</p>
<ul>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> indicates how the changes of the parameter values <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> may contribute to the change of <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span>.</p></li>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is a <strong>vector</strong> with the direction pointing at the <strong>global minimum</strong> of the loss function.</p></li>
<li><p>The farther the <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is way from the global minimum, the larger the gradient vector.</p></li>
</ul>
</li>
<li><p>In Deep Learning, the training goes as follows:</p>
<ul>
<li><p>We initialize the parameters <span class="math notranslate nohighlight">\(x_0,x_1\)</span> with some values <span class="math notranslate nohighlight">\(p_0, p_1\)</span>;</p></li>
<li><p>We compute the loss function <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span></p></li>
<li><p>We compute the gradient of <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span> when the parameter <span class="math notranslate nohighlight">\(x_0 = p_0\)</span> and <span class="math notranslate nohighlight">\(x_1 = p_1\)</span></p></li>
<li><p>We use the gradient to determine how to update/modify all the model parameters, i.e.,</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
x_0 = x_0 + \eta\frac{\partial f}{\partial x_0} \\
x_1 = x_1 + \eta\frac{\partial f}{\partial x_1} 
\end{split}\]</div>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is again <strong>learning rate</strong>.</p></li>
</ul>
</section>
<section id="types-of-gradient-descent">
<h3>Types of Gradient Descent<a class="headerlink" href="#types-of-gradient-descent" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Batch</strong> Gradient Descent: Update the model weights after one epoch of the entire training set.</p></li>
<li><p><strong>Stochastic</strong> Gradient Descent (SGD): Update the model weights after every instance of the training set (online).</p></li>
<li><p><strong>Mini-batch</strong> Gradient Descent: Update the model weights after a subset of the training set. (Recommended!)</p></li>
</ul>
<p><img alt="" src="../_images/dl-gradient-stochastic.gif" /></p>
<p><img alt="" src="../_images/dl-gradient-mini.gif" /></p>
<p><img alt="" src="../_images/dl-gradient-batch.gif" /></p>
</section>
<section id="gradients-in-python-skipped">
<h3>Gradients in Python (skipped)<a class="headerlink" href="#gradients-in-python-skipped" title="Permalink to this headline">#</a></h3>
<p>In the following graph, each vector represents the gradient at a specific <span class="math notranslate nohighlight">\((x_0, x_1)\)</span>, i.e., when <span class="math notranslate nohighlight">\(x_0 = p_0\)</span> and <span class="math notranslate nohighlight">\(x_1 = p_1\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/59885582f09df353ce9df6967d22c1476b1df0eb6ba90a1afbc5aad2aa7bf90b.png" src="../_images/59885582f09df353ce9df6967d22c1476b1df0eb6ba90a1afbc5aad2aa7bf90b.png" />
</div>
</div>
</section>
</section>
<section id="overfitting-and-underfitting">
<h2><span class="section-number">1.9. </span>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this headline">#</a></h2>
<section id="optimization-vs-generalization">
<h3>Optimization vs. Generalization<a class="headerlink" href="#optimization-vs-generalization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>ML is always dealing with the tension between <strong>optimization</strong> and <strong>generalization</strong>.</p>
<ul>
<li><p>Optimization: the process of adjusting a model to get the best performance possible on the training data (i.e., to minimize the <em>bias</em> of the model)</p></li>
<li><p>Generalization: the performance of the model on data it has never seen before (i.e., to minimize the <strong>variation</strong> of the model performance on different datasets.)</p></li>
</ul>
</li>
</ul>
</section>
<section id="overfitting-underfitting">
<h3>Overfitting/Underfitting<a class="headerlink" href="#overfitting-underfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>During the training stage, if both the losses on training data and validation data are dropping, the model is <strong>underfit</strong> and in a way to be better fit. (Great bias, Small variation)</p></li>
<li><p>During the training, if the loss on the validation data stalls while that of the training data still drops, the model starts to <strong>overfit</strong>. (Small bias, Great variation)</p></li>
</ul>
</section>
<section id="how-to-deal-with-overfitting">
<h3>How to Deal With Overfitting?<a class="headerlink" href="#how-to-deal-with-overfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Get more training data</p></li>
<li><p>Reduce the capacity of the network (e.g., number of layers/nodes)</p></li>
<li><p>Add weight <strong>Regularization</strong></p></li>
<li><p>Add <strong>dropout</strong></p></li>
</ul>
</section>
<section id="regularization">
<h3>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>What is <strong>regularization</strong>?</p>
<ul>
<li><p>Modulate the quantity of information that the model is allowed to store</p></li>
<li><p>Add constraints on what information the model is allowed to store</p></li>
</ul>
</li>
<li><p>Weight regularization is to add to the loss function of the network a <strong>cost</strong> associated with having large weights.</p></li>
<li><p>That is, large weights will have greater penalties to the loss function, hence discouraged.</p></li>
<li><p>Common methods:</p>
<ul>
<li><p>L1 regularization: the cost added is proportional to the <strong>absolute values</strong> of the weights (<code class="docutils literal notranslate"><span class="pre">keras.regularizer.l1()</span></code>)</p></li>
<li><p>L2 regularization: the cost added is propositional to the <strong>square values</strong> of the weights (<code class="docutils literal notranslate"><span class="pre">keras/regularizer.l2()</span></code>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Dropout consists of randomly dropping out a number of output features of the applied layer during training.</p></li>
<li><p>The dropout rate is the <strong>fraction</strong> of the features that are <strong>zeroed out</strong>.</p></li>
<li><p>The dropout rate is usually set between 0.2 and 0.5.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">keras</span></code>, it can an independent layer, <code class="docutils literal notranslate"><span class="pre">keras.layers.dropout()</span></code>, or paremters to be set within specific layers (e.g., <code class="docutils literal notranslate"><span class="pre">keras.layers.LSTM()</span></code>).</p></li>
</ul>
</section>
<section id="heuristics-for-regularization">
<h3>Heuristics for Regularization<a class="headerlink" href="#heuristics-for-regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Applying dropout before a recurrent layer hinders learning rather than helping with regularization.</p></li>
<li><p>The same dropout mask (the same pattern of dropped units) should be applied at every time step, instead of a dropout mask that varies randomly from timestep to timestep. (Yarin Gal)</p></li>
<li><p>In order to regularize the representations formed by the recurrent gates of layers (e.g., LSTM and GRU), a temporally constant dropout mask should be applied to the inner activations of the layer (a recurrent dropout mask). (Yarin Gal)</p></li>
</ul>
</section>
<section id="normalization">
<h3>Normalization<a class="headerlink" href="#normalization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Normalization</strong> is to make sure that sets of numeric values are of a uniform scale (e.g., 0 to 1).</p></li>
<li><p>If the dataset contains numeric data varying in a huge range, it will skew the learning process, resulting in a bad model.</p></li>
<li><p>To properly add normalization layers can help speed up the model convergence for more effective training.</p></li>
</ul>
</section>
<section id="batch-normalization-and-layer-normalization">
<h3>Batch Normalization and Layer Normalization<a class="headerlink" href="#batch-normalization-and-layer-normalization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In Batch Normalization, input values of the same neuron for all the samples in the mini-batch are normalized.</p></li>
<li><p>In Layer Normalization, input values for all neurons in the same layer are normalized for each data sample.</p></li>
<li><p>Heuristics</p>
<ul>
<li><p>Batch Normalization depends on mini-batch size while Layer Normalization doesn’t.</p></li>
<li><p>Batch Normalization works better with CNN while Layer Normalization works better with RNN.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="some-math-skipped">
<h2><span class="section-number">1.10. </span>Some Math (Skipped)<a class="headerlink" href="#some-math-skipped" title="Permalink to this headline">#</a></h2>
<p>The following presents some important mathematical constructs related to the understanding of neural network.</p>
<section id="chain-rule-and-back-propagation">
<h3>Chain Rule and Back Propagation<a class="headerlink" href="#chain-rule-and-back-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Because there are many parameters in a network, we can compute the gradients (or partial derivatives) of all the weights using the chain rules of derivatives.</p></li>
<li><p>Specifically, the total error is essentially broken up and distributed back through the network to every single weight with the help of chain rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}.\frac{\partial y}{\partial x}\]</div>
<ul class="simple">
<li><p>This process is referred to as <strong>back propagation</strong>: moving back through the network, back-propagating the total errors to every single weight, and updating the weights.</p></li>
<li><p>The principle of weights-updating: the larger the gradient, the more the adjustments.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W_1 = W_1 - \eta \frac{\partial E}{\partial W_1}\]</div>
<ul class="simple">
<li><p>The above adjustment formula suggests that the weight updates are proportional to the partial derivatives of the weight.</p></li>
<li><p>The <strong><span class="math notranslate nohighlight">\(\eta\)</span></strong> in the formula controls the amount of adjustment, which is referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</section>
<section id="elementwise-operations-of-matrix">
<h3>Elementwise Operations of Matrix<a class="headerlink" href="#elementwise-operations-of-matrix" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A two-dimensional matrix</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} x = 
\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6 \\
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 2],
       [3, 4],
       [4, 6]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 3]
 [4 5]
 [5 7]]
[[ 5 10]
 [15 20]
 [20 30]]
[[0.2 0.4]
 [0.6 0.8]
 [0.8 1.2]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Matrix Elementwise Multiplication</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
5&amp;6 \\
7&amp;8
\end{pmatrix} =
\begin{pmatrix}
5&amp;12 \\
21&amp;32
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1 2]
 [3 4]]
[[5 6]
 [7 8]]
[[ 5 12]
 [21 32]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="broadcast">
<h3>Broadcast<a class="headerlink" href="#broadcast" title="Permalink to this headline">#</a></h3>
<p>In matrix elementwise computation, the smaller tensor will be <strong>broadcasted</strong> to match the shape of the larger tensor.</p>
<ul class="simple">
<li><p>Axes (called broadcast axes) are added to the smaller tensor to match the <code class="docutils literal notranslate"><span class="pre">ndim</span></code> of the larger tensor.</p></li>
<li><p>The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
10&amp;20
\end{pmatrix} =
\begin{pmatrix}
10&amp;40 \\
30&amp;80
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2, 2)
(2,)
[[10 40]
 [30 80]]
(2, 2)
</pre></div>
</div>
</div>
</div>
</section>
<section id="matrices-dot-production">
<h3>Matrices Dot Production<a class="headerlink" href="#matrices-dot-production" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/matrices-dot-product.png" />
(Source: Chollet [2018], Ch 2., Figure 2.5)</p>
<ul class="simple">
<li><p>The most common applications may be the <strong>dot product</strong> between two matrices.</p></li>
<li><p>You can take the dot product of two matrices x and y (<code class="docutils literal notranslate"><span class="pre">dot(x,</span> <span class="pre">y)</span></code>) if and only if <code class="docutils literal notranslate"><span class="pre">x.shape[1]</span> <span class="pre">==</span> <span class="pre">y.shape[0]</span></code>. The result is a matrix with shape (<code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code>, <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>), where the coefficients are the vector products between the rows of <span class="math notranslate nohighlight">\(x\)</span> and the columns of <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6
\end{pmatrix}
\begin{pmatrix}
5&amp;6&amp;7 \\
8&amp;9&amp;10
\end{pmatrix} =
\begin{pmatrix}
21&amp;24&amp;27 \\
47&amp;54&amp;62 \\
73&amp;84&amp;95
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>

<span class="n">xy_dot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy_dot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[21 24 27]
 [47 54 61]
 [73 84 95]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="matrices-dot-production-and-forward-propagation">
<h3>Matrices Dot Production and Forward Propagation<a class="headerlink" href="#matrices-dot-production-and-forward-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>For example, let’s assume that we have a word, one-hot encoded as [0,1,0,0,0,0].</p></li>
<li><p>An embedding model consists of parameters like the two-dimensional tensor shown below.</p></li>
<li><p>The output of the model is the dot product of the input word vector and the model parameter tensor.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
-2.8&amp;1.6&amp;0.9&amp;0.7&amp;-0.7&amp;-1.8 \\
0.3&amp;-2.3&amp;0.8&amp;1.8&amp;1.5&amp;0.7\\
0.9&amp;0.3&amp;-1.6&amp;-2.8&amp;0.5&amp;0.4\\
1.8&amp;-0.5&amp;-1.6&amp;-2.8&amp;-1.7&amp;1.7
\end{pmatrix}
\begin{pmatrix}
0\\
1\\
0\\
0\\
0\\
0\\
\end{pmatrix}=
\begin{pmatrix}
1.6 \\
-2.3 \\
0.3\\
-0.5
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">word_one_hot</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_one_hot</span><span class="p">)</span>
<span class="n">model_parameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0]
 [1]
 [0]
 [0]
 [0]
 [0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">,</span> <span class="n">word_one_hot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.6],
       [-2.3],
       [ 0.3],
       [-0.5]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="derivatives">
<h3>Derivatives<a class="headerlink" href="#derivatives" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Given a normal function, <span class="math notranslate nohighlight">\(f(x) = y \)</span> , if the <span class="math notranslate nohighlight">\(x\)</span> value changes, <span class="math notranslate nohighlight">\(y\)</span> will change as well.</p></li>
<li><p>So if we increase <span class="math notranslate nohighlight">\(x\)</span> by a small factor <span class="math notranslate nohighlight">\(h\)</span>, this results in a small change to <span class="math notranslate nohighlight">\(y\)</span>, i.e., <span class="math notranslate nohighlight">\(f(x+h) - f(x)\)</span>.</p></li>
<li><p>We can then compute the change of <span class="math notranslate nohighlight">\(y\)</span> relative to the small change of <span class="math notranslate nohighlight">\(x\)</span>, i.e., <span class="math notranslate nohighlight">\(\frac{f(x+h) - f(x)}{h}\)</span></p></li>
<li><p>When <span class="math notranslate nohighlight">\(h\)</span> is very very small around a certain point <span class="math notranslate nohighlight">\(p\)</span>, we can then estimate the change of <span class="math notranslate nohighlight">\(y\)</span> at the point when  <span class="math notranslate nohighlight">\(x = p\)</span>, i.e., <span class="math notranslate nohighlight">\(\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\)</span></p></li>
</ul>
<ul class="simple">
<li><p>This instantaneous change of <span class="math notranslate nohighlight">\(y\)</span> is called the <strong>derivetaive</strong> of <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\(p\)</span>.</p>
<ul>
<li><p>If it is negative, it means a small change of <span class="math notranslate nohighlight">\(x\)</span> around <span class="math notranslate nohighlight">\(p\)</span> will result in a decrease of <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
<li><p>If it is positive, a small change in <span class="math notranslate nohighlight">\(x\)</span> will result in an increase of <span class="math notranslate nohighlight">\(f(x)\)</span>.</p></li>
<li><p>The absolute value (i.e., the magnitude) of the derivative indicates how quickly this increase or decrease will happen.</p></li>
</ul>
</li>
<li><p>This can be mathematically represented as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x}= \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]</div>
<ul class="simple">
<li><p>The <strong>derivative</strong> turns out to be the <strong>slope of the tangent line</strong> at <span class="math notranslate nohighlight">\(x = p\)</span>.</p></li>
<li><p>If we are trying to update <span class="math notranslate nohighlight">\(x\)</span> by a factor <span class="math notranslate nohighlight">\(h\)</span> in order to minimize <span class="math notranslate nohighlight">\(f(x)\)</span>, and we know the derivative of <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>, we have everything we need:</p>
<ul>
<li><p>The derivative completely describes how <span class="math notranslate nohighlight">\(f(x)\)</span> evolves when we change <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>If we want to reduce the value of <span class="math notranslate nohighlight">\(f(x)\)</span>, we just need to move <span class="math notranslate nohighlight">\(x\)</span> a little in the opposite direction from the derivative.</p></li>
</ul>
</li>
<li><p>In Deep Learning, the <span class="math notranslate nohighlight">\(f(x)\)</span> is often the <strong>loss function</strong>, and <span class="math notranslate nohighlight">\(x\)</span> is often the parameter of the model.</p>
<ul>
<li><p>We initialize the parameter <span class="math notranslate nohighlight">\(x\)</span> with some value <span class="math notranslate nohighlight">\(p\)</span>;</p></li>
<li><p>We compute the loss function <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
<li><p>We compute the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> when the parameter <span class="math notranslate nohighlight">\(x = p\)</span></p></li>
<li><p>We use the derivative to determine how to update/modify the parameter, i.e., <span class="math notranslate nohighlight">\(x_{new} = x_{old} + \eta\frac{\partial f}{\partial x} \)</span></p></li>
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is commonly referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1">## change of y when a very small change in x</span>
    <span class="c1">#print(d)</span>
    <span class="c1"># d turns out to be the slope of the tangent line</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">d</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Take the derivates of the following function when x = 5 and 10:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[y = 4x^2 + 2x\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">fun_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">tf5</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y5</span> <span class="o">=</span> <span class="n">tf5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">tf10</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y10</span> <span class="o">=</span> <span class="n">tf10</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7f94e528b1b216518b8b72cd95fc358aa9df406f49b1a04ad8167dcfc8669c0e.png" src="../_images/7f94e528b1b216518b8b72cd95fc358aa9df406f49b1a04ad8167dcfc8669c0e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1d7c768fee2d2134bd8713705c1db0ae647c7afca53e674fb44b691df004f1cb.png" src="../_images/1d7c768fee2d2134bd8713705c1db0ae647c7afca53e674fb44b691df004f1cb.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1"># small change of x when x = 5 will slighly change y in positive direction</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1">## small change of x when x = 10 will greatly change y in positive direction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>41.99999999997317
81.99999999987995
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In python, we use the <strong>numerical differentiation</strong> method to find the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> when x = 5 and 10.</p></li>
<li><p>We can use the <strong>analytic differentiation</strong> method and derive the <strong>derivatie function</strong> <span class="math notranslate nohighlight">\(f'(x)\)</span> first:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f'(x) = \frac{\partial f}{\partial x}= 4x^2 + 2x = 8x + 2
\]</div>
<ul class="simple">
<li><p>Numerical differentiation produces derivatives with errors; analytic differentiation produces exact derivatives.</p></li>
</ul>
</section>
<section id="partial-derivatives">
<h3>Partial Derivatives<a class="headerlink" href="#partial-derivatives" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>When a function has many parameters, we can take the derivate of the function with respect to one particular parameter.</p></li>
<li><p>This parameter-specific derivative is called <strong>partial derivative</strong>.</p></li>
<li><p>Take the partial derivatives of the following function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x_0, x_1)=x_0^2 + x_1^2 \]</div>
<ul class="simple">
<li><p>Once if we have defined the function for the model (e.g., the Loss Function), we can calculate to what extent the change in weights would affect the change in the function.</p></li>
<li><p>The partial derivative refers to how a change in a specific weight <span class="math notranslate nohighlight">\(x_1\)</span> affects the function, i.e., the Loss Function or the total error.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial x_1}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## fun_2 has two variables/weights</span>
<span class="k">def</span> <span class="nf">fun_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_0\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_0}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp1</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">4.0</span><span class="o">**</span><span class="mi">2</span>


<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.00000000000378
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_1}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp2</span><span class="p">(</span><span class="n">x1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">3.0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span>


<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7.999999999999119
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2><span class="section-number">1.11. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>I highly recommend the following two books for deep learning with Python:</p>
<ul>
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010761759">Deep Learning：用Python進行深度學習的基礎理論實作</a></p></li>
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010817138?loc=P_br_r0vq68ygz_D_2aabd0_B_1">Deep Learning 2｜用Python進行自然語言處理的基礎理論實作</a></p></li>
</ul>
</li>
<li><p>This post collections a comprehensive list of learning resourcess for deep learning: <a class="reference external" href="https://buzzorange.com/techorange/2017/08/21/the-best-ai-lesson/">史上最完整機器學習自學攻略！我不相信有人看完這份不會把它加進我的最愛</a>.</p></li>
<li><p>Taylor, Michael. (2017). Neural Networks: A Visual Introduction for Beginners. (cf. Course Data)</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ml-emsemble-learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Emsemble Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="dl-simple-case.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Deep Learning: A Simple Example</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-of-neural-network">1.1. Workflow of Neural Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-overview">1.2. Neural Network Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation">Backward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons">Neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions-in-python">Activation Functions in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-nodes-to-layers">From Nodes to Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-parameters-and-matrix-mutiplication">Layer, Parameters, and Matrix Mutiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-neural-networks">Types of Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-neural-network-model-in-python">1.3. Building a Neural Network Model in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-in-focus">Packages in Focus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model">Create Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-modules-in-keras">Useful Modules in keras</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-utils"><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-layers"><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-modules">Other modules</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-example-of-model-training">1.4. A Quick Example of Model Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-and-prediction">Evaluation and Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-and-training">1.5. Learning and Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-neural-network-learn-the-parameters">How does the neural network learn the parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principles-for-loss-functions">Principles for Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-variables-encoding">1.6. Categorical Variables Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-wrong-with-categorical-variables">What’s wrong with categorical variables?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-ways-of-encodings">Three Ways of Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-and-loss-function">One-hot Encoding and Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-loss-errors-skipped">1.7. Examples of Loss Errors (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-based-on-one-sample">Error based on One Sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#errors-based-on-batch-samples">Errors based on Batch Samples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">1.8. Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-adjust-the-model-parameters">How do we adjust the model parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-gradients-an-example-skipped">Intuition of Gradients: An Example (skipped)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent">Types of Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-in-python-skipped">Gradients in Python (skipped)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting">1.9. Overfitting and Underfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-vs-generalization">Optimization vs. Generalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-underfitting">Overfitting/Underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-overfitting">How to Deal With Overfitting?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristics-for-regularization">Heuristics for Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization-and-layer-normalization">Batch Normalization and Layer Normalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-math-skipped">1.10. Some Math (Skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-and-back-propagation">Chain Rule and Back Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementwise-operations-of-matrix">Elementwise Operations of Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcast">Broadcast</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production">Matrices Dot Production</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production-and-forward-propagation">Matrices Dot Production and Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives">Derivatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives">Partial Derivatives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">1.11. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>