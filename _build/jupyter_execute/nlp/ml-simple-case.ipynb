{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning: A Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Quick Example: Name Gender Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's assume that we have collected a list of personal names and we have their corresponding gender labels, i.e., whether the name is a male or female one.\n",
    "\n",
    "The goal of this example is to create a classifier that would automatically classify a given name into either male or female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We use the data provided in NLTK. Please download the corpus data if necessary.\n",
    "- We load the corpus, `nltk.corpus.names` and randomize it before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now our unit for classification is a name. \n",
    "- In **feature engineering**, our goal is to transform the texts (i.e., names) into vectorized representations.\n",
    "- To start with, let's represent each text (name) by using its last character as the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'k'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_vectorizer(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "\n",
    "text_vectorizer('Shrek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We then apply the feature engineering method to every text in the data and split the data into **training** and **testing** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "featuresets = [(text_vectorizer(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A good start is to try the simple Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(text_vectorizer('Neo')))\n",
    "print(classifier.classify(text_vectorizer('Trinity')))\n",
    "print(classifier.classify(text_vectorizer('Alvin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Post-hoc Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- One of the most important steps after model training is to examine which features contribute the most to the classifier prediction of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     37.9 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     20.9 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.6 : 1.0\n",
      "             last_letter = 'v'              male : female =     16.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Please note that in `NLTK`, we can use the `apply_features` to create training and testing datasets.\n",
    "- When you have a very large feature set, this can be more effective in terms of memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is our earlier method of creating training and testing sets:\n",
    "\n",
    "```\n",
    "featuresets = [(text_vectorizer(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.classify import apply_features\n",
    "train_set = apply_features(text_vectorizer, labeled_names[500:])\n",
    "test_set = apply_features(text_vectorizer, labeled_names[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can we improve the model/classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following, we will talk about methods that we may consider to further improve the model training.\n",
    "\n",
    "- Feature Engineering\n",
    "- Error Analysis\n",
    "- Cross Validation\n",
    "- Try Different Machine-Learning Algorithms\n",
    "- (Ensemble Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Sophisticated Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can extract more features from the names.\n",
    "- Use the following features for vectorized representations of names:\n",
    "    - The first/last letter\n",
    "    - Frequencies of all 26 alphabets in the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letter': 'a',\n",
       " 'last_letter': 'n',\n",
       " 'count(a)': 1,\n",
       " 'has(a)': True,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 0,\n",
       " 'has(h)': False,\n",
       " 'count(i)': 1,\n",
       " 'has(i)': True,\n",
       " 'count(j)': 0,\n",
       " 'has(j)': False,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 1,\n",
       " 'has(l)': True,\n",
       " 'count(m)': 0,\n",
       " 'has(m)': False,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 0,\n",
       " 'has(o)': False,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 0,\n",
       " 'has(s)': False,\n",
       " 'count(t)': 0,\n",
       " 'has(t)': False,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 1,\n",
       " 'has(v)': True,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_vectorizer2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "\n",
    "text_vectorizer2('Alvin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letter': 'j',\n",
       " 'last_letter': 'n',\n",
       " 'count(a)': 0,\n",
       " 'has(a)': False,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 0,\n",
       " 'has(i)': False,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 0,\n",
       " 'has(m)': False,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 1,\n",
       " 'has(o)': True,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 0,\n",
       " 'has(s)': False,\n",
       " 'count(t)': 0,\n",
       " 'has(t)': False,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer2('John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776\n"
     ]
    }
   ],
   "source": [
    "train_set = apply_features(text_vectorizer2, labeled_names[500:])\n",
    "test_set = apply_features(text_vectorizer2, labeled_names[:500])\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     37.9 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     20.9 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.6 : 1.0\n",
      "             last_letter = 'v'              male : female =     16.4 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.8 : 1.0\n",
      "             last_letter = 'o'              male : female =      9.3 : 1.0\n",
      "                count(v) = 2              female : male   =      9.2 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.6 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.6 : 1.0\n",
      "             last_letter = 'g'              male : female =      6.3 : 1.0\n",
      "             last_letter = 'w'              male : female =      5.1 : 1.0\n",
      "            first_letter = 'w'              male : female =      4.9 : 1.0\n",
      "                count(w) = 1                male : female =      4.6 : 1.0\n",
      "                  has(w) = True             male : female =      4.6 : 1.0\n",
      "             last_letter = 't'              male : female =      4.1 : 1.0\n",
      "                count(a) = 3              female : male   =      4.0 : 1.0\n",
      "             last_letter = 's'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'z'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'i'            female : male   =      3.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-Development-Test Data Splits for Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Normally we have **training**-**testing** splits of data\n",
    "- Sometimes we can use **development (dev)** set for error analysis and feature engineering.\n",
    "- This dev set should be independent of training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now let's train the model on the **training set** and first check the classifier's performance on the **dev** set.\n",
    "- We then identify the errors the classifier made in the **dev** set.\n",
    "- We perform error analysis for further improvement.\n",
    "- We only test our **final model** on the testing set. (Note: Testing set can only be used **once**.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n"
     ]
    }
   ],
   "source": [
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]\n",
    "\n",
    "train_set = [(text_vectorizer2(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(text_vectorizer2(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(text_vectorizer2(n), gender) for (n, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(text_vectorizer2(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('error-analysis.csv', 'w') as f:\n",
    "\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['tag', 'guess', 'name'])\n",
    "    write.writerows(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ideally, we can inspect the errors in a spreadsheet and come up with better rules (features) that could help improve the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>guess</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Leon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Constantin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Noe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Prudence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Loren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Violet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Simone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Phillipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Justin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Fairfax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Corny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Philis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Ravi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Georgie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Suzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Marybeth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "      <td>Pris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Gabriele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>Chance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tag   guess        name\n",
       "0      male  female        Leon\n",
       "1      male  female  Constantin\n",
       "2    female    male         Gay\n",
       "3      male  female         Noe\n",
       "4    female    male    Prudence\n",
       "5    female    male       Loren\n",
       "6    female    male      Violet\n",
       "7      male  female      Simone\n",
       "8      male  female    Phillipe\n",
       "9    female    male      Justin\n",
       "226    male  female     Fairfax\n",
       "227  female    male       Corny\n",
       "228  female    male      Philis\n",
       "229    male  female        Ravi\n",
       "230  female    male     Georgie\n",
       "231  female    male       Suzie\n",
       "232  female    male    Marybeth\n",
       "233  female    male        Pris\n",
       "234    male  female    Gabriele\n",
       "235    male  female      Chance"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## check first and last N rows\n",
    "pd.read_csv('error-analysis.csv').iloc[[*range(10), *range(-10, 0)],]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/confusion-matrix.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Confusion Matrix:\n",
    "    - **True positives** are relevant items that we correctly identified as relevant.\n",
    "    - **True negatives** are irrelevant items that we correctly identified as irrelevant.\n",
    "    - **False positives** (or Type I errors) are irrelevant items that we incorrectly identified as relevant.\n",
    "    - **False negatives** (or Type II errors) are relevant items that we incorrectly identified as irrelevant.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given these four numbers, we can define the following model evaluation metrics:\n",
    "- **Accuracy**: How many items were correctly classified, i.e., $\\frac{TP + TN}{N}$\n",
    "- **Precision**: How many of the items identified by the classifier as relevant are indeed relevant, i.e., $\\frac{TP}{TP+FP}$.\n",
    "- **Recall**: How many of the true relevant items were successfully identified by the classifier, i.e., $\\frac{TP}{TP+FN}$.\n",
    "- **F-Measure (or F-Score)**: the harmonic mean of the precision and recall,i.e.:\n",
    "    \n",
    "\n",
    "$$ \n",
    "F= \\frac{(2 × Precision × Recall)}{(Precision + Recall)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{note}\n",
    "\n",
    "When dealing with imbalanced class distributions, we need to take into account the baseline performance in our model evaluation. For example. if the distribution of `Class 0` and `Class 1` is 9:1, then a naive classifier might as well classify all cases as `Class 0`, yielding a high-**precision** performance (i.e., Precision = 90%).\n",
    "\n",
    "Given this baseline, to better evaluate the classifier on imbalanced dataset, probably the classifier's **recall rates** are more important.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:4.2f}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Compute the Confusion Matrix\n",
    "t_f = [feature for (feature, label) in test_set]  # features of test set\n",
    "t_l = [label for (feature, label) in test_set]  # labels of test set\n",
    "t_l_pr = [classifier.classify(f) for f in t_f]  # predicted labels of test set\n",
    "cm = nltk.ConfusionMatrix(t_l, t_l_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |      f        |\n",
      "       |      e        |\n",
      "       |      m      m |\n",
      "       |      a      a |\n",
      "       |      l      l |\n",
      "       |      e      e |\n",
      "-------+---------------+\n",
      "female | <52.2%> 10.4% |\n",
      "  male |  12.6% <24.8%>|\n",
      "-------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(t_l, t_l_pr)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def createCM(classifier, test_set):\n",
    "    t_f = [feature for (feature, label) in test_set]\n",
    "    t_l = [label for (feature, label) in test_set]\n",
    "    t_l_pr = [classifier.classify(f) for f in t_f]\n",
    "    cm = nltk.ConfusionMatrix(t_l, t_l_pr)\n",
    "    print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |      f        |\n",
      "       |      e        |\n",
      "       |      m      m |\n",
      "       |      a      a |\n",
      "       |      l      l |\n",
      "       |      e      e |\n",
      "-------+---------------+\n",
      "female | <52.2%> 10.4% |\n",
      "  male |  12.6% <24.8%>|\n",
      "-------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createCM(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can also check our average model performance using the cross-validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "![](../images/ml-kfold.png)\n",
    "(Source: https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n",
      "accuracy: 0.79\n",
      "accuracy: 0.78\n",
      "accuracy: 0.79\n",
      "accuracy: 0.78\n",
      "accuracy: 0.75\n",
      "accuracy: 0.77\n",
      "accuracy: 0.8\n",
      "accuracy: 0.78\n",
      "accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "acc_kf = []  ## accuracy holder\n",
    "\n",
    "## Cross-validation\n",
    "for train_index, test_index in kf.split(train_set):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(\n",
    "        train_set[train_index[0]:train_index[len(train_index) - 1]])\n",
    "    cur_fold_acc = nltk.classify.util.accuracy(\n",
    "        classifier, train_set[test_index[0]:test_index[len(test_index) - 1]])\n",
    "    acc_kf.append(cur_fold_acc)\n",
    "    print('accuracy:', np.round(cur_fold_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7813138143214553"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Try Different Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are many ML algorithms for classification tasks.\n",
    "- Here we will demonstrate a few more classifiers implemented in NLTK, including:\n",
    "    - Maximum Entropy Classifier (Logistic Regression)\n",
    "    - Decision Tree Classifier\n",
    "- Also, in NLTK, we can use the classification methods provided in `sklearn` as well, including:\n",
    "    - Naive Bayes\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When we try another ML algorithm, we do the following:\n",
    "    - train the model\n",
    "    - check model performance (accuracy and confusion matrix)\n",
    "    - check the most informative features\n",
    "    - obtain average performance using *k*-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Maxent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Maxent is memory hungry, slower, and it requires `numpy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 114 ms, total: 2min 9s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.classify import MaxentClassifier\n",
    "classifier_maxent = MaxentClassifier.train(train_set,\n",
    "                                           algorithm='iis',\n",
    "                                           trace=0,\n",
    "                                           max_iter=10000,\n",
    "                                           min_lldelta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{note}\n",
    "The default algorithm for training is `iis` (Improved Iterative Scaling). Another alternative is `gis` (General Iterative Scaling), which is faster.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_maxent, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -3.280 last_letter=='a' and label is 'male'\n",
      "  -2.516 last_letter=='k' and label is 'female'\n",
      "  -2.262 last_letter=='v' and label is 'female'\n",
      "  -2.254 last_letter=='p' and label is 'female'\n",
      "  -2.021 count(v)==2 and label is 'male'\n",
      "  -1.696 last_letter=='f' and label is 'female'\n",
      "  -1.399 last_letter=='m' and label is 'female'\n",
      "  -1.347 last_letter=='d' and label is 'female'\n",
      "  -1.276 last_letter=='o' and label is 'female'\n",
      "  -1.178 last_letter=='i' and label is 'male'\n",
      "   1.059 last_letter=='c' and label is 'male'\n",
      "  -1.011 last_letter=='g' and label is 'female'\n",
      "  -0.923 last_letter=='r' and label is 'female'\n",
      "   0.767 count(g)==3 and label is 'male'\n",
      "  -0.755 count(e)==3 and label is 'male'\n",
      "  -0.744 count(i)==3 and label is 'female'\n",
      "  -0.739 first_letter=='q' and label is 'female'\n",
      "  -0.663 last_letter=='s' and label is 'female'\n",
      "   0.662 last_letter==' ' and label is 'female'\n",
      "  -0.655 last_letter=='t' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "classifier_maxent.show_most_informative_features(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |      f        |\n",
      "       |      e        |\n",
      "       |      m      m |\n",
      "       |      a      a |\n",
      "       |      l      l |\n",
      "       |      e      e |\n",
      "-------+---------------+\n",
      "female | <56.2%>  6.4% |\n",
      "  male |  14.6% <22.8%>|\n",
      "-------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createCM(classifier_maxent, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7313664596273292\n",
      "accuracy: 0.7080745341614907\n",
      "accuracy: 0.6645962732919255\n",
      "accuracy: 0.6925465838509317\n",
      "accuracy: 0.6749611197511665\n",
      "accuracy: 0.6858475894245724\n",
      "accuracy: 0.7200622083981337\n",
      "accuracy: 0.7262830482115086\n",
      "accuracy: 0.687402799377916\n",
      "accuracy: 0.702954898911353\n",
      "CPU times: user 2min 19s, sys: 50.5 ms, total: 2min 19s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for train_index, test_index in kf.split(train_set):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    classifier = MaxentClassifier.train(\n",
    "        train_set[train_index[0]:train_index[len(train_index) - 1]],\n",
    "        algorithm='gis',\n",
    "        trace=0,\n",
    "        max_iter=100,\n",
    "        min_lldelta=0.01) ## set smaller value for `min_lldelta`\n",
    "    print(\n",
    "        'accuracy:',\n",
    "        nltk.classify.util.accuracy(\n",
    "            classifier,\n",
    "            train_set[test_index[0]:test_index[len(test_index) - 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Parameters:\n",
    "    - `binary`: whether the features are binary\n",
    "    - `entropy_cutoff`: a value used during tree refinement process\n",
    "        - entropy = 1 -> high-level uncertainty\n",
    "        - entropy = 0 -> perfect model prediction\n",
    "    - `depth_cutoff`: to control the depth of the tree\n",
    "    - `support_cutoff`: the minimum number of instances that are required to make a decision about a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.08 s, sys: 4.53 ms, total: 9.08 s\n",
      "Wall time: 9.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "classifier_dt = DecisionTreeClassifier.train(train_set,\n",
    "                                             binary=True,\n",
    "                                             entropy_cutoff=0.7,\n",
    "                                             depth_cutoff=5,\n",
    "                                             support_cutoff=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_dt, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |      f        |\n",
      "       |      e        |\n",
      "       |      m      m |\n",
      "       |      a      a |\n",
      "       |      l      l |\n",
      "       |      e      e |\n",
      "-------+---------------+\n",
      "female | <59.4%>  3.2% |\n",
      "  male |  25.8% <11.6%>|\n",
      "-------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createCM(classifier_dt, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7329192546583851\n",
      "accuracy: 0.7267080745341615\n",
      "accuracy: 0.7142857142857143\n",
      "accuracy: 0.7391304347826086\n",
      "accuracy: 0.71850699844479\n",
      "accuracy: 0.7169517884914464\n",
      "accuracy: 0.7107309486780715\n",
      "accuracy: 0.7231726283048211\n",
      "accuracy: 0.7309486780715396\n",
      "accuracy: 0.7091757387247278\n",
      "CPU times: user 1min 27s, sys: 32.2 ms, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for train_index, test_index in kf.split(train_set):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    classifier = DecisionTreeClassifier.train(\n",
    "        train_set[train_index[0]:train_index[len(train_index) - 1]],\n",
    "        binary=True,\n",
    "        entropy_cutoff=0.7,\n",
    "        depth_cutoff=5,\n",
    "        support_cutoff=5)\n",
    "    print(\n",
    "        'accuracy:',\n",
    "        nltk.classify.util.accuracy(\n",
    "            classifier,\n",
    "            train_set[test_index[0]:test_index[len(test_index) - 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try `sklearn` Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `sklearn` is a very useful module for machine learning. We will talk more about this module in our later lectures.\n",
    "- This package provides a lot more ML algorithms for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Naive Bayes in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "sk_classifier = SklearnClassifier(MultinomialNB())\n",
    "sk_classifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(sk_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Logistic Regression in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sk_classifier = SklearnClassifier(LogisticRegression(max_iter=500))\n",
    "sk_classifier.train(train_set)\n",
    "nltk.classify.accuracy(sk_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Support Vector Machine in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `sklearn` provides several implementations for Support Vector Machines.\n",
    "- Please see its documentation for more detail: [Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "sk_classifier = SklearnClassifier(SVC())\n",
    "sk_classifier.train(train_set)\n",
    "nltk.classify.accuracy(sk_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "sk_classifier = SklearnClassifier(LinearSVC(max_iter=2000))\n",
    "sk_classifier.train(train_set)\n",
    "nltk.classify.accuracy(sk_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "sk_classifier = SklearnClassifier(NuSVC())\n",
    "sk_classifier.train(train_set)\n",
    "nltk.classify.accuracy(sk_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remaining Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Feature engineering is crucial to the process of machine learning.\n",
    "- The quality of the text vectorization almost determines the classifier's performance to a great deal.\n",
    "- Every ML algorithm requires a lot of **hyperparameter** settings, which can have substantial impact on the model performances.\n",
    "- We need a more **systematic** way to find the optimal combinations of hyperparameters for a given ML algorithm.\n",
    "- We will come back to issue when we talk about doing ML with `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- NLTK Book, [Chapter 6 Learning to Classify Texts](https://www.nltk.org/book/ch06.html)\n",
    "- Géron (2019), Chapter 3 Classification"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": "2",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "706px",
    "left": "1519.67px",
    "top": "85px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}