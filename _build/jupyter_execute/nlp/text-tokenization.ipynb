{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e86-CfdAy-K"
   },
   "source": [
    "# Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUR8JgOMAy-N"
   },
   "source": [
    "The objective of text tokenization is to break the text into smaller units which are often more linguistically meaningful.\n",
    "\n",
    "These smaller linguistic units are usually easier to deal with computationally and semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1709151256810,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "nJN7TmF_Ay-O"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1709151257287,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "O1IuxowGAy-P",
    "outputId": "edfcc0f5-0203-47b7-e99a-c03ccdf4858b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Colab Only\n",
    "## nltk download\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPW7Aqe9Ay-Q"
   },
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1709151257288,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "y4qzDDOXAy-Q",
    "outputId": "398f6799-ddad-459c-84c5-25d28b4681e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear!\n",
      "\n",
      "Oh dear!\n",
      "\n",
      "I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "para = '''There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.'''\n",
    "\n",
    "for s in sent_tokenize(para):\n",
    "    print(s+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSElcePIAy-Q"
   },
   "source": [
    "- The `sent_tokenize()` function uses an instance of `PunktSentenceTokenizer` from the `ntlk.tokenize.punkt` module.\n",
    "\n",
    "- To process large amount of data, it is recommended to load the pre-trained `PunktSentenceTokenizer` once, and call its `tokenizer()` method for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709151257288,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "HhPxNBHHAy-R"
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1709151257288,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "xYiW2WySAy-R",
    "outputId": "cd4646c1-44cc-4695-cfcb-036e7b420af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear!',\n",
       " 'Oh dear!',\n",
       " 'I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "un0MwEcDAy-R"
   },
   "source": [
    "The `nltk` also provides many pre-trained `PunktSentenceTokenizer` for other European languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "YdScpBymAy-S",
    "outputId": "7c10a5f4-8965-4b49-aefe-716c25dce975",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/Users/Alvinchen/nltk_data/tokenizers/punkt/PY3': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/Alvinchen/nltk_data/tokenizers/punkt/PY3\n",
    "# !dir C:/Users/alvinchen/AppData/Roaming/nltk_data/tokenziers/punkt/PY3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lbb1dMCAy-S"
   },
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55nMfPn3Ay-S"
   },
   "source": [
    "Similarly, the `word_tokenize()` function is a wrapper function that calls the `tokenize()` method on a instance of `TreebankWordTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "aaDDImzBAy-S",
    "outputId": "9fe556df-74ef-4f0b-d4e4-bb4cc120133d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(para)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoA4BOpjAy-S"
   },
   "source": [
    "- To process large amount of data, please create an instance of `TreebankWordTokenizer` and call its `tokenize()` method for more efficient processing.\n",
    "\n",
    "- We will get the same results with the following codes as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "d5SOGVMGAy-S",
    "outputId": "76a9e193-5f46-4ec5-f8f7-0077aec24e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(para)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq9SQ3W0Ay-S"
   },
   "source": [
    "The `nltk` module has implemented other more task-oriented word tokenizers, which differ in terms of their specific handling of the punctuations and contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP_vhbcmAy-S"
   },
   "source": [
    "![](../images/nltk-tokenizer-class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAauwK1gAy-S"
   },
   "source": [
    "### Comparing different word tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaETVieCAy-S"
   },
   "source": [
    "- `TreebankWordTokenizer` follows the Penn Treebank conventions for word tokenization.\n",
    "- `WordPunctTokenizer` splits all punctuations into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "z9cHq9v7Ay-S"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wpt = WordPunctTokenizer()\n",
    "tbwt = TreebankWordTokenizer()\n",
    "\n",
    "sent = \"Isn't this great? I can't tell!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "mAYa7XCuAy-T",
    "outputId": "b90bafca-6ddc-4427-db43-a1dc6c7ec3e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Isn', \"'\", 't', 'this', 'great', '?', 'I', 'can', \"'\", 't', 'tell', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "aQF7s_GsAy-T",
    "outputId": "25b93554-1a6f-46c6-da9e-83199a3d26fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', \"n't\", 'this', 'great', '?', 'I', 'ca', \"n't\", 'tell', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbwt.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dopXTAXaAy-T"
   },
   "source": [
    "## Tokenization using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkJQ5aLDAy-T"
   },
   "source": [
    "The `nltk` also provides another flexible way for text tokenization based on regular expression.\n",
    "\n",
    "The `RegexTokenizer` class allows for text tokenization based on the self-defined regular expression patterns.\n",
    "\n",
    "The regular expression can be created/defined for either the token or the delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "L41c2BIHAy-T"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1709151257669,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "ljekhc77Ay-T"
   },
   "outputs": [],
   "source": [
    "\n",
    "retok1 = RegexpTokenizer(pattern= \"[a-zA-Z_'-]+\")\n",
    "retok2 = RegexpTokenizer(pattern= \"[a-zA-Z_-]+\")\n",
    "retok3 = RegexpTokenizer(pattern= \"\\s+\", gaps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709151257670,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "tu7UFV6wAy-T",
    "outputId": "e8d7d7c0-684f-4c70-f8a3-07954b612cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Isn't\", 'this', 'great', 'I', \"can't\", 'tell']\n"
     ]
    }
   ],
   "source": [
    "print(retok1.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqZyv0bTAy-T"
   },
   "source": [
    "See how this tokenizer deals with the apostrophe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709151257670,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "GPQJre0rAy-T",
    "outputId": "100df02e-5bbe-42b0-e740-f181c5454a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Isn', 't', 'this', 'great', 'I', 'can', 't', 'tell']\n"
     ]
    }
   ],
   "source": [
    "print(retok2.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1709151257670,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "uoUsIDMOAy-T",
    "outputId": "efc24850-2342-4923-c87b-f8dd2de16865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Isn't\", 'this', 'great?', 'I', \"can't\", 'tell!']\n"
     ]
    }
   ],
   "source": [
    "print(retok3.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsfk1kCuAy-T"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0V3qc9bAy-U"
   },
   "source": [
    "- NLTK Book, Ch 1-3"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}