{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The objective of text enrichment is to utilize computational techniques of automatic annotations and extract additional linguistic information from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parts-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Every POS tagger needs to first operationlize a tagset, i.e., a complete list of possible tags for the entire corpus.\n",
    "- A common tagset for English is [Penn-treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- NLTK default POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"This is a sentence.\"\n",
    "text_word = nltk.word_tokenize(text)\n",
    "text_pos = nltk.pos_tag(text_word)\n",
    "print(text_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Using NLTK to Train Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3914\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "print(len(treebank.tagged_sents())) # total number of sents\n",
    "\n",
    "# train-test for training and testing taggers\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "train_sents = treebank.tagged_sents()[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('early', 'RB'),\n",
       " ('trading', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Tokyo', 'NNP'),\n",
       " ('Thursday', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Nikkei', 'NNP'),\n",
       " ('index', 'NN'),\n",
       " ('fell', 'VBD'),\n",
       " ('63.79', 'CD'),\n",
       " ('points', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('35500.64', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- NgramTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/nltk-fig-5-1-tag-context.png) <small>(From NLTK Book Ch 5. Figure 5-1)</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "unigram_tagger = UnigramTagger(train_sents)\n",
    "bigram_tagger = BigramTagger(train_sents)\n",
    "trigram_tagger = TrigramTagger(train_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/47nxlfjn26536t51wxj9j6q40000gn/T/ipykernel_30383/3641959337.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  unigram_tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8571551910209367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/47nxlfjn26536t51wxj9j6q40000gn/T/ipykernel_30383/705572540.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  bigram_tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11318799913662854"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/47nxlfjn26536t51wxj9j6q40000gn/T/ipykernel_30383/2977493380.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  trigram_tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06902654867256637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- NLTK Taggers (`nltk.tag`):\n",
    "    - `DefaultTagger`\n",
    "    - `UnigramTagger`, `BigramTagger`, `TrigramTagger`\n",
    "    - `RegexpTagger`\n",
    "    - `AffixTagger`\n",
    "    - `ClassifierBasedPOSTagger`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Backoff Tagging\n",
    "    - The idea of **backoff** is that for longer sequences, we are more likely to encounter *unseen* n-grams in the test data.\n",
    "    - To avoid the zero probability issue due to the unseen **n**-grams, we can backoff the probability estimates using the lower-order (**n-1**)-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "unigram_tagger = UnigramTagger(train_sents)\n",
    "bigram_tagger = BigramTagger(train_sents,backoff=unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(train_sents, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/47nxlfjn26536t51wxj9j6q40000gn/T/ipykernel_30383/2977493380.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  trigram_tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8647096913447011"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{tip}\n",
    "Sometimes it may take a while to train a tagger. We can pickle a trained tagger for later usage.\n",
    "\n",
    "```\n",
    "import pickle\n",
    "f = open('trigram-backoff-tagger.pickle', 'wb')\n",
    "f.close()\n",
    "f = open('trigram-backoff-tagger.pickle', 'rb')\n",
    "tagger = pickle.load(f)\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Classifier-based Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/47nxlfjn26536t51wxj9j6q40000gn/T/ipykernel_30383/2907403403.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cbtagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9309734513274336"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "cbtagger = ClassifierBasedPOSTagger(train=train_sents)\n",
    "cbtagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "By default, the `ClassifierBasedPOSTagger` uses a `NaiveBayesClassifier` for training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To try other classifers, e.g., Maximum Entropy Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "# warning is not logged here. Perfect for clean unit test output\n",
    "with numpy.errstate(divide='ignore'):\n",
    "    numpy.float64(1.0) / 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.008\n",
      "             2          -0.76859        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alvinchen/opt/anaconda3/envs/nlp1/lib/python3.10/site-packages/nltk/classify/maxent.py:1381: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2**nf_delta\n",
      "/Users/alvinchen/opt/anaconda3/envs/nlp1/lib/python3.10/site-packages/nltk/classify/maxent.py:1383: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/Users/alvinchen/opt/anaconda3/envs/nlp1/lib/python3.10/site-packages/nltk/classify/maxent.py:1384: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "metagger = ClassifierBasedPOSTagger(train=train_sents,\n",
    "                                   classifier_builder=MaxentClassifier.train)\n",
    "metagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Classifier-based with Cut-off Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "default = DefaultTagger('NN')\n",
    "cbtagger2 = ClassifierBasedPOSTagger(train=train_sents,\n",
    "                                    backoff=default,\n",
    "                                    cutoff_prob=0.3)\n",
    "\n",
    "cbtagger2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Another useful module for English POS tagging is to use `spacy`. We will come back to this module when we talk about parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Chunk** extraction is the processing of extracting short phrases from a part-of-speech tagged sentence.\n",
    "- This is different from parsing in that we are only interested in standalone chunks or phrases, instead of the full parsed syntactic tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In NLTK, A `ChunkRule` class specifies what to *include* in a chunk, while a `ChinkRule` class specifies what to *exclude* from a chunk.\n",
    "- **Chunking** creates chunks and **chinking** breaks up those chunks.\n",
    "- Both rules utilize **regular expressions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For example, we can extract NP chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150 # default for me was 75\n",
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "np_chunker = RegexpParser(r'''\n",
    "NP:\n",
    "{<DT><NN.*><.*>*<NN.*>} # chunk\n",
    "}<VB.*>{ # chink\n",
    "''')\n",
    "\n",
    "s = \"This course has many interesting topics\"\n",
    "np_chunker.parse(nltk.pos_tag(nltk.word_tokenize(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define a function to extract chunks from the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sub_leaves(tree, label):\n",
    "    return [t.leaves() for t in \n",
    "            tree.subtrees(lambda s:s.label()==label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s_chunk_tree = np_chunker.parse(\n",
    "    nltk.pos_tag(\n",
    "        nltk.word_tokenize(s)))\n",
    "\n",
    "sub_leaves(s_chunk_tree, \"NP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Named Entity Chunker (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "ne_chunks=ne_chunk(treebank.tagged_sents()[0])\n",
    "ne_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sub_leaves(ne_chunks, \"PERSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sub_leaves(ne_chunks, \"ORGANIZATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Parsing, we will use `spacy`, a powerful package for natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# load language model\n",
    "nlp_en = spacy.load('en_core_web_sm') \n",
    "# parse text \n",
    "doc = nlp_en('This is a sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print((token.text, \n",
    "            token.lemma_, \n",
    "            token.pos_, \n",
    "            token.tag_,\n",
    "            token.dep_,\n",
    "            token.shape_,\n",
    "            token.is_alpha,\n",
    "            token.is_stop,\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Check meaning of a POS tag\n",
    "spacy.explain('VBZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To get the dependency relations, we first need to extract NP chunks, on which dependency relations are annotated.\n",
    "- Please refer to [spacy documentation](https://spacy.io/usage/linguistic-features#dependency-parse) for more detail on dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc2 = nlp_en(' '.join(treebank.sents()[0]))\n",
    "\n",
    "for c in doc2.noun_chunks:\n",
    "    print((c.text, \n",
    "           c.root.text, \n",
    "           c.root.dep_, \n",
    "           c.root.head.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each NP chunk includes several important pieces of information:\n",
    "- **Text**: The original noun chunk text.\n",
    "- **Root text**: The original text of the word connecting the noun chunk to the rest of the parse.\n",
    "- **Root dep**: Dependency relation connecting the root to its head.\n",
    "- **Root head text**: The text of the root token’s head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc2, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Named Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc2.ents:\n",
    "    print((ent.text,\n",
    "           ent.start_char,\n",
    "           ent.end_char,\n",
    "           ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Please check the documentation of [Universal Dependency Types](https://universaldependencies.org/docs/u/dep/index.html) proposed by [Marneffe et al.](https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [NLTK Ch.5 Categorizing and Tagging Words](http://www.nltk.org/book/ch05.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "nlp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}