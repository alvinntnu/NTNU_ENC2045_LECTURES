{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PeFP5M2orsA"
   },
   "source": [
    "\n",
    " # NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LMwUQZYorsC"
   },
   "source": [
    "## A General NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XX204gyorsC"
   },
   "source": [
    "![nlp-pipeline](../images/nlp-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gis1o_fcorsD"
   },
   "source": [
    "### Varations of the NLP Pipelines\n",
    "\n",
    "- The process may not always be linear.\n",
    "- There are loops in between.\n",
    "- These procedures may depend on specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{contents}\n",
    ":depth: 3\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8GzYzzorsD"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP1cSXqZorsD"
   },
   "source": [
    "### Data Acquisition: Heart of ML System\n",
    "\n",
    "- Ideal Setting: We have everything needed.\n",
    "- Labels and Annotations\n",
    "- Very often we are dealing with less-than-ideal scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nwF5xB0orsD"
   },
   "source": [
    "### Less-than-ideal Scenarios\n",
    "\n",
    "- Initial datasets with limited annotations/labels\n",
    "- Initial datasets labeled/extracted based on regular expressions or heuristics\n",
    "- Public datasets (cf. [Google Dataset Search](https://datasetsearch.research.google.com/) or [kaggle](https://www.kaggle.com/))\n",
    "- Scraped data\n",
    "- Product intervention (Product intervention refers to the deliberate manipulation or curation of data by product developers or platform owners, which may introduce biases or distortions in the dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WclhzugcorsD"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "- Data augmentation in natural language processing (NLP) is a technique used to increase the diversity and quantity of training data by leveraging language properties to create new text samples that are syntactically similar to the original source text data.\n",
    "- Types of strategies:\n",
    "    - Synonym replacement\n",
    "    - Related word replacement (based on association metrics)\n",
    "    - Back translation\n",
    "    - Replacing entities\n",
    "    - Adding noise to data (e.g. spelling errors, random words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22trbXiJorsE"
   },
   "source": [
    "## Text Extraction and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgsIMKS5orsE"
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "- Extracting raw texts from the input data\n",
    "    - HTML\n",
    "    - PDF\n",
    "- Relevant vs. irrelevant information\n",
    "    - non-textual information\n",
    "    - markup\n",
    "    - metadata\n",
    "- Encoding format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUnHJJVuorsE"
   },
   "source": [
    "#### Extracting texts from webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxWcWM6jorsE"
   },
   "source": [
    "Extracting textual contents from the website is a very common way to obtain data. It requires a close study of the structure of the HTML content of the web pages.\n",
    "\n",
    "The following code attempts to extract hyperlinks from the index page of PTT, a renowned public bulletin board in Taiwan, and automatically navigates to the first hyperlink to access its textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25718,
     "status": "ok",
     "timestamp": 1708557401377,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "PU3gQKhPwdZq",
    "outputId": "33918d1c-8d74-4b21-c82e-0065797a3685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_html in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.31.0)\n",
      "Requirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from requests_html) (1.4.0)\n",
      "Requirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from requests_html) (1.20.1)\n",
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from requests_html) (0.0.2)\n",
      "Requirement already satisfied: w3lib in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.1.2)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (2024.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (7.0.1)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (11.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.66.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.26.18)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests_html) (4.12.3)\n",
      "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (4.9.4)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests_html) (4.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests_html) (2.5)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n"
     ]
    }
   ],
   "source": [
    "## -------------------- ##\n",
    "## Colab Only           ##\n",
    "## -------------------- ##\n",
    "\n",
    "! pip install requests_html\n",
    "! apt install tesseract-ocr\n",
    "! apt install libtesseract-dev\n",
    "! pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1708557684454,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "8A6ZM1oK7GQU",
    "outputId": "21131edc-16bc-4f7f-946d-7b42478dd2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/bbs/Gossiping/M.1708555641.A.B86.html', '/bbs/Gossiping/M.1708555709.A.95A.html', '/bbs/Gossiping/M.1708555913.A.A76.html', '/bbs/Gossiping/M.1708556060.A.DD7.html', '/bbs/Gossiping/M.1708556086.A.06B.html']\n",
      "https://www.ptt.cc//bbs/Gossiping/M.1708555641.A.B86.html\n",
      "作者：A98454 ( 阿肥是肥宅的肥)\n",
      "看板：Gossiping\n",
      "標題：Re: [問卦] 科舉制度是控制人民的手段嗎？\n",
      "日期：Thu Feb 22 03:47:22 2024\n",
      "內容： 米那桑 空尼幾蛙 關於J個問題  阿肥覺得是這樣的\n",
      "\n",
      " 說到控制人民就要說說 法家思想了，荀子認為任性本惡，因此人需要學習去壓制內心慾望\n",
      "的邪惡，遵行禮法\n",
      "\n",
      " 法家延續了這個思想，認為學習的成效還是不夠，人就是犯賤必須要有法來約束他們，才\n",
      "會乖乖\n",
      "\n",
      " 因此商鞅變法並提出農戰的理論，農戰理論就是說呢，國家糧食充足就能變強，所以人民\n",
      "就別想有的沒的好好跟著國家給的kpi ,乖乖種田就好了\n",
      "\n",
      " 至於那些有其他想法的，思想hen 危險的，就發配充軍思想改造，避免這些人影響農民種\n",
      "田，所以人民不要想有的沒乖乖種田就好了\n",
      "\n",
      " 然後糧食充足了，飽暖思淫慾，就有更多農奴了\n",
      "\n",
      " 如果農奴太多土地不夠怎麽辦呢？這時候就要發動戰爭消耗多餘的人口了\n",
      "\n",
      " 這就是農戰的核心思想\n",
      "\n",
      "  至於科舉制度嗎，算是一種安撫懷柔政策吧，早期九品門第思想，造成平民沒有上升通道\n",
      "，大概武則天開始，出現科舉制度的雛形，（也許要拉攏一些底層勢力抗衡 朝中的貴族階\n",
      "級，畢竟武后的政敵很多）\n",
      "\n",
      "  農民有了科舉制度後，也就是下階層能夠上升，國家有更多人才使用，另外下階層會跟上\n",
      "階層內鬥，間接分化內部矛盾指向統治者，保護王權\n",
      "\n",
      " 所以 科舉制度 算是 帝國2.0的新制度發明吧\n",
      "\n",
      "\n",
      " 大概就是醬\n",
      "\n",
      " 樓下  李白 怎麽看？\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Example 1: PTT Gossipping\n",
    "from requests_html import HTMLSession\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "## HTML session\n",
    "session = HTMLSession()\n",
    "session.cookies.set('over18', '1')  # age verification\n",
    "\n",
    "## Processing index page\n",
    "newsurl = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "response = session.get(newsurl)\n",
    "web_content = response.text\n",
    "\n",
    "## Extract all hyperlinks\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "title = [x['href'] for x in soup.select('div.title > a')]\n",
    "print(title[:5])\n",
    "\n",
    "## Extract first article content\n",
    "first_art_link = 'https://www.ptt.cc/'+ title[0]\n",
    "print(first_art_link)\n",
    "\n",
    "response = session.get(first_art_link)\n",
    "web_content = response.text\n",
    "\n",
    "## Extract article from HTML\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "\n",
    "## Article Metadata\n",
    "header = soup.find_all('span','article-meta-value')\n",
    "\n",
    "# Author\n",
    "author = header[0].text\n",
    "# Board\n",
    "board = header[1].text\n",
    "# Title\n",
    "title = header[2].text\n",
    "# Date\n",
    "date = header[3].text\n",
    "\n",
    "\n",
    "## Artcle Content\n",
    "main_container = soup.find(id='main-container')\n",
    "\n",
    "## Paragraphs\n",
    "all_text = main_container.text\n",
    "pre_text = all_text.split('--')[0]\n",
    "texts = pre_text.split('\\n')\n",
    "contents = texts[2:]\n",
    "content = '\\n'.join(contents)\n",
    "\n",
    "# Printing\n",
    "print('作者：'+author)\n",
    "print('看板：'+board)\n",
    "print('標題：'+title)\n",
    "print('日期：'+date)\n",
    "print('內容：'+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPJ6J-4horsE",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "## Example 2: China Times (Not working coz IP is locked down)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "## China Times Homepage\n",
    "newsurl = 'https://www.chinatimes.com/realtimenews/?chdtv'\n",
    "r = requests.get(newsurl)\n",
    "web_content = r.text\n",
    "print(r.text)\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "title = [x['href'] for x in soup.select('h3.title > a')]\n",
    "\n",
    "first_art_link = 'https://www.chinatimes.com'+ title[0]+'?chdtv'\n",
    "\n",
    "#print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding='utf8'\n",
    "soup_art = BeautifulSoup(art_request.text,'html.parser')\n",
    "\n",
    "art_content = soup_art.find_all('p')\n",
    "art_texts = [p.text for p in art_content]\n",
    "print(art_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8ohIJ7ZorsG"
   },
   "source": [
    "#### Extracting texts from scanned PDF\n",
    "\n",
    ":::{important}\n",
    "[Tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) is an open source text recognition (OCR) Engine, available under the Apache 2.0 license. It can be used directly, or (for programmers) using an API to extract printed text from images. It supports a wide variety of languages.\n",
    "\n",
    "You have to **manually** install it before you can use it in Python.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- ##\n",
    "## Colab Only           ##\n",
    "## -------------------- ##\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11024,
     "status": "ok",
     "timestamp": 1708557701281,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "uUC0oKqJorsG",
    "outputId": "4d921cd4-11c1-4f97-af0f-4bb5516054f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stellenbosch Papers in Linguistics, Vol. 15, 1986, 31-60 doi: 10.5774/15-0-96\n",
      "\n",
      "SPIL 14 (1986) 31- 6¢ 31\n",
      "\n",
      "THE LINGUISTIC THOUGHT OF J.R. FIRTH\n",
      "\n",
      "Nigel Love\n",
      "\n",
      "\"The study of the living votce of a\n",
      "man tn aectton ts a very btg job in-\n",
      "\n",
      "ii\n",
      "deed.\" --- J.R. Firth\n",
      "\n",
      "John Rupert Firth was born in 1890. After serving as Pro-\n",
      "fessor of English at the University of the Punjab from 1919\n",
      "to 1928, he took up a pest in the phonetics department of\n",
      "University College, London. In 1938 he moved to the lin-\n",
      "guistics department of the School of Oriental and African\n",
      "Studies in London, where from 1944 until his retirement in\n",
      "1956 he was Professor of Generali Linguistics. He died in\n",
      "1960. He was an influential teacher, some of whose doctrines\n",
      "(especially those concerning phonology) were widely propa-~\n",
      "gated and developed by his students in what came to be known\n",
      "\n",
      "as the \"London school” of linguistics.\n",
      "\n",
      "\"The business of linguistics\", according to Firth, \"is to\n",
      "\n",
      "1}\n",
      "\n",
      "describe languages\". In saying as much he would have the\n",
      "assent of most twentieth-century linguistic theorists.\n",
      "\n",
      "Where he parts company with many is in holding that this\n",
      "enterprise is not incompatible with, or even separable from,\n",
      "studying “the living voice of a man in action\"; and his\n",
      "chief interest as a linguistic thinker lies in his attempt\n",
      "to resist the idea that synchronic descriptive linguistics\n",
      "should treat what he calis “speech-events\" as no more than\n",
      "a means of access to what really interests the linguist:\n",
      "\n",
      "the Language-system underlying them.\n",
      "\n",
      "Languages, according to many theorists, are to be envisaged\n",
      "as systems of abstract entities. These entities are units\n",
      "\n",
      "of linguistic “form. Units of linguistic form are of two\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "#import os\n",
    "#print(os.getcwd())\n",
    "\n",
    "YOUR_DEMO_DATA_PATH = \"/content/drive/MyDrive/ENC2045_demo_data/\"  # please change your file path on local machines\n",
    "filename = YOUR_DEMO_DATA_PATH+'pdf-firth-text.png'\n",
    "\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3Qs8BOkorsG"
   },
   "source": [
    "### Unicode normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1708557429691,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "q-EZd4D0orsG",
    "outputId": "d7c7e069-5f43-42df-85b4-ad68d112b26f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ\n",
      "b'I feel really \\xf0\\x9f\\x98\\xa1. GOGOGO!! \\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa  \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f\\xa4\\xa3 \\xc8\\x80\\xc3\\x86\\xc4\\x8e\\xc7\\xa6\\xc6\\x93'\n"
     ]
    }
   ],
   "source": [
    "text = 'I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ'\n",
    "print(text)\n",
    "\n",
    "## encode the strings in bytes\n",
    "text2 = text.encode('utf-8') \n",
    "print(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1708557432385,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "1f0MgZ5KorsG",
    "outputId": "a2eafa1f-60c3-4e52-a06d-8c0ee2990e04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel really . GOGOGO!!    ADG'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "## normalize by decomposition\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel really . GOGOGO!!    '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## normalize by composition\n",
    "unicodedata.normalize('NFKC', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize full-widths punks\n",
    "text_punks = '，。「」『』！《'\n",
    "text_punks2 = unicodedata.normalize('NFKD', text_punks).encode('utf-8', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，。「」『』！《\n",
      ",。「」『』!《\n"
     ]
    }
   ],
   "source": [
    "print(text_punks)\n",
    "print(text_punks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65292\n",
      "44\n",
      "65281\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "## Check comma and exclamation mark\n",
    "print(ord(text_punks[0]))\n",
    "print(ord(text_punks2[0])) ## normalized\n",
    "\n",
    "print(ord(text_punks[6]))\n",
    "print(ord(text_punks2[6])) ## normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    ":class: dropdown\n",
    "\n",
    "1. **NFKC (Normalization Form Compatibility Composition)**:\n",
    "   - **Compatibility**: NFKC focuses on ensuring compatibility between different characters or character sequences that have similar meanings or appearances but may be encoded differently.\n",
    "   - **Composition**: NFKC may compose multiple characters into a single, equivalent character if they represent the same semantic meaning. It aims to create a more compact representation by combining characters where possible.\n",
    "\n",
    "2. **NFKD (Normalization Form Compatibility Decomposition)**:\n",
    "   - **Compatibility**: Like NFKC, NFKD also focuses on ensuring compatibility between characters, but it does this through decomposition rather than composition.\n",
    "   - **Decomposition**: NFKD decomposes characters or character sequences into their simplest form. It separates complex characters into their basic components, including base characters and any diacritic marks or modifiers.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    ":class: dropdown\n",
    "\n",
    "A few notes on unicode:\n",
    "\n",
    "1. **Unicode**: Unicode is a standard for encoding characters in digital systems. Each character is assigned a unique code point, which is a numerical value that represents that character. For example, the Unicode code point for the character \"我\" is `U+6211`.\n",
    "\n",
    "2. **ord()**: In Python, the `ord()` function returns the Unicode code point (in decimal format) for a given character. For example, to get the Unicode code point for the character \"我\", you would use `ord(\"我\")`, which returns `25105`.\n",
    "\n",
    "3. **hex()**: The `hex()` function in Python converts an integer to its hexadecimal representation. So, if we pass the Unicode code point of \"我\" to `hex()`, it will return its hexadecimal representation. For example, `hex(25105)` will return `'0x6211'`, where `'0x'` indicates that the following digits are in hexadecimal format.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25105\n",
      "0x6211\n",
      "我\n"
     ]
    }
   ],
   "source": [
    "## char vs. code number (decimal), code number (hexadecimal)\n",
    "print(ord('我'))\n",
    "print(hex(ord('我')))\n",
    "print('\\u6211')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL1I95ukorsG"
   },
   "source": [
    "- Please check [unicodedata documentation](https://docs.python.org/3/library/unicodedata.html) for more detail on character normalization.\n",
    "- Other useful libraries\n",
    "    - Spelling check: pyenchant, Microsoft REST API\n",
    "    - PDF:  PyPDF, PDFMiner\n",
    "    - OCR: pytesseract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-XzXkz2orsG"
   },
   "source": [
    "### Cleanup (Preprocessing)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fCXpRYoorsG"
   },
   "source": [
    "#### Segmentation and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J73YOjdUorsG"
   },
   "source": [
    ":::{important}\n",
    "NLTK package provides many useful datasets for text analysis. Some of the codes may require you to download the corpus data first. Please see [Installing NLTK Data](https://www.nltk.org/data.html) for more information.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1708557518828,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "rRsVNxhZ9fPK",
    "outputId": "aca65a42-c786-4ec4-ee3a-52a023c5c464"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## -------------------- ##\n",
    "## Colab Only           ##\n",
    "## -------------------- ##\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1708557477059,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "ciPslhKlorsG",
    "outputId": "38a17d7b-0ab9-481b-ae32-db1ad6f27971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is an interpreted, high-level and general-purpose programming language.\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.']\n",
      "Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
      "['Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.']\n",
      "Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
      "['Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB5pdUwKorsG"
   },
   "source": [
    "- Frequent preprocessing\n",
    "    - Stopword removal\n",
    "    - Stemming and/or lemmatization\n",
    "    - Digits/Punctuaions removal\n",
    "    - Case normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yPuG9eGorsG"
   },
   "source": [
    "#### Removing stopwords, punctuations, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1708557503587,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "K7OE7_JporsG",
    "outputId": "41f0461c-bb8d-4422-f6b9-d58998c10fc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"O'Neil\", 'works', 'at', 'Wonderland', ',', 'located', 'at', '245', 'Goleta', 'Avenue', ',', 'CA.', ',', '74208', '.']\n",
      "Mr.\n",
      "John\n",
      "O'Neil\n",
      "works\n",
      "Wonderland\n",
      "located\n",
      "Goleta\n",
      "Avenue\n",
      "CA.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIEVDPbvorsG"
   },
   "source": [
    "#### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1708557530111,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "JPiNaZuPorsH",
    "outputId": "c885b066-dec4-4be8-8b69-1b703cdc31d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'revolut', 'better']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1708557532113,
     "user": {
      "displayName": "Alvin Chen",
      "userId": "06244732172561186175"
     },
     "user_tz": -480
    },
    "id": "GmMTvhkMorsH",
    "outputId": "ff98d706-7dd7-4e1e-984f-c9d049437f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "revolution\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaStMvLsorsH"
   },
   "source": [
    "- Task-specific preprocessing\n",
    "    - Unicode normalization\n",
    "    - Language detection\n",
    "    - Code mixing\n",
    "    - Transliteration (e.g., using piyin for Chinese words in English-Chinese code-switching texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHpSPWdJorsH"
   },
   "source": [
    "- Automatic annotations\n",
    "    - POS tagging\n",
    "    - Syntactic/Dependency Parsing\n",
    "    - Named Entity Recognition\n",
    "    - Coreference Resolution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SUP7zZdorsH"
   },
   "source": [
    "### Important Reminders for Preprocessing\n",
    "\n",
    "- Not all steps are necessary\n",
    "- These steps are NOT sequential\n",
    "- These steps are task-dependent and language-dependent\n",
    "- Goals\n",
    "    - Text Normalization\n",
    "    - Text Tokenization\n",
    "    - Text Enrichment/Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWYwFZGkorsH"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zZG9WGeorsH"
   },
   "source": [
    "### What is feature engineering?\n",
    "\n",
    "- Feature engineering is the process of transforming raw data into informative features that are suitable for machine learning algorithms. \n",
    "- It usually involves creating, selecting, or modifying features from the original dataset to improve the performance of a machine learning model.\n",
    "- Feature engineering aims to extract relevant information from the data and represent it in a way that enhances the model's ability to learn patterns and make accurate predictions.\n",
    "- It aims at capturing the characteristics of the text into a numeric vector that can be understood by the ML algorithms. (Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3gaNSoWorsH"
   },
   "source": [
    "### Feature Engineering for Classical ML\n",
    "\n",
    "- Word-based frequency lists\n",
    "- Bag-of-words representations\n",
    "- Domain-specific word frequency lists\n",
    "- Handcrafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ7-SA0iorsH"
   },
   "source": [
    "### Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "- Deep learning models, particularly deep neural networks, are capable of automatically extracting meaningful features from raw data through multiple layers of abstraction. \n",
    "- This process is known as feature learning or representation learning, and it is a key advantage of deep learning over traditional machine learning approaches.\n",
    "- However, a drawback is that deep learning models are often less interpretable compared to traditional machine learning approaches.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiCtKBaqorsH"
   },
   "source": [
    "### Strengths and Weakness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs9wIKeSorsL"
   },
   "source": [
    "![](../images/feature-engineer-strengths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPOSbdXNorsL"
   },
   "source": [
    "![](../images/feature-engineer-weakness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AN0UVzKEorsL"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAs4xXKmorsL"
   },
   "source": [
    "### From Simple to Complex\n",
    "\n",
    "- Start with heuristics or rules\n",
    "- Experiment with different ML models\n",
    "    - From heuristics to features\n",
    "    - From manual annotation to automatic extraction\n",
    "    - Feature importance (weights)\n",
    "- Find the most optimal model\n",
    "    - Ensemble and stacking\n",
    "    - Redo feature engineering\n",
    "    - Transfer learning\n",
    "    - Reapply heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "- Ensemble learning combines predictions from multiple individual models directly, while stacking involves training a meta-model to combine predictions from base models.\n",
    "- Ensemble learning is typically simpler to implement but may not capture complex relationships as effectively as stacking.\n",
    "- Stacking can lead to improved performance but requires additional data for training the meta-model and is more complex to implement and tune.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtuJw1teorsL"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSYKPR9forsM"
   },
   "source": [
    "### Why evaluation?\n",
    "\n",
    "- We need to know how *good* the model we've built is -- \"Goodness\"\n",
    "- Factors relating to the evaluation methods\n",
    "    - Model building\n",
    "    - Deployment\n",
    "    - Production\n",
    "- ML metrics vs. Business metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We9gk-hCorsM"
   },
   "source": [
    "### Intrinsic vs. Extrinsic Evaluation\n",
    "\n",
    "Intrinsic evaluation assesses the performance of a machine learning model based solely on its performance on a specific task or dataset. Extrinsic evaluation assesses the performance of a machine learning model within the context of a larger system or real-world application.\n",
    "\n",
    "- Take spam-classification system as an example\n",
    "    -  Intrinsic:\n",
    "        - the precision and recall of the spam classification/prediction\n",
    "    - Extrinsic:\n",
    "        - the amount of time users spent on a spam email\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8a77KFnorsM"
   },
   "source": [
    "### General Principles\n",
    "\n",
    "- Do intrinsic evaluation before extrinsic.\n",
    "- Extrinsic evaluation is more expensive because it often invovles project stakeholders outside the AI team.\n",
    "- Only when we get consistently good results in intrinsic evaluation should we go for extrinsic evaluation.\n",
    "- Bad results in intrinsic often implies bad results in extrinsic as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8XeBd-iorsM"
   },
   "source": [
    "### Common Intrinsic Metrics\n",
    "\n",
    "- Principles for Evaluation Metrics Selection\n",
    "- Data type of the labels (ground truths)\n",
    "    - Binary (e.g., sentiment)\n",
    "    - Ordinal (e.g., informational retrieval)\n",
    "    - Categorical (e.g., POS tags)\n",
    "    - Textual (e.g., named entity, machine translation, text generation)\n",
    "- Automatic vs. Human Evalation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7M-EBYeorsM"
   },
   "source": [
    "## Post-Modeling Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWl7_aE_orsM"
   },
   "source": [
    "### Post-Modeling Phases\n",
    "\n",
    "- Deployment of the model in a  production environment (e.g., web service)\n",
    "- Monitoring system performance on a regular basis\n",
    "- Updating system with new-coming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q9LNGAnorsM"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQz0nXEuorsM"
   },
   "source": [
    "- Chapter 2 of Practical Natural Language Processing. {cite}`vajjala2020`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.571px",
    "left": "653px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}