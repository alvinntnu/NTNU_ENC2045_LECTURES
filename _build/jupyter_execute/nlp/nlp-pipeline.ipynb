{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    " # NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A General NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nlp-pipeline](../images/nlp-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varations of the NLP Pipelines\n",
    "\n",
    "- The process may not always be linear.\n",
    "- There are loops in between.\n",
    "- These procedures may depend on specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Acquisition: Heart of ML System\n",
    "\n",
    "- Ideal Setting: We have everything needed.\n",
    "- Labels and Annotations\n",
    "- Very often we are dealing with less-than-ideal scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Less-than-ideal Scenarios\n",
    "\n",
    "- Initial datasets with limited annotations/labels\n",
    "- Initial datasets labeled based on regular expressions or heuristics\n",
    "- Public datasets (cf. [Google Dataset Search](https://datasetsearch.research.google.com/) or [kaggle](https://www.kaggle.com/))\n",
    "- Scrape data\n",
    "- Product intervention\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "- It is a technique to exploit language properties to create texts that are syntactically similar to the source text data.\n",
    "- Types of strategies:\n",
    "    - synonym replacement\n",
    "    - Related word replacement (based on association metrics)\n",
    "    - Back translation\n",
    "    - Replacing entities\n",
    "    - Adding noise to data (e.g. spelling errors, random words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Extraction and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "- Extracting raw texts from the input data\n",
    "    - HTML\n",
    "    - PDF\n",
    "- Relevant vs. irrelevant information\n",
    "    - non-textual information\n",
    "    - markup\n",
    "    - metadata\n",
    "- Encoding format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Extracting texts from webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Extracting textual contents from the website is a very common way to obtain data. It requires a close study of the structure of the HTML content of the web pages.\n",
    "\n",
    "The following codes attempt to extract the hyperlinks from the homepage of a news agency and automatically visit the first hyperlink for its textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['新北板橋私幼疑似餵藥案，涉案的9名教師均獲不起訴，被新北市政府點名需道歉的行政院副院長鄭文燦，今稱新北市府才需解釋為何撤照又復照。對此，新北市府回批怎麼會連為了保護孩童安全必須施行的預防性撤照都不了解？呼籲包括鄭文燦在內的造謠者們面對事實，拿出勇氣道歉，還給「被抹黑者」一個公道！', '', '市府表示，新北地檢署偵結板橋私立幼兒園餵藥案，透過科學驗證、司法及教育局行政調查，證明孩子並未被餵食不明藥物，孩子也沒有被不當對待，因此教育局立即主動撤銷案件爆發初始， 為了預防性保護幼兒安全所做的廢止設立許可行政處分，並積極予以協助，盼能儘早重新建立親師生間的信任關係。', '', '然而當初帶頭造謠的府院黨人士依舊不願誠實面對自己說過的謊言，拿出道歉的勇氣，竟還惡人先告狀，請有心人士捫心自問，到底是誰公然說出「幼兒園的餵毒案」不該發生？」，到底是誰刻意指稱「教育部有提醒，一直到提醒5次之後，檢調進行偵辦，移送這個案件，才做成撤照的處分」，到底是誰以直接或間接的影射性發言製造社會恐慌？不就是國家最高行政機關的行政院副院長嗎？', '', '無獨有偶，執政黨立委賴品妤委員曾企圖將汐止幼兒園案製造成另一波餵藥案延燒的假象，如今真相大白，不但不思改過道歉， 竟反過來要新北市政府回頭是岸 ，極盡顛倒是非之能事。請賴委員別耍賴了！「回頭是岸」這句話送給您自己最為貼切。', '', '新北市府指出，人民用神聖選票選出政黨和民意代表是希望能真心為民喉舌，而非為了選舉、為了打擊政敵信口開河、漫天撒謊，撕裂社會互信，台灣社會經過餵藥案事件後，除了省思，更需要縫合，找回人與人之間的信任與良善，呼籲造謠者們拿出勇氣誠懇道歉，找回身為政治公眾人物應有的道德良心與底線。', '', '', '中時新聞網對留言系統使用者發布的文字、圖片或檔案保有片面修改或移除的權利。當使用者使用本網站留言服務時，表示已詳細閱讀並完全了解，且同意配合下述規定：', '違反上述規定者，中時新聞網有權刪除留言，或者直接封鎖帳號！請使用者在發言前，務必先閱讀留言板規則，謝謝配合。']\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    " \n",
    "## China Times Homepage\n",
    "newsurl = 'https://www.chinatimes.com/realtimenews/?chdtv'\n",
    "r = requests.get(newsurl)\n",
    "web_content = r.text\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "title = [x['href'] for x in soup.select('h3.title > a')]\n",
    "first_art_link = 'https://www.chinatimes.com'+ title[0]+'?chdtv'\n",
    "\n",
    "#print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding='utf8'\n",
    "soup_art = BeautifulSoup(art_request.text,'html.parser')\n",
    "\n",
    "art_content = soup_art.find_all('p')\n",
    "art_texts = [p.text for p in art_content]\n",
    "print(art_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Extracting texts from scanned PDF\n",
    "\n",
    ":::{important}\n",
    "[Tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) is an open source text recognition (OCR) Engine, available under the Apache 2.0 license. It can be used directly, or (for programmers) using an API to extract printed text from images. It supports a wide variety of languages.\n",
    "\n",
    "You have to **manually** install it before you can use it in Python.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stellenbosch Papers in Linguistics, Vol. 15, 1986, 31-60 doi: 10.5774/15-0-96\n",
      "\n",
      "SPIL 16 (1986) 31- 6¢ 31\n",
      "\n",
      "THE LINGUISTIC THOUGHT OF J.R. FIRTH\n",
      "\n",
      "Nigel Love\n",
      "\n",
      "\"The study of the living votce of a\n",
      "man tn aectton ts a very btg job in-\n",
      "\n",
      "ii\n",
      "deed.\" --- J.R. Firth\n",
      "\n",
      "John Rupert Firth was born in 1890. After serving as Pro-\n",
      "fessor of English at the University of the Punjab from 1919\n",
      "to 1928, he took up a pest in the phonetics department of\n",
      "University College, London. In 1938 he moved to the lin-\n",
      "guistics department of the School of Oriental and African\n",
      "Studies in London, where from 1944 until his retirement in\n",
      "1956 he was Professor of Generali Linguistics. He died in\n",
      "1960. He was an influential teacher, some of whose doctrines\n",
      "(especially those concerning phonology) were widely propa-~\n",
      "gated and developed by his students in what came to be known\n",
      "\n",
      "as the \"London school” of linguistics.\n",
      "\n",
      "\"The business of linguistics\", according to Firth, \"is to\n",
      "\n",
      "1}\n",
      "\n",
      "describe languages\". In saying as much he would have the\n",
      "assent of most twentieth-century linguistic theorists.\n",
      "\n",
      "Where he parts company with many is in holding that this\n",
      "enterprise is not incompatible with, or even separable from,\n",
      "studying “the living voice of a man in action\"; and his\n",
      "chief interest as a linguistic thinker lies in his attempt\n",
      "to resist the idea that synchronic descriptive linguistics\n",
      "should treat what he calis “speech-events\" as no more than\n",
      "a means of access to what really interests the linguist:\n",
      "\n",
      "the Language-system underlying them.\n",
      "\n",
      "Languages, according to many theorists, are to be envisaged\n",
      "as systems of abstract entities. These entities are units\n",
      "\n",
      "of linguistic “form\". Units of linguistic form are of two\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "#import os\n",
    "#print(os.getcwd())\n",
    "\n",
    "YOUR_DEMO_DATA_PATH = \"../../../RepositoryData/data/\"  # please change your file path\n",
    "filename = YOUR_DEMO_DATA_PATH+'pdf-firth-text.png'\n",
    "\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Unicode normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ\n",
      "b'I feel really \\xf0\\x9f\\x98\\xa1. GOGOGO!! \\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa  \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f\\xa4\\xa3 \\xc8\\x80\\xc3\\x86\\xc4\\x8e\\xc7\\xa6\\xc6\\x93'\n"
     ]
    }
   ],
   "source": [
    "text = 'I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ'\n",
    "print(text)\n",
    "text2 = text.encode('utf-8') # encode the strings in bytes\n",
    "print(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel really . GOGOGO!!    ADG'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Please check [unicodedata documentation](https://docs.python.org/3/library/unicodedata.html) for more detail on character normalization.\n",
    "- Other useful libraries\n",
    "    - Spelling check: pyenchant, Microsoft REST API\n",
    "    - PDF:  PyPDF, PDFMiner\n",
    "    - OCR: pytesseract\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cleanup\n",
    "\n",
    "- Preliminaries\n",
    "    - Sentence segmentation\n",
    "    - Word tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Segmentation and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{important}\n",
    "NLTK package provides many useful datasets for text analysis. Some of the codes may require you to download the corpus data first. Please see [Installing NLTK Data](https://www.nltk.org/data.html) for more information.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is an interpreted, high-level and general-purpose programming language.\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.']\n",
      "Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
      "['Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.']\n",
      "Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
      "['Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Frequent preprocessing\n",
    "    - Stopword removal\n",
    "    - Stemming and/or lemmatization\n",
    "    - Digits/Punctuaions removal\n",
    "    - Case normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Removing stopwords, punctuations, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"O'Neil\", 'works', 'at', 'Wonderland', ',', 'located', 'at', '245', 'Goleta', 'Avenue', ',', 'CA.', ',', '74208', '.']\n",
      "Mr.\n",
      "John\n",
      "O'Neil\n",
      "works\n",
      "Wonderland\n",
      "located\n",
      "Goleta\n",
      "Avenue\n",
      "CA.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'revolut', 'better']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "revolution\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Task-specific preprocessing\n",
    "    - Unicode normalization\n",
    "    - Language detection\n",
    "    - Code mixing\n",
    "    - Transliteration (e.g., using piyin for Chinese words in English-Chinese code-switching texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Automatic annotations\n",
    "    - POS tagging\n",
    "    - Parsing\n",
    "    - Named Entity Recognition\n",
    "    - Coreference resolution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Important Reminders for Preprocessing\n",
    "\n",
    "- Not all steps are necessary\n",
    "- These steps are NOT sequential\n",
    "- These steps are task-dependent\n",
    "- Goals\n",
    "    - Text Normalization\n",
    "    - Text Tokenization\n",
    "    - Text Enrichment/Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is feature engineering?\n",
    "\n",
    "- It refers to a process to feed the extracted and preprocessed texts into a machine-learning algorithm.\n",
    "- It aims at capturing the characteristics of the text into a numeric vector that can be understood by the ML algorithms. (Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for Classical ML\n",
    "\n",
    "- Word-based frequency lists\n",
    "- Bag-of-words representations\n",
    "- Domain-specific word frequency lists\n",
    "- Handcrafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "- The DL model is capable of learning features from the texts (e.g., embeddings)\n",
    "- The price is that the model is often less interpretable.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strengths and Weakness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/feature-engineer-strengths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/feature-engineer-weakness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Simple to Complex\n",
    "\n",
    "- Start with heuristics or rules\n",
    "- Experiment with different ML models\n",
    "    - From heuristics to features\n",
    "    - From manual annotation to automatic extraction\n",
    "    - Feature importance (weights)\n",
    "- Find the most optimal model\n",
    "    - Ensemble and stacking\n",
    "    - Redo feature engineering\n",
    "    - Transfer learning\n",
    "    - Reapply heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why evaluation?\n",
    "\n",
    "- We need to know how *good* the model we've built is -- \"Goodness\"\n",
    "- Factors relating to the evaluation methods\n",
    "    - Model building\n",
    "    - Deployment\n",
    "    - Production\n",
    "- ML metrics vs. Business metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intrinsic vs. Extrinsic Evaluation\n",
    "\n",
    "- Take spam-classification system as an example\n",
    "- Intrinsic:\n",
    "    - the precision and recall of the spam classification/prediction\n",
    "- Extrinsic:\n",
    "    - the amount of time users spent on a spam email\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General Principles\n",
    "\n",
    "- Do intrinsic evaluation before extrinsic.\n",
    "- Extrinsic evaluation is more expensive because it often invovles project stakeholders outside the AI team.\n",
    "- Only when we get consistently good results in intrinsic evaluation should we go for extrinsic evaluation.\n",
    "- Bad results in intrinsic often implies bad results in extrinsic as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Intrinsic Metrics\n",
    "\n",
    "- Principles for Evaluation Metrics Selection\n",
    "- Data type of the labels (ground truths)\n",
    "    - Binary (e.g., sentiment)\n",
    "    - Ordinal (e.g., informational retrieval)\n",
    "    - Categorical (e.g., POS tags)\n",
    "    - Textual (e.g., named entity, machine translation, text generation)\n",
    "- Automatic vs. Human Evalation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-Modeling Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Post-Modeling Phases\n",
    "\n",
    "- Deployment of the model in a  production environment (e.g., web service)\n",
    "- Monitoring system performance on a regular basis\n",
    "- Updating system with new-coming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Chapter 2 of Practical Natural Language Processing. {cite}`vajjala2020`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes-2023",
   "language": "python",
   "name": "python-notes-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.571px",
    "left": "653px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}