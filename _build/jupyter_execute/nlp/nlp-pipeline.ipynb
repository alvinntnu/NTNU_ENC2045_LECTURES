{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    " # NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A General NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nlp-pipeline](../images/nlp-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varations of the NLP Pipelines\n",
    "\n",
    "- The process may not always be linear.\n",
    "- There are loops in between.\n",
    "- These procedures may depend on specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Acquisition: Heart of ML System\n",
    "\n",
    "- Ideal Setting: We have everything needed.\n",
    "- Labels and Annotations\n",
    "- Very often we are dealing with less-than-ideal scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Less-than-ideal Scenarios\n",
    "\n",
    "- Initial datasets with limited annotations/labels\n",
    "- Initial datasets labeled based on regular expressions or heuristics\n",
    "- Public datasets (cf. [Google Dataset Search](https://datasetsearch.research.google.com/) or [kaggle](https://www.kaggle.com/))\n",
    "- Scrape data\n",
    "- Product intervention\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "- It is a technique to exploit language properties to create texts that are syntactically similar to the source text data.\n",
    "- Types of strategies:\n",
    "    - synonym replacement\n",
    "    - Related word replacement (based on association metrics)\n",
    "    - Back translation\n",
    "    - Replacing entities\n",
    "    - Adding noise to data (e.g. spelling errors, random words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Extraction and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "- Extracting raw texts from the input data\n",
    "    - HTML\n",
    "    - PDF\n",
    "- Relevant vs. irrelevant information\n",
    "    - non-textual information\n",
    "    - markup\n",
    "    - metadata\n",
    "- Encoding format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Extracting texts from webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Extracting textual contents from the website is a very common way to obtain data. It requires a close study of the structure of the HTML content of the web pages.\n",
    "\n",
    "The following codes attempt to extract the hyperlinks from the homepage of a news agency and automatically visit the first hyperlink for its textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['據《華爾街日報》報導，眾多中國大陸城市在資金短缺、經濟困難之際，正紛紛拋出前所未有的甜頭向西方企業示好。中國大陸政府在2023年啟動了「投資中國年」活動，地方官員們開啟了面向海外的推介宣傳之旅，以吸引投資者的興趣。但上述招商努力迎頭撞上了國家主席習近平的國家安全議程，相關議程側重抵禦感受到的外來威脅，對外國公司來說，這已經使任何對華投資都成為潛在雷區。', '', '今年以來的一場由習近平領導的運動，讓西方管理顧問公司、審計公司等機構遭遇了一連串的突擊搜查、調查和拘留行動。與此同時，反間諜法的拓展使外國公司高管愈發擔心，在中國大陸開展市場調研等常規商業活動可能被視為間諜活動。', '', '中國大陸經濟已經因私營部門投資不振、消費疲軟和青年失業率飆升而步履艱難，而有關在中國大陸做生意風險大增的看法正妨礙著資金流入。', '', '報導引述研究公司榮鼎集團（Rhodium Group）分析師Mark Witzke對中國大陸政府數據的分析，今年第一季，中國大陸的外商直接投資額降至200億美元，去年第一季度則為1000億美元。此外，高盛（Goldman Sachs）經濟學家預計，今年中國大陸的資金流出額將抵消投資流入額，對一個在過去40年裡資金流入一直多於流出的國家來說，這是一個相當驚人的變化。', '', '報導稱，近幾十年來，對西方開放為中國大陸經濟增長提供了助力，這一增長依靠外國投資和專業知識來推動創新和提高生產率。對中國大陸領導人來說，一方面向外國企業施壓，同時還要努力吸引這些企業投資，這樣的試圖平衡之舉充滿風險，有可能使中國大陸失去助力其崛起的資金、技術、理念和管理技能。', '', '陷入困境的城市', '', '報導稱，這場拉鋸戰使中國大陸各地陷入財政困境的城鎮飽受煎熬。許多城市都急需資金，在經歷了三年的新冠限制政策後，這些城市深陷債務泥淖，難以創造就業機會。', '', '報導引述中國大陸官方統計，去年地方政府的支出同比增加，主要原因是用於支付新冠檢測和相關費用的衛生支出猛增了18％。與此同時，地方政府的收入卻下降，主要原因是向開發商出讓土地的收入同比銳減23％，而土地出讓是地方政府長期依賴的資金來源。地方政府的借貸超過了他們的償還能力，直接欠下的債務高達收入的120％。', '', '許多官員表示，他們吸引外資的傳統策略不再奏效。', '', '報導稱，在中國大陸西南部四川省的省會成都，一位貿易官員最近赴歐洲招商引資，最終卻空手而歸。這位官員說，在他對歐洲招商引資的20年生涯裡，這是第一次連一份諒解備忘錄都沒有簽到。', '', '報導引述知情人士透露，在南方省份廣東的一個縣，一位高官最近告訴一個來訪的美國貿易代表團，該縣將對在當地投資的美國企業決策者「回饋」所承諾投資交易價值的10％。廣東省今年早些時候制定了未來5年吸引近3000億美元投資的目標。', '', '上述知情人士稱，這個貿易代表團拒絕了該縣這名官員的提議，因為這種行為在美國將構成非法賄賂。廣東省政府沒有回覆記者的置評請求。', '', '外企按下暫停鍵', '', '報導稱，近期中國大陸商業團體開展的調查顯示，美國、德國和其他歐洲公司暫停了在華擴張或減少了投資。克瑞公司（Crane）是一家美國大型製造商，自上世紀90年代以來一直在中國大陸生產自動售貨機和其他工業產品，據接近該公司的人士說，克瑞公司已大幅縮減了在中國大陸的投資，部分是因為中國大陸政策的不確定性加大。總部位於康乃狄克州史丹佛的克瑞公司對記者提出的問題未予回應。', '', '上海美國商會主席、曾任美國駐上海總領事的譚森（Sean Stein）表示，最近美國諮詢公司承受的壓力有可能「切斷外國企業獲取資訊的管道」。', '', '美國財長葉倫（Janet Yellen）上周在北京與中國大陸高級官員會面時就中國大陸對待美國公司的做法表達了反對。預計美國商務部長雷蒙多（Gina Raimondo）也將在她即將開始的中國大陸之行中提出這個問題。', '', '報導稱，在浙江省的海港城市寧波，當地官員舉辦了一次「投資浙裡」論壇，在論壇上，他們向外國投資者推介了一系列舉措，包括修建更好的道路和管道，為購買高端設備提供稅收優惠和補貼。美國諮詢公司TidalWave Solutions的合夥人Cameron Johnson說：「他們要傳達的資訊是：我們真的對商業持開放態度。」', '', '他表示，與此同時，中國大陸政府政策的不確定性導致全球企業董事會一籌莫展。「政府的真正關注點是什麼？」身處中國大陸逾20年的美國人Johnson問道。「能否在政策方面更加清晰或者提供更多指引，以便外國企業可以制定出合規的路線圖？」', '', '自相矛盾的資訊', '', '報導稱，總部位於美國奧勒岡州波特蘭的Pixelworks一直受到上海當地官員的熱烈歡迎，該公司是影片和其他電子顯示設備所用晶片的設計商和生產商，在上海設有辦事處。這些官員尤其支持Pixelworks首席執行官杜泰德（Todd DeBonis）為其中國大陸子公司在中國版納斯達克上海科創板上市所做的努力。', '', '杜泰德說：「我們將全力投入中國市場。」他還說，Pixelworks的大部分研發人才都在中國大陸，該公司的大部分收入也來自中國大陸。', '', '報導稱，儘管擁有上海當地政府的支持，Pixelworks仍面臨著來自中國大陸中央政府要求其重組中國大陸子公司的壓力，目的是要確保這家子公司能獨立於Pixelworks的在美業務。為跨國公司提供諮詢服務的商業顧問和律師說，作為中國大陸政府國家安全議程的一部分，對於外國公司的這種要求正變得越來越多。', '', '對Pixelworks來說，這樣的要求就意味著該公司不得不將自身一分為二，把中國大陸業務從美國母公司中分離出來，以便獲得中國大陸監管機構批准其IPO申請。', '', '在過去兩年半的時間裡，Pixelworks為了讓其中國大陸子公司獨立於美國母公司，歷經了艱辛。在分離過程中，Pixelworks已將在華運營業務所用到的智慧財產權從美國母公司轉移到了中國大陸實體，此舉旨在確保這些專利和商標的安全，防止美國的潛在制裁使這些智慧財產權無法在中國大陸市場使用。', '', '報導稱，為了滿足中國大陸的安全關切，Pixelworks最近將為美國母公司項目工作的15名員工遷到了該公司辦公樓的另外一層。這些員工都是中國大陸公民，他們使用的是與Pixelworks在華業務完全分開的獨立辦公網路，他們的工作範圍僅限於美國項目。', '', '杜泰德說，6月下旬，北京的幾位中國大陸商務部官員到訪了Pixelworks的辦公室，以「更好地了解」該公司業務及其分拆中國大陸部門的進展情況。當時，杜泰德正在蒙大拿州參加一個中美論壇和飛釣活動，他的中國大陸員工向他通報了這次訪問的消息。', '', '杜泰德說，Pixelworks中國大陸部門致力於在今年晚些時候向中國大陸監管機構提交IPO申請。要想獲得批准，該公司需要讓中國大陸政府相信，其智慧財產權能夠抵禦任何潛在美國制裁的影響。', '', '杜泰德說：「除非你降低中國股東的風險，否則他們不會批准你的申請。」', '中時新聞網對留言系統使用者發布的文字、圖片或檔案保有片面修改或移除的權利。當使用者使用本網站留言服務時，表示已詳細閱讀並完全了解，且同意配合下述規定：', '違反上述規定者，中時新聞網有權刪除留言，或者直接封鎖帳號！請使用者在發言前，務必先閱讀留言板規則，謝謝配合。']\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    " \n",
    "## China Times Homepage\n",
    "newsurl = 'https://www.chinatimes.com/realtimenews/?chdtv'\n",
    "r = requests.get(newsurl)\n",
    "web_content = r.text\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "title = [x['href'] for x in soup.select('h3.title > a')]\n",
    "first_art_link = 'https://www.chinatimes.com'+ title[0]+'?chdtv'\n",
    "\n",
    "#print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding='utf8'\n",
    "soup_art = BeautifulSoup(art_request.text,'html.parser')\n",
    "\n",
    "art_content = soup_art.find_all('p')\n",
    "art_texts = [p.text for p in art_content]\n",
    "print(art_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Extracting texts from scanned PDF\n",
    "\n",
    ":::{important}\n",
    "[Tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) is an open source text recognition (OCR) Engine, available under the Apache 2.0 license. It can be used directly, or (for programmers) using an API to extract printed text from images. It supports a wide variety of languages.\n",
    "\n",
    "You have to **manually** install it before you can use it in Python.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytesseract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytesseract\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_to_string\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#import os\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(os.getcwd())\u001b[39;00m\n\u001b[1;32m      7\u001b[0m YOUR_DEMO_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../RepositoryData/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# please change your file path\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytesseract'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "#import os\n",
    "#print(os.getcwd())\n",
    "\n",
    "YOUR_DEMO_DATA_PATH = \"../../../RepositoryData/data/\"  # please change your file path\n",
    "filename = YOUR_DEMO_DATA_PATH+'pdf-firth-text.png'\n",
    "\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Unicode normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = 'I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ'\n",
    "print(text)\n",
    "text2 = text.encode('utf-8') # encode the strings in bytes\n",
    "print(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Please check [unicodedata documentation](https://docs.python.org/3/library/unicodedata.html) for more detail on character normalization.\n",
    "- Other useful libraries\n",
    "    - Spelling check: pyenchant, Microsoft REST API\n",
    "    - PDF:  PyPDF, PDFMiner\n",
    "    - OCR: pytesseract\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cleanup\n",
    "\n",
    "- Preliminaries\n",
    "    - Sentence segmentation\n",
    "    - Word tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Segmentation and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{important}\n",
    "NLTK package provides many useful datasets for text analysis. Some of the codes may require you to download the corpus data first. Please see [Installing NLTK Data](https://www.nltk.org/data.html) for more information.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Frequent preprocessing\n",
    "    - Stopword removal\n",
    "    - Stemming and/or lemmatization\n",
    "    - Digits/Punctuaions removal\n",
    "    - Case normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Removing stopwords, punctuations, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Task-specific preprocessing\n",
    "    - Unicode normalization\n",
    "    - Language detection\n",
    "    - Code mixing\n",
    "    - Transliteration (e.g., using piyin for Chinese words in English-Chinese code-switching texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Automatic annotations\n",
    "    - POS tagging\n",
    "    - Parsing\n",
    "    - Named Entity Recognition\n",
    "    - Coreference resolution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Important Reminders for Preprocessing\n",
    "\n",
    "- Not all steps are necessary\n",
    "- These steps are NOT sequential\n",
    "- These steps are task-dependent\n",
    "- Goals\n",
    "    - Text Normalization\n",
    "    - Text Tokenization\n",
    "    - Text Enrichment/Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is feature engineering?\n",
    "\n",
    "- It refers to a process to feed the extracted and preprocessed texts into a machine-learning algorithm.\n",
    "- It aims at capturing the characteristics of the text into a numeric vector that can be understood by the ML algorithms. (Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for Classical ML\n",
    "\n",
    "- Word-based frequency lists\n",
    "- Bag-of-words representations\n",
    "- Domain-specific word frequency lists\n",
    "- Handcrafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "- The DL model is capable of learning features from the texts (e.g., embeddings)\n",
    "- The price is that the model is often less interpretable.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strengths and Weakness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/feature-engineer-strengths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/feature-engineer-weakness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Simple to Complex\n",
    "\n",
    "- Start with heuristics or rules\n",
    "- Experiment with different ML models\n",
    "    - From heuristics to features\n",
    "    - From manual annotation to automatic extraction\n",
    "    - Feature importance (weights)\n",
    "- Find the most optimal model\n",
    "    - Ensemble and stacking\n",
    "    - Redo feature engineering\n",
    "    - Transfer learning\n",
    "    - Reapply heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why evaluation?\n",
    "\n",
    "- We need to know how *good* the model we've built is -- \"Goodness\"\n",
    "- Factors relating to the evaluation methods\n",
    "    - Model building\n",
    "    - Deployment\n",
    "    - Production\n",
    "- ML metrics vs. Business metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intrinsic vs. Extrinsic Evaluation\n",
    "\n",
    "- Take spam-classification system as an example\n",
    "- Intrinsic:\n",
    "    - the precision and recall of the spam classification/prediction\n",
    "- Extrinsic:\n",
    "    - the amount of time users spent on a spam email\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General Principles\n",
    "\n",
    "- Do intrinsic evaluation before extrinsic.\n",
    "- Extrinsic evaluation is more expensive because it often invovles project stakeholders outside the AI team.\n",
    "- Only when we get consistently good results in intrinsic evaluation should we go for extrinsic evaluation.\n",
    "- Bad results in intrinsic often implies bad results in extrinsic as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Intrinsic Metrics\n",
    "\n",
    "- Principles for Evaluation Metrics Selection\n",
    "- Data type of the labels (ground truths)\n",
    "    - Binary (e.g., sentiment)\n",
    "    - Ordinal (e.g., informational retrieval)\n",
    "    - Categorical (e.g., POS tags)\n",
    "    - Textual (e.g., named entity, machine translation, text generation)\n",
    "- Automatic vs. Human Evalation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-Modeling Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Post-Modeling Phases\n",
    "\n",
    "- Deployment of the model in a  production environment (e.g., web service)\n",
    "- Monitoring system performance on a regular basis\n",
    "- Updating system with new-coming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Chapter 2 of Practical Natural Language Processing. {cite}`vajjala2020`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes-2023",
   "language": "python",
   "name": "python-notes-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.571px",
    "left": "653px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}