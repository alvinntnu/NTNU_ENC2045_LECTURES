{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Before the era of deep learning, probability-based classifiers are commonly used in many ML NLP tasks.\n",
    "- There are two types of probability-based models: **generative** and **disriminative** models.\n",
    "- Let's assume that we have our data as ***d***, and their class labels as ***c***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Generative**:\n",
    "    - The goal of training is NOT to find $P(c|d)$. \n",
    "    - Rather, based on the **Bayes Theorem**, we can estimate the joint probability of $P(c,d)$, which, according to Bayes Theorem, can be reformulated as $P(c,d)=P(d)\\times P(c|d) =P(c) \\times P(d|c)$\n",
    "    - That is, the training in generative models is based on the **joint** probability of the data and the class, i.e., $P(c,d)$\n",
    "    - Examples: N-gram Language Models, Naive Bayes, Hidden Markov Model, Probabilistic Context-Free Grammars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Distriminative**: \n",
    "    - The goal of training is to directly find $P(c|d)$.\n",
    "    - Training is based on the **conditional** Probablity of the class given the data, i.e., $P(c|d)$.\n",
    "    - Examples: Logistic regression, Maximum Entropy Models, Conditional Random Field, Support Vector Machine, Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "- Strengths of **discriminative** models:\n",
    "    - They give high accuracy performance.\n",
    "    - They make it easier to include linguistically relevant features.\n",
    "- Other classification models:\n",
    "    - Tree-based methods (Decision tree, Random Forest)\n",
    "    - Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/naive-bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "In Bayes, the $P(Y)$ is referred to as the **prior probability** of Y and the $P(Y|X)$ is referred to as the **posterior probability** of Y.\n",
    "```\n",
    "````\n",
    "\n",
    "- Naive Bayes features the Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(Y|X) = P(Y) \\frac{P(X|Y)}{P(X)} \n",
    "$$ (naive_bayes1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intuition of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/ml-naive-bayes.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In Naive Bayes, given a document $d$ and a class $c$, the goal is to find the **maximum joint probability** $P(c,d)$. And according bayes rule, the goal (of finding the maximum joint probability) can be reformulated as finding the maximum of the **posterior probability** of the class, $P(c|d)$:\n",
    "\n",
    "$$\n",
    "P(c,d) = P(d|c)\\times P(c) = P(c|d) \\times P(d)\\\\\n",
    "P(c|d) = \\frac{P(d|c)\\times P(c)}{P(d)}\n",
    "$$ (navie_bayes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Because the $P(d)$ is a constant for all classes estimation, we can drop the denominator. And now the goal is to find the class $c$ that maximizes the **posterior probability** of the class, i.e., $P(c|d)$. (MAP = Maximum A Posterior Estimation)\n",
    "\n",
    "$$\n",
    "C_{MAP} = \\underset{c \\in C}{\\arg\\max}P(c|d)\\\\\n",
    "C_{MAP}= \\underset{c \\in C}{\\arg\\max}\\frac{P(d|c)\\times P(c)}{P(d)} \\\\\n",
    "= \\underset{c \\in C}{\\arg\\max}P(d|c)\\times P(c) \\\\\n",
    "$$ (naive_bayes3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In Naive Bayes, the probabilities $P(C=c_i)$ and $P(X_i|C=c_i)$ are **parameters**. \n",
    "- The standard, maximum likelihood, approach is to calculate these parameters (probabilities) using MLE estimators based on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For example, for class prior probabilities, $P(C=c_i)$, we count the cases where $C=c_i$ and divide by the sample size.\n",
    "- Similarly, each feature $x_i$ of the document is in the form of a probability value of an observed event given the class, $P(x_i|C=c_i)$. For example, the probability of observing the word '好看' (**$x_i$**) when the document class is **$c_i$**. Same for conditional probabilities $P(好看|C=c_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A document needs to be vectorized through feature engineering into a numeric representation, i.e., a set of **n** features from the document, $\\{x_1, x_2, ..., x_n\\}$\n",
    "- Given a class $c$ and a set of **$n$** features from the document, $\\{x_1, x_2, ..., x_n\\}$, we can denote the **posterior probability** of each class $c$ as follows:\n",
    "\n",
    "$$\n",
    "C_{MAP}= \\underset{c \\in C}{\\arg\\max}P(c|d)\\\\\\\\\n",
    "\\underset{c \\in C}{\\arg\\max}  P(d|c) \\times P(c) \\\\\n",
    "= \\underset{c \\in C}{\\arg\\max} P(x_1,x_2,...,x_n|c) \\times P(c) \\\\\n",
    "$$ (naive_bayes4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Important assumptions in Naive Bayes:\n",
    "    - Independence Assumption: Assume features are independent of each other.\n",
    "    - Conditional Independence: Assume the feature probabilities $P(x_i|C=c_i)$ are independent given the label $c$.\n",
    "\n",
    "$$\n",
    "P(x_1,x_2,...,x_n|c)=P(x_1|c)\\times P(x_2|c)\\times P(x_3|c) \\times ... \\times P(x_n|c)\n",
    "$$ (naive_bayes5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Then we can simply the equation:\n",
    "\n",
    "$$\n",
    "C_{MAP} = \\underset{c \\in C}{\\arg\\max} P(x_1,x_2,...,x_n|c) \\times P(c) \\\\\n",
    "= \\underset{c \\in C}{\\arg\\max} P(c) \\prod_{x\\in X}P(x|c)\n",
    "$$ (naive_bayes6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Multinomial Naive Bayes** is an extension of the Naive Bayes for predicting and classifying data points, where the number of distinct labels or outcomes are more than two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Issues of **Smoothing**\n",
    "    - For unseen features in a new document, e.g., $P(x|c)$, this would render the entire **posterior probability** of the class to be zero. \n",
    "    - Laplace smoothing (Add-one)\n",
    "    - Lidstone smooting (Self-defined $\\alpha$ < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Logistic Regression** is also a form of probabilistic statistical classification model.\n",
    "- The model is trained by maximizing directly the probability of the **class** (c) given the observed **data** (d), i.e., $P(c|d)$.\n",
    "- Logistic Regression is similar to Linear Regression, whose predicted values are both numeric (cf. [Generalized Linear Models](https://en.wikipedia.org/wiki/Generalized_linear_model))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A document needs to be vectorized through feature engineering into a numeric representation, i.e., a set of **$n$** features characterizing the semantics of the document, $\\{x_1, x_2, ..., x_n\\}$\n",
    "\n",
    "- Then Logistic Regression models the **probability** of the class given these observed values as follows:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + ... + \\beta_ix_i\n",
    "= \\sum_{i}\\beta_ix_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each feature $x_i$ of the document is a function that chracterizes the relevant linguistic properties of the document. These features can be all manually annotated or created automatically.\n",
    "- These features are often in simple forms that are of binary values or numeric values within a range:\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    x_1 &= f_1(x_1) = [Contains(\"好看\") ] = \\{0, 1\\} \\\\\n",
    "    x_2 &= f_2(x_2) = [Contains(\"絕配\") ] = \\{0, 1\\} \\\\\n",
    "    x_3 &= f_3(x_3) = [FreqOf(\"覺得\")] = \\{\\textit{Any Positive Number}\\}\\\\\n",
    "    &...\\\\\n",
    "    \\end{align*}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- But so far the model prediction ($y$) could be any number ($-\\infty$ to $\\infty$) based on the current linear model. \n",
    "- In order to make sure the predicted values of the model are within the range of 0 and 1 (i.e., a probability-like value), the dependent variable $y$ is transformed with a so-called **link function**.\n",
    "- That is, **link functions** transform the predicted $y$ into a range more appropriate for linear modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In particular, in Binary Logistic Regression, the **inverse logit transformation** is used, transforming $y$ from the range of $-\\infty$ to $\\infty$ to into probability values ranging from 0 to 1.\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + ... + \\beta_ix_i\n",
    "= \\sum_{i}\\beta_ix_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\\\\n",
    "P(c|d) &= \\frac{1}{1+e^{-y}}\\\\\n",
    "                    &= \\frac{1}{1+e^{-(\\sum_{i}\\beta_ix_i)}}\\\\\n",
    "                    &= P(c|x_1,x_2,...,x_i)\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- And we can re-formulate the model equation as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\\\\n",
    "P(c|d) &= P(c|x_1,x_2,...,x_i)\\\\\n",
    "&= \\frac{1}{1+e^{-(\\sum_{i}{\\beta_ix_i})}}\n",
    "\\end{align}\n",
    "$$\n",
    "- In model prediction, the model will then determine the **class** of the document by choosing the class that maximizes the conditional probability of $P(c|d)$.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In training:\n",
    "    - Given a document in a training set, $d$, the initial Logistic Regression model will output the predicted conditional probability of $d$ being the class, $c$, i.e., $P(c|d)$.\n",
    "    - This is the **likelihood** of the document. \n",
    "    - And the optimal Logistic Regression is the one that **maximizes the likelihoods** of the documents in the entire training set.\n",
    "    - This **maximum likelihood** approach is similar to the least squares method in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Interpreting the parameters (coefficients $\\beta_i$)\n",
    "\n",
    "    - The coefficient refers to the change of the odds of having the target class label in relation to a specific predictor. \n",
    "    - Odds are defined as: \n",
    "    \n",
    "    $$\n",
    "    odds =\\frac{\\textit{The probability that the event will occur}}{\\textit{The probability that the event will NOT occur}} = \\frac{P_{\\textit{relevant class}}}{1 - P_{\\textit{relevant class}}}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A quick example (based on Stefan Gries' data):\n",
    "    - We use the type of the subordinate clauses (`SUBORDTYPE`) to predict the `ORDER` of the main and subordinate clauses.\n",
    "        - $X: \\{caus, temp\\}$\n",
    "        - $y: \\{mc-sc, sc-mc\\}$\n",
    "    - Unit = sentences; Label = main-subordinate clauses orders.\n",
    "    - We run a logistic regression, using SUBORDTYPE as the predictor and ORDER as the response variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>SUBORDTYPE</th>\n",
       "      <th>LEN_MC</th>\n",
       "      <th>LEN_SC</th>\n",
       "      <th>LENGTH_DIFF</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>MORETHAN2CL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4777</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>temp</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-6</td>\n",
       "      <td>als/when</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1698</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>temp</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>als/when</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>953</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>temp</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>als/when</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1681</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>temp</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>-9</td>\n",
       "      <td>als/when</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4055</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>temp</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>als/when</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1794</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>caus</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>-10</td>\n",
       "      <td>weil/because</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>4095</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>caus</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>weil/because</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>2733</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>caus</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>weil/because</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>350</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>caus</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>-3</td>\n",
       "      <td>weil/because</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>208</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>caus</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>weil/because</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CASE  ORDER SUBORDTYPE  LEN_MC  LEN_SC  LENGTH_DIFF          CONJ  \\\n",
       "0    4777  sc-mc       temp       4      10           -6      als/when   \n",
       "1    1698  mc-sc       temp       7       6            1      als/when   \n",
       "2     953  sc-mc       temp      12       7            5      als/when   \n",
       "3    1681  mc-sc       temp       6      15           -9      als/when   \n",
       "4    4055  sc-mc       temp       9       5            4      als/when   \n",
       "..    ...    ...        ...     ...     ...          ...           ...   \n",
       "398  1794  mc-sc       caus      13      23          -10  weil/because   \n",
       "399  4095  mc-sc       caus      10       7            3  weil/because   \n",
       "400  2733  mc-sc       caus       8       5            3  weil/because   \n",
       "401   350  mc-sc       caus       8      11           -3  weil/because   \n",
       "402   208  sc-mc       caus       9       6            3  weil/because   \n",
       "\n",
       "    MORETHAN2CL  \n",
       "0            no  \n",
       "1            no  \n",
       "2           yes  \n",
       "3            no  \n",
       "4           yes  \n",
       "..          ...  \n",
       "398          no  \n",
       "399         yes  \n",
       "400         yes  \n",
       "401          no  \n",
       "402         yes  \n",
       "\n",
       "[403 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## data from Stefan Gries' \"Statistics for Linguistics with R\"\n",
    "csv= pd.read_table('../../../RepositoryData/data/gries_sflwr/_inputfiles/05-3_clauseorders.csv')\n",
    "csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To explain the model in a more comprehensive way, I like to switch back to R for the statistical outputs. I think they are more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF     CONJ MORETHAN2CL\n",
      "0 4777 sc-mc       temp      4     10          -6 als/when          no\n",
      "1 1698 mc-sc       temp      7      6           1 als/when          no\n",
      "2  953 sc-mc       temp     12      7           5 als/when         yes\n",
      "3 1681 mc-sc       temp      6     15          -9 als/when          no\n",
      "4 4055 sc-mc       temp      9      5           4 als/when         yes\n",
      "5  967 sc-mc       temp      9      5           4 als/when         yes\n",
      "      \n",
      "       mc-sc sc-mc\n",
      "  caus   184    15\n",
      "  temp    91   113\n"
     ]
    }
   ],
   "source": [
    "%%R -i csv\n",
    "\n",
    "# library(readr)\n",
    "# csv= read_delim('../../../RepositoryData/data/gries_sflwr/_inputfiles/05-3_clauseorders.csv','\\t')\n",
    "print(head(csv))\n",
    "print(table(csv$SUBORDTYPE, csv$ORDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "glm(formula = factor(csv$ORDER) ~ factor(csv$SUBORDTYPE), family = \"binomial\")\n",
      "\n",
      "Deviance Residuals: \n",
      "    Min       1Q   Median       3Q      Max  \n",
      "-1.2706  -0.3959  -0.3959   1.0870   2.2739  \n",
      "\n",
      "Coefficients:\n",
      "                           Estimate Std. Error z value Pr(>|z|)    \n",
      "(Intercept)                 -2.5069     0.2685  -9.336   <2e-16 ***\n",
      "factor(csv$SUBORDTYPE)temp   2.7234     0.3032   8.982   <2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "\n",
      "(Dispersion parameter for binomial family taken to be 1)\n",
      "\n",
      "    Null deviance: 503.80  on 402  degrees of freedom\n",
      "Residual deviance: 386.82  on 401  degrees of freedom\n",
      "AIC: 390.82\n",
      "\n",
      "Number of Fisher Scoring iterations: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "lg = glm(factor(csv$ORDER)~factor(csv$SUBORDTYPE), family=\"binomial\")\n",
    "summary(lg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Based on the model parameters, we get the formula for the probability prediction of our response variable:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x_1 = -2.50 + 2.72x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{tip}\n",
    "\n",
    "To interpret the coefficients we need to know the order of the two class labels in the outcome variable. The most straightforward way to do this is to create a table of the outcome variable. \n",
    "\n",
    "As shown above, the second level of `ORDER` is `sc-mc`, this tells us that the coefficients in the Logistic Regression are predicting whether or not the clause order is `sc-mc`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now we can estimate the following probabilities:\n",
    "\n",
    "    - Probability of `sc-mc` when subordinate is `cause`:\n",
    "    \n",
    "    $$\n",
    "    \\begin{align*}\\\\\n",
    "    P(type=scmc|x_1=cause) &= \\frac{1}{1+e^{-y}}\\\\\n",
    "    &=\\frac{1}{1+e^{-({-2.50 + 2.72 \\times x_1})}} \\\\\n",
    "    &=\\frac{1}{1+e^{-({-2.50 + 2.72 \\times 0})}}\\\\\n",
    "    &= 0.0758\n",
    "    \\end{align*}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   - Probability of `sc-mc` when subordinate is `temp`:\n",
    "    \n",
    "    $$\n",
    "    \\begin{align*}\\\\\n",
    "    P(type=scmc|x_1=temp) &= \\frac{1}{1+e^{-y}}\\\\\n",
    "    &=\\frac{1}{1+e^{-({-2.50 + 2.72 \\times x_1})}} \\\\\n",
    "    &=\\frac{1}{1+e^{-({-2.50 + 2.72 \\times 1})}}\\\\\n",
    "    &= 0.5547\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now we can also compute the **odds** of the predicted probability.\n",
    "- The odds of an event is the ratio of:\n",
    "\n",
    "$$\n",
    "\\frac{\\textit{The probability that the event will occur}}{\\textit{The probability that the event will NOT occur}}\n",
    "$$\n",
    "\n",
    "- Simply put, odds are just a ratio of two complementary probability values.\n",
    "\n",
    "$$\n",
    "Odds = \\frac{P(c)}{1-P(c)} = \\frac{P(type=scmc)}{1-P(type=scmc)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now we can compute the two odds:\n",
    "\n",
    "    - The odds of the probabilities when the subordinate is `caus`:\n",
    "\n",
    "    $$\n",
    "    odds_1 = \\frac{0.07}{1-0.07} = 0.08\n",
    "    $$\n",
    "    \n",
    "    - The odds of the probabilities when the subordinate is `temp`:\n",
    "\n",
    "    $$\n",
    "    odds_2 = \\frac{0.55}{1-0.55} = 1.25\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Finally, we can compute the **log odds ratios** (the log of the ratios between two odds):\n",
    "\n",
    "    $$\n",
    "    \\textit{Odds Ratio} = \\frac{odds_2}{odds_1} = \\frac{1.25}{0.08} = 15.18\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\textit{Log Odds Ratio} = log\\left(\\frac{odds_2}{odds_1}\\right) = log\\left(\\frac{1.25}{0.08}\\right) = 2.72\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "y = \\beta_0 + \\beta_1x_1 = -2.50 + 2.72x_1\n",
    "$$\n",
    "\n",
    "- That is the meaning of the coefficient $\\beta_1, 2.72$:\n",
    "  - The **odds** (of having `sc-mc` order) when the subordinate clause is `temp` are $e^{2.7234}= 15.23$ times more than the **odds** when the subordinate clause is `caus`.\n",
    "  - Or the **log odds ratio** increase 2.72 times when the subordinate clause is `temp` as compared to when the subordinate is `caus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob(type=sc-mc|subordtype=cause): 0.07585818 \n",
      "The odds of Prob(type=mc-sc|subordtype=cause) vs. Prob(type=sc-mc|subordtype=cause): 0.082085 \n",
      "\n",
      "Prob(type=sc-mc|subordtype=temp): 0.5547792 \n",
      "The odds of Prob(type=mc-sc|subordtype=temp) vs. Prob(type=sc-mc|subordtype=temp): 1.246077 \n",
      "\n",
      "Odds Ratioos of subordtype=temp vs. subordtype=cause: 15.18032 \n",
      "Log Odds Ratios of subordtype=temp vs. subordtype=cause: 2.72"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "y1= 1/(1+exp(-(-2.50 + 2.72*0))) # prob of sc-mc when SUBORDTYPE = 0 (caus)\n",
    "y2= 1/(1+exp(-(-2.50 + 2.72*1))) # prob of sc-mc when SUBORDTYPE = 1 (temp)\n",
    "odds1 = y1/(1-y1) # odds when SUBORDTYPE = 0\n",
    "odds2 = y2/(1-y2) # odds when SUBORDTYPE = 1\n",
    "\n",
    "cat(\"Prob(type=sc-mc|subordtype=cause):\", y1, \"\\n\")\n",
    "cat(\"The odds of Prob(type=mc-sc|subordtype=cause) vs. Prob(type=sc-mc|subordtype=cause):\", odds1,\"\\n\\n\")\n",
    "\n",
    "cat(\"Prob(type=sc-mc|subordtype=temp):\", y2, \"\\n\")\n",
    "cat(\"The odds of Prob(type=mc-sc|subordtype=temp) vs. Prob(type=sc-mc|subordtype=temp):\", odds2, \"\\n\\n\")\n",
    "\n",
    "odds_ratio = odds2/odds1\n",
    "log_odds_ratio = log(odds_ratio)\n",
    "cat(\"Odds Ratioos of subordtype=temp vs. subordtype=cause:\", odds_ratio, \"\\n\")\n",
    "cat(\"Log Odds Ratios of subordtype=temp vs. subordtype=cause:\", log_odds_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To understand what SVM is, we probably need to introduce the idea of **support vectors**.\n",
    "- Given a linearly separable dataset, it is easy to find a hyperplane (in 2D, that would be a line) that separates data into two distinct sub-groups.\n",
    "    - The shortest distance between the observations and the threshold (hyperplane) is called the **margin**.\n",
    "    - A hyperplane that gives us the largest margin to make classification is referred to as **Maximum Margin Classifier**.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- These threshold cases are the vectors that support the maximum margin classifier, henceforth, known as the \"support vectors\".\n",
    "![](../images/svm/svm.002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Issues with Maximum Margin Classifier:\n",
    "    - Maximum Margin Classifier is very sensitive to outliers.\n",
    "    - The classifier may be too biased toward the class with fewer outliers.\n",
    " \n",
    "![](../images/svm/svm.003.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If we choose a hyperplane which allows misclassifications, we may be able to find a better classifier. \n",
    "    - When we allow misclassifications, the distance between the observations and the hyperplane is called a **soft margin**.\n",
    "    - A classifier allowing misclassifications (i.e., based on soft margins) is referred to as **soft margin classifier**, or **support vector classifier**.\n",
    "    - The observations on the edge and within the **soft margin** are called **support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/svm/svm.004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Support Vector Classifiers can deal with observations with outliers and the assumption is that the observations are linearly separable.\n",
    "- What if the observations are not linearly separable in the first place? This is where **Support Vector Machine** comes in!\n",
    "![](../images/svm.005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Intuition of Support Vector Machine:\n",
    "    - Start with data in a relatively low dimension and seek linear solutions\n",
    "    - Move the data into a higher dimension (when no obvious linear classifier found)\n",
    "    - Find a **Support Vector Classifier** that separates the higher dimensional data into two groups\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/svm-kernel.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The mathematical tricks of moving data into a higher dimension are the **kernel functions**. This process is called the **Kernel Tricks**. \n",
    "- SVM often uses two kernel functions to find support vector classifiers in higher dimensions.\n",
    "    - Polynomial Kernel\n",
    "    - Radial Basis Function Kernel (RBF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- SVM has a **cost function (C)**, which controls the compromise of the **misclassifications** and **margin** sizes.\n",
    "- C is the model penalty for the data points that fall on the wrong side of the hyperplane.\n",
    "    - A smaller C creates a softer (larger) margin, allowing more misclassifications. \n",
    "    - A larger C creates a narrower margin, allowing fewer misclassifications. \n",
    "- The greater the cost parameter, the harder the optimization will try to achieve 100 percent separation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Heuristics for SVM\n",
    "  - There is no reliable rule for matching a kernel to a particular learning task. \n",
    "  - The fit depends heavily on the concept to be learned as well as the amount of training data and the relationships among the features.\n",
    "  - Trial and error is required by training and evaluating several SVMs on a validation dataset (i.e., ***k***-fold cross validation) \n",
    "  - The choice of kernel is often arbitrary because the performances with different kernels may vary only slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many more ways to learn the weights (i.e., $\\beta_i$) in the discriminative models. Different algorithms may prioritize different **link functions** to train the model parameters.\n",
    "\n",
    "In this course, we will not cover all the details of these variants. Please refer to machine learning courses for more details on these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (Maxent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Maxent is very similar to logistic regression, and differs in mainly its way of parameterization (e.g., feature functions, link functions)\n",
    "- In Maxent, each feature function is a function of both the text feature and the class, i.e., $f(x_i, c_i)$.\n",
    "\n",
    "$$\n",
    "f_1(c_i, x_1) \\equiv [C = c_i \\land Contains(\"好看\")]\\\\\n",
    "f_2(c_i, x_2) \\equiv [C = c_i \\land Contains(\"絕配\")]\\\\\n",
    "f_3(c_i, x_3) \\equiv [C = c_1 \\land Ends(w,\"c\")]\\\\\n",
    "...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The model will assign to each feature a **weight**.\n",
    "- Maxent uses a *softmax* function to transform the sum of the weigthed feature values into probabilities. (The exponetial transformation of the sum of the weighted feature values make all predicted values positive.)\n",
    "   - Numerator = The probability of the feature i co-occuring with specific class.\n",
    "   - Denominator = The sum of the probability of the feature co-occurring with the other classes.\n",
    "   \n",
    "$$\n",
    "p(c|d,\\lambda) = \\frac{e^{\\sum_{i}\\lambda_if_i(c,x_i)}}{\\sum_{c'}{e^{\\sum_{i}\\lambda_if_i(c',x_i)}}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are other classification models that are not probabilistic\n",
    "    - **Decision Tree**\n",
    "        - It uses a split condition to predict class labels based on one or multiple input features.\n",
    "        - The classification process starts from the root node of the tree; at each node, the classifier will check which input feature will most effectively split the data into sub-groups. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Emsemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Bagging \n",
    "    - Take subsets of data (e.g., bootstrap samples) and train a model on each subset in parallel\n",
    "    - The subsets are allowed to simultaneously vote on the outcome\n",
    "    - The final outcome is often an average aggregation.\n",
    "    - Example: **Random Forests**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Boosting\n",
    "    - Utilize sequential modeling by improving one model from the errors of the previous model\n",
    "    - Often use the output of one model as an input into the next in a form of sequential processing\n",
    "    - Example: **Gradient boosting machines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- Feature engineering is a lot more important than algorithm selection.\n",
    "- With good feature representations of the data, usually it doesn't really matter which machine learning algorithm we choose.\n",
    "- Generally, **discriminative** models out-perform **generative** models.\n",
    "- Maybe one day the optimal algorithm can be determined *automatically* via machine learning as well!!\n",
    "- Please see [Google's AutoML effort: Google AI Introduces Model Search, An Open Source Platform for Finding Optimal Machine Learning Models](https://www.marktechpost.com/2021/02/28/google-ai-introduces-model-search-an-open-source-platform-for-finding-optimal-machine-learning-ml-models/?fbclid=IwAR3FkqJzXGIqG-X25BXslDZ077F9s4onOaQzlosc9u7qeto9LNd7prt_PNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- NLTK Book, Chapter 6.\n",
    "- Sarkar (2020), Chapter 5.\n",
    "- Geron (2019), Chapter 5-7"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "232px",
    "left": "1305px",
    "top": "80px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}