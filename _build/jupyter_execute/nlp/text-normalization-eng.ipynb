{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The objective of text normalization is to clean up the text by removing unnecessary and irrelevant components. What to include or exclude for the later analysis is highly dependent on the research questions. Before any data preprocessing, you always need to ask yourself whether it would have great impact on the data distribution if you remove the target structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 11:06:32.330270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTML Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- One of the most frequent data source is the Internet. Assuming that we web-crawl text data from a wikipedia page, we need to clean up the HTML codes quite a bit.\n",
    "- Important steps:\n",
    "    - Download and parse the HTML codes of the webpage\n",
    "    - Identify the elements from the page that we are interested in\n",
    "    - Extract the textual contents of the HTML elements\n",
    "    - Remove unnecessary HTML tags\n",
    "    - Remove extra spacing/spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<title>Python (programming language) -'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "content = data.content\n",
    "print(content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General-purpose programming language\n",
      "PythonParadigmMulti-paradigm: object-oriented,[1] procedural (imperative), functional, structured, reflectiveDesigned byGuido van RossumDeveloperPython Software FoundationFirst appeared20 February 1991; 32 years ago (1991-02-20)[2]Stable release3.11.4[3] \n",
      "   / 7 June 2023; 35 days ago (7 June 2023)Preview release3.12.0b4[4] \n",
      "   / 11 July 2023; 1 day ago (11 July 2023)\n",
      "Typing disciplineDuck, dynamic, strong typing;[5] gradual (since 3.5, but ignored in CPython)[6]OSWindows, macOS, Linux/UNIX, Android[7][8] and more[9]LicensePython Software Foundation LicenseFilename extensions.py, .pyi, .pyc, .pyd, .pyw, .pyz (since 3.5),[10] .pyo (prior to 3.5)[11]Websitepython.orgMajor implementationsCPython, PyPy, Stackless Python, MicroPython, CircuitPython, IronPython, JythonDialectsCython, RPython, Starlark[12]Influenced byABC,[13] Ada,[14] ALGOL 68,[15] APL,[16] C,[17] C++,[18] CLU,[19] Dylan,[20] Haskell,[21][16] Icon,[22] Lisp,[23] Modula-3,[15][18] Perl,[2\n"
     ]
    }
   ],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    ## Can include more HTML preprocessing here...\n",
    "    stripped_html_elements = soup.findAll(name='div',attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    stripped_text = ' '.join([h.get_text() for h in stripped_html_elements])\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{tip}\n",
    "The above preprocessing of the wiki page content is less ideal. One can further improve the text processing by targeting at the real text content of the entry.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Stemming is the process where we standardize word forms into their base stem irrespective of their inflections.\n",
    "- The `nltk` provides several popular stemmers for English:\n",
    "    - `nltk.stem.PorterStemmer`\n",
    "    - `nltk.stem.LancasterStemmer`\n",
    "    - `nltk.stem.RegexpStemmer`\n",
    "    - `nltk.stem.SnowballStemmer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can compare the results of different stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "\n",
    "words = ['jumping', 'jumps', 'jumped', 'jumpy']\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "ss = SnowballStemmer('english')\n",
    "\n",
    "rs = RegexpStemmer('ing$|s$|ed$|y$', min=4) # set the minimum of the string to stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jump', 'jump', 'jump', 'jumpi']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Porter Stemmer\n",
    "[ps.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jump', 'jump', 'jump', 'jumpy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lancaster Stemmer\n",
    "[ls.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jump', 'jump', 'jump', 'jumpi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Snowball Stemmer\n",
    "[ss.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jump', 'jump', 'jump', 'jump']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Regular Expression Stemmer\n",
    "[rs.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Lemmatization is similar to stemmatization.\n",
    "- It is a process where we remove word affixes to get the **root word** but not the **root stem**.\n",
    "- These root words, i.e., lemmas, are lexicographically correct words and always present in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{admonition} Question\n",
    ":class: attention\n",
    "In terms of Lemmatization and Stemmatization, which one requires more computational cost? That is, which processing might be slower?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Two frequently-used lemmatizers\n",
    "    - `nltk.stem.WordNetLemmatizer`\n",
    "    - `spacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- WordNetLemmatizer utilizes the dictionary of WordNet.\n",
    "- It requires the **parts of speech** of the word for lemmatization.\n",
    "- I think right now only nouns, verbs and adjectives are important in `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "For Wordnet-based Lemmatizer, it is important to specify the part of speech of the word form.\n",
    "\n",
    "Valid options are `\"n\"` for nouns, `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"` for satellite adjectives.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'men', 'foot']\n"
     ]
    }
   ],
   "source": [
    "# nouns\n",
    "nouns = ['cars','men','feet']\n",
    "nouns_lemma = [wnl.lemmatize(n, 'n') for n in nouns]\n",
    "print(nouns_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'eat', 'grow', 'grow']\n"
     ]
    }
   ],
   "source": [
    "# verbs\n",
    "verbs = ['running', 'ate', 'grew', 'grown']\n",
    "verbs_lemma = [wnl.lemmatize(v, 'v') for v in verbs]\n",
    "print(verbs_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'fancy', 'jumpy', 'good', 'least', 'less', 'bad', 'bad', 'best', 'good']\n"
     ]
    }
   ],
   "source": [
    "# adj\n",
    "adjs = ['saddest', 'fancier', 'jumpy', 'better', 'least', 'less', 'worse', 'worst','best', 'better']\n",
    "adjs_lemma = [wnl.lemmatize(a, 'a') for a in adjs]\n",
    "print(adjs_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{warning}\n",
    "To use `spacy` properly, you need to download/install the language models of the English language first before you load the parameter files. Please see [spacy documentation](https://spacy.io/usage/models/) for installation steps.\n",
    "\n",
    "Also, please remember to install the language models in the right conda environment.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For example, in my Mac:\n",
    "\n",
    "```\n",
    "$ conda activate python-notes\n",
    "$ pip install spacy\n",
    "$ python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parse','entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "text_tagged = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `spacy` processes the text by tokenizing it into tokens and enriching the tokens with many annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My/my/PRON\n",
      "system/system/NOUN\n",
      "keeps/keep/VERB\n",
      "crashing/crash/VERB\n",
      "his/his/PRON\n",
      "crashed/crashed/NOUN\n",
      "yesterday/yesterday/NOUN\n",
      ",/,/PUNCT\n",
      "ours/ours/PRON\n",
      "crashes/crash/VERB\n",
      "daily/daily/ADV\n"
     ]
    }
   ],
   "source": [
    "for t in text_tagged:\n",
    "    print(t.text+'/'+t.lemma_ + '/'+ t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash ! his crashed yesterday , ours crash daily'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For the English data, contractions are problematic sometimes. \n",
    "- These may get even more complicated when different tokenizers deal with contractions differently.\n",
    "- A good way is to expand all contractions into their original independent word forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{note}\n",
    "Please download the `TAWP` directory from the `demo_data`. This directory includes code snippets provided in Sarkar's (2020) book.\n",
    "\n",
    "Also, you need to put this `TAWP` under your working directory for importing.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import TAWP  ## You have to put TAWA directory under your WORKING DIRECTORY\n",
    "from TAWP.contractions import CONTRACTION_MAP\n",
    "import re\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    ## create a regex pattern of all contracted forms\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(\n",
    "        contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)  # the whole matched contraction\n",
    "\n",
    "        # if the matched contraction (=keys) exists in the dict,\n",
    "        # get its corresponding uncontracted form (=values)\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "\n",
    "        return expanded_contraction\n",
    "\n",
    "    # find each contraction in the pattern,\n",
    "    # find it from text,\n",
    "    # and replace it using the output of\n",
    "    # expand_match\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{note}\n",
    "In `re.sub(repl, str)`, when `repl` is a function like above, the function is called for every non-overlapping occurrence of pattern `contractions_pattern`. The function `expand_match` takes a single matched contraction, and returns the replacement string, i.e., its uncontracted form in the dictionary. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you all cannot expand contractions I would think\n",
      "I am very glad he is here! And it is not here!\n"
     ]
    }
   ],
   "source": [
    "print(expand_contractions(\"Y'all can't expand contractions I'd think\"))\n",
    "\n",
    "print(expand_contractions(\"I'm very glad he's here! And it ain't here!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"ain't\", 'is not'),\n",
       " (\"aren't\", 'are not'),\n",
       " (\"can't\", 'cannot'),\n",
       " (\"can't've\", 'cannot have'),\n",
       " (\"'cause\", 'because')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(CONTRACTION_MAP.items())[:5] # check the first five items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accented Characters (Non-ASCII)\n",
    "\n",
    "- The `unicodedata` module handles unicode characters very efficiently. Please check [unicodedata dcoumentation](https://docs.python.org/3/library/unicodedata.html) for more details.\n",
    "- When dealing with the English data, we may often encounter foreign characters in texts that are not part of the ASCII character set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "#     ```\n",
    "#     (NFKD) will apply the compatibility decomposition, i.e. \n",
    "#     replace all compatibility characters with their equivalents. \n",
    "#     ```\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')\n",
    "\n",
    "# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt'))\n",
    "# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt').encode('ascii','ignore'))\n",
    "# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt').encode('ascii','ignore').decode('utf-8', 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{note}\n",
    "- `str.encode()` returns an encoded version of the string as a bytes object using the specified encoding.\n",
    "- `byes.decode()` returns a string decoded from the given bytes using the specified encoding.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Another common scenario is the case where texts include both English and Chinese characters. What's worse, the English characters are in full-width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中英文abc,,。..ABC123\n",
      "中英文abc,,。..ABC123\n",
      "中英文abc,，。.．ＡＢＣ１２３\n",
      "中英文abc,，。.．ＡＢＣ１２３\n"
     ]
    }
   ],
   "source": [
    "## Chinese characters with full-width English letters and punctuations\n",
    "text = '中英文abc,，。.．ＡＢＣ１２３'\n",
    "print(unicodedata.normalize('NFKD', text))\n",
    "print(unicodedata.normalize('NFKC', text))  # recommended method\n",
    "print(unicodedata.normalize('NFC', text))\n",
    "print(unicodedata.normalize('NFD', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Sometimes, we may even want to keep characters of one language only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文ＣＨＩＮＥＳＥ o 2020520 alvingmalcob\n",
      "中文ＣＨＩＮＥＳＥoalvingmalcob\n",
      "中文。！＝=.= ＾＾ 2020/5/20 @.@%&*\n",
      "中文\n"
     ]
    }
   ],
   "source": [
    "text = \"中文ＣＨＩＮＥＳＥ。！＝=.= ＾o＾ 2020/5/20 alvin@gmal.cob@%&*\"\n",
    "\n",
    "# remove puncs/symbols\n",
    "print(''.join(\n",
    "    [c for c in text if unicodedata.category(c)[0] not in [\"P\", \"S\"]]))\n",
    "\n",
    "# select letters\n",
    "print(''.join([c for c in text if unicodedata.category(c)[0] in [\"L\"]]))\n",
    "\n",
    "# remove alphabets\n",
    "print(''.join(\n",
    "    [c for c in text if unicodedata.category(c)[:2] not in [\"Lu\", 'Ll']]))\n",
    "\n",
    "# select Chinese chars?\n",
    "print(''.join([c for c in text if unicodedata.category(c)[:2] in [\"Lo\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{note}\n",
    "Please check [this page](https://www.fileformat.info/info/unicode/category/index.htm) for unicode category names.\n",
    "\n",
    "It seems that the unicode catetory `Lo` is good to identify Chinese characters?\n",
    "\n",
    "We can also make use of the category names to identify punctuations.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{note}\n",
    "[This page](https://www.compart.com/en/unicode/) shows how unicode deals with combining or decomposing character classes. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Depending on the research questions and the defined tasks, we often need to decide whether to remove irrelevant characters.\n",
    "- Common irrelevant (aka. non-informative) characters may include:\n",
    "    - Punctuation marks and symbols\n",
    "    - Digits\n",
    "    - Any other non-alphanumeric characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a simple case Removing 1 or 2 symbols is probably ok\n",
      "This is a simple case Removing  or  symbols is probably ok\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "s = \"This is a simple case! Removing 1 or 2 symbols is probably ok...:)\"\n",
    "print(remove_special_characters(s))\n",
    "print(remove_special_characters(s, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{warning}\n",
    "In the following example, if we use the same `remove_special_characters()` to pre-process the text, what additional problems will we encounter?\n",
    "\n",
    "Any suggestions or better alternative methods?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its a complex sentences and Im not sure if its ok to replace all symbols then  What now\n"
     ]
    }
   ],
   "source": [
    "s = \"It's a complex sentences, and I'm not sure if it's ok to replace all symbols then :( What now!!??)\"\n",
    "print(remove_special_characters(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- At the word-token level, there are words that have little semantic information and are usually removed from text in text preprocessing. These words are often referred to as **stopwords**.\n",
    "- However, there is no universal stopword list. Whether a word is informative or not depends on your research/project objective. It is a linguistic decision.\n",
    "- The `nltk.corpus.stopwords.words()` provides a standard English language stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer() # assuming per line per sentence \n",
    "    # for other Tokenizer, maybe sent.tokenize should go first\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can check the languages of the stopwords lists provided by `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redundant Whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Very often we would see redundant duplicate whitespaces in texts. \n",
    "- Sometimes, when we remove special characters (punctuations, digits etc.), we may replace those characters with whitespaces (not empty string), which may lead to duplicate whitespaces in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def remove_redundant_whitespaces(text):\n",
    "    text = re.sub(r'\\s+',\" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are humans and we often have typos.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"We are humans  and we   often have typos.  \"\n",
    "remove_redundant_whitespaces(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Packing things together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ideally, we can wrap all the relevant steps of text preprocessing in one coherent procedure.\n",
    "\n",
    "- Please study Sarkar's `text_normalizer.py`\n",
    "\n",
    "```\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list):\n",
    "                     \n",
    "                     ## Your codes here\n",
    "    return corpus_normalized\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Sarkar (2020), Chapter 3."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes-2023",
   "language": "python",
   "name": "python-notes-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}