{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Models Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In deep learning NLP, sequence models are often the most widely adopted methods.\n",
    "- In this tutorial, we will go over the intuitions of sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Sequence Models?\n",
    "\n",
    "- Humans process texts in a sequential order (from left to right)\n",
    "- When we process texts and make classification, we utilize our reasoning about previous sequences to inform later decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Recurrent neural network** (RNN) addresses this issue by implementing networks with loops that allow information to persist.\n",
    "- The loop allows information to be passed from previous time step to the next time step.\n",
    "\n",
    "![](../images/s2s-rnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "- Neural network expects an numeric input, i.e., a numeric representation of the input text.\n",
    "- So the first step in deep learning is the same as traditional ML, which is text vectorization.\n",
    "- And because a sequence model eats in one word as the input, word representation/vectorization is crucial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Representations in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Like in traditional machine learning, feature engineering is crucial to the success of the computational modeling.\n",
    "- Of particular importance is the transformation of each text into numeric representation that has a significant portion of **textual semantics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- One-hot encoding is the most intuitive way to represent lexical words numerically.\n",
    "- If the language vocabulary size is *V*, each word can be represented as a vector of size *V*, with its correpsonding dimension to be the value of **1** and the rest being **0**'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/seq-1hot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The main problem with is one-hot encoding is that the semantic distances in-between words are all the same, i.e., $D(mice,rats)=D(mice, horses)= 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now via neural network, we can learn **word embeddings** automatically. (See Word Embeddings notes).\n",
    "- These word embeddings allows us to perform computation of lexical semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/seq-embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.97575491, 0.16937447],\n",
       "       [0.97575491, 1.        , 0.30567806],\n",
       "       [0.16937447, 0.30567806, 1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "mice = np.array([0.2,0.0, 0.8, 0.1, 0.0])\n",
    "rats = np.array([0.2,0.1,0.9, 0.3,0.0])\n",
    "horses = np.array([0.0,0.0,0.1,0.9,0.8])\n",
    "cosine_similarity([mice, rats, horses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- While each dimension of the word embeddings may not have very transparent **semantic fields**, the results out of the semantic computation do reflect a lot the lexical semantic relations between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From One-hot to Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Therefore, we often add **Embedding Layer** as the first layer of a sequence model to render all lexical items in a text into semantically informative embeddings (vectors).\n",
    "- Weights of the Embedding Layer can be trained along with the sequence model.\n",
    "- Or alternatively, we can use pre-trained word embeddings, which were trained in a different task based on a much larger corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/seq-1hot-to-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is how an Embedding Layer works in `keras.layers.Embedding`:\n",
    "\n",
    "\n",
    "![](../images/name-gender-classifier-dl/name-gender-classifier-dl.005.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Network (RNN) Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Words, as semantically represented by embeddings, can now be the input tensors for a RNN.\n",
    "\n",
    "![](../images/s2s-rnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Moreover, we can create a RNN-based language model.\n",
    "- A language model has two main objectives in its NLP applications:\n",
    "    - To estimate the probability of a given sentence (or any other meaningful linguistic units)\n",
    "    - To predict the upcoming word given limited linguistic inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/s2s-rnn-lm.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The RNN Langage model takes one word at each time step as the input and output the predicted next word.\n",
    "- And the output at time step *i* becomes the input of the RNN at time step *i+1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The loss function of the RNN Language Model is the distance between the predicted word, *y*, and the correct next-word (in its one-hot form).\n",
    "- The training of the RNN language model is thus to minimize the sum of the **cross-entropy** at all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/s2s-rnn-lm-loss.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For example, if the target next word is *dog*, it's one-hot representation is `[0, 1, 0, 0 ,0, 0]`, and the RNN LM predicted *y* is `[0.2, 0.4, 0.1, 0.1, 0.2, 0.0]`, we can compute the **cross-entropy** at this time step as follows.\n",
    "\n",
    "$$\n",
    "E = - \\sum_{k}t_k \\log{y_k}\n",
    "$$\n",
    "\n",
    "    - k: refers to the dimensions of the one-hot vectors\n",
    "    - t: refers to the target next word\n",
    "    - y: refers to the predicted y from the RNN LM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9162904818741863"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "\n",
    "t = [0, 1, 0, 0 ,0, 0]\n",
    "y = [0.2, 0.4, 0.1, 0.1, 0.2, 0.0]\n",
    "\n",
    "cross_entropy(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- And we compute the average cross-entropy error across all time steps for a particular sample (i.e., for the entire sequence).\n",
    "\n",
    "$$\n",
    "E = - \\frac{1}{N}\\sum_n\\sum_{k}t_{nk} \\log{y_{nk}}\n",
    "$$\n",
    "\n",
    "    - N: the number of words in the input text\n",
    "- We can also compute the average cross-entropy error for samples of a batch size.\n",
    "- We can also compute the average cross-entropy error for the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back Propogation (skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- With the defined **loss function**, we can learn how good our current model is in the training process (i.e., the distance between the true target and the predicted label).\n",
    "- In deep learning, we can use **back propogation** to find out:\n",
    "    - how each parameter of the RNN LM is connected to the loss function\n",
    "    - or, which parameter of the RNN LM contributes to the change of the loss function more\n",
    "    - And therefore, we can **adjust** the parameters of the RNN LM accordingly.\n",
    "    - The algorithm often used is called **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Gradient Descent (skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As we can compute the cross entropy in different ways, we can perform the gradient descent in different ways as well.\n",
    "\n",
    "- **Batch Gradient Descent**: Update the model weights after we get the average cross entropy of all the sequences in the entire training set (as one epoch).\n",
    "\n",
    "- **Stochastic Gradient Descent**(SGD): Update the model weights after we get the cross entropy of every sequence of the training set (across all time steps of course) (online).\n",
    "\n",
    "- **Mini-batch Gradient Descent**: Update the model weights after we get the average cross entropy of a subset of the sequences in the training set. (Recommended!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From Vanilla RNN to LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Issues with Vanilla RNN\n",
    "\n",
    "- An RNN on a sequence of words can be taken as a very deep neural network, where the depths of the network are the number of time steps of the sequence.\n",
    "- In back propagation, for longer sequences, we would run into the **vanishing gradient problems**. To simply put, the gradients would become smaller and smaller as we back propagate further back to the previous contexts of the sequence.\n",
    "- The further back the weights are, the more likely their gradients would approach zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why is vanishing gradient an issue?\n",
    "\n",
    "If the gradient becomes vanishingly small over longer distances:\n",
    "\n",
    "- it is more difficult for RNN to learn the long-distance dependency relations between different time steps in the sequence.\n",
    "- it is less likely that RNN would learn to preserve information over many timesteps.\n",
    "- it is more likely that RNN would pay more attention to the effects of the recent time steps (i.e., biasing the RNN towards learning from **sequential recency**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM (Long Short-Term Memory)\n",
    "\n",
    "- A type of RNN proposed by [Hochreiter and Schmidhuber in 1997](https://www.bioinf.jku.at/publications/older/2604.pdf) as a solution to the vanishing gradients problem.\n",
    "- For every time step, LSTM keeps track of two states: a **hidden** state and a **cell** state\n",
    "    - Both are of the vector length $n$ same as the node/neuron number of the LSTM.\n",
    "    - The **cell** state stores long-term information in the sequence.\n",
    "    - The LSTM can erase, write and read information from the **cell**.\n",
    "- The selection of which information is erased/written/read is controlled by three corresponding **gates**.\n",
    "    - There are three **gates** in LSTM: **output**, **input**, and **forget** gates.\n",
    "    - The gates are also of the vector length $n$ same as the node/neuron number of the LSTM.\n",
    "    - On each time step, each element of the gates can be open(1),closed(0),or somewhere in-between.\n",
    "    - The gates are dynamic: their value is computed based on the current **cell** state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Vanilla RNN\n",
    "\n",
    "![](../images/chris-olah/LSTM3-SimpleRNN.png)\n",
    "(Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- LSTM\n",
    "\n",
    "![](../images/chris-olah/LSTM3-chain-annotated.jpeg)\n",
    "(Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- LSTM gates:\n",
    "    - **Input** gate controls how much of the input ($X_t$) is used in computing the new **cell** state ($C_t$)\n",
    "    - **Output** gate determines how much of the new **cell** state ($C_t$) is used in the output **hidden** state \n",
    "    - **Forget** gate determines how much of the old **cell** state ($C_{t-1}$) is used in the new **cell** state ($C_t$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/s2s-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GRU (Gated Recurrent Unit)\n",
    "\n",
    "- Proposed by [Cho et al. in 2014](https://arxiv.org/pdf/1406.1078v3.pdf) as a simpler alternative to the LSTM.\n",
    "- On each time step, GRU keeps track of only the **hidden** state (no cell state)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- GRU\n",
    "\n",
    "![](../images/chris-olah/LSTM3-var-GRU.png)\n",
    "(Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/s2s-gru.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Heuristics\n",
    "\n",
    "- LSTM is a good default choice (especially if our data has particularly long dependencies, or we have lots of training data)\n",
    "- We may switch to GRUs for **speed** and fewer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variations of Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are four major variants of sequence models depending on the types of input and output of the RNN\n",
    "    - Many to Many (Same lengths for input and output sequences)\n",
    "        - Most of the tagging problems fall into this category.\n",
    "    - Many to One\n",
    "        - Most of the classification problems fall into this category.\n",
    "    - One to Many\n",
    "        - Image Captioning\n",
    "    - Many to Many (Variable lengths for input and output sequences)\n",
    "        - Machine Translation\n",
    "        - Chatbot Q&A\n",
    "        - Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-----\n",
    "![](../images/seq2seq-m2m-s.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-----\n",
    "![](../images/seq2seq-m21.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-----\n",
    "![](../images/seq2seq-12m.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-----\n",
    "![](../images/seq2seq-m2m-d.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- A must-read article on LSTM: Chris Olah's blog post on [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Check Ch 5 and 6 in [Deep Learning 2｜用Python進行自然語言處理的基礎理論實作](https://www.books.com.tw/products/0010817138)\n",
    "- These lecture notes are based on a talk presented by Ananth Sankar: [Sequence to Sequence Learning with Encoder-Decoder Neural Network Models](https://confengine.com/conferences/odsc-india-2019/proposal/10176/sequence-to-sequence-learning-with-encoder-decoder-neural-network-models). Some of the graphs used here are taken from Dr. Sankar's slides. His talk is highly recommended!\n",
    "- [Sutskever et al. (2014). Sequence to sequence learning with neural networks.](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)\n",
    "- New York Times: [The Great A.I. Awakening](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}