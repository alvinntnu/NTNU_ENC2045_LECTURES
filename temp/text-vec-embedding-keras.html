
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Word Embedding Using Keras &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Statistical Language Model" href="dl-statistical-language-model.html" />
    <link rel="prev" title="Word Embeddings" href="text-vec-embedding.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../nlp/text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-algorithm.html">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-neural-network-from-scratch.html">
   Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-simple-case.html">
   Deep Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sentiment-case.html">
   Deep Learning: Sentiment Analysis
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Embeddings and Language Model
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="text-vec-embedding.html">
   Word Embeddings
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Word Embedding Using Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-statistical-language-model.html">
   Statistical Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sequence-models-intuition.html">
   Sequence Models Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-neural-language-model-primer.html">
   Neural Language Model: A Start
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Seq2Seq, Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-seq-to-seq-types.html">
   Attention: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-seq-to-seq-attention-addition.html">
   Seqeunce Model with Attention for Addition Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-transformers-intuition.html">
   Transformers: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-transformers-keras.html">
   Text Classification with Transformer
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   1. Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   2. Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   3. Assignment III: Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-3-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-3-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   4. Assignment IV: Chinese Language Processing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-4-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-4-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   5. Assignment V: Text Vectorization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-5-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-5-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/temp/text-vec-embedding-keras.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/temp/text-vec-embedding-keras.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/temp/text-vec-embedding-keras.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sample-corpus-of-text-documents">
   Sample corpus of text documents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-text-pre-processing">
   Simple text pre-processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-up-sample-corpus-bible">
   Load up sample corpus - Bible
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementing-a-word2vec-model-using-a-cbow-continuous-bag-of-words-neural-network-architecture">
   Implementing a word2vec model using a CBOW (Continuous Bag of Words) neural network architecture
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-vocabulary">
     Build Vocabulary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-context-words-target-word-pair-generator">
     Build (context_words, target_word) pair generator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-cbow-deep-network-model">
     Build CBOW Deep Network Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-model-for-5-epochs">
     Train model for 5 epochs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-word-embeddings">
     Get word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-a-distance-matrix-to-view-the-most-similar-words-contextually">
     Build a distance matrix to view the most similar words (contextually)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementing-a-word2vec-model-using-a-skip-gram-neural-network-architecture">
   Implementing a word2vec model using a skip-gram neural network architecture
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Build Vocabulary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-and-view-sample-skip-grams-word1-word2-relevancy">
     Build and View sample skip grams ((word1, word2) -&gt; relevancy)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-skip-gram-deep-network-model">
     Build Skip-gram Deep Network Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-the-model-for-5-epochs">
     Train the model for 5 epochs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Get word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Build a distance matrix to view the most similar words (contextually)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualize-word-embeddings">
     Visualize word embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="word-embedding-using-keras">
<h1>Word Embedding Using Keras<a class="headerlink" href="#word-embedding-using-keras" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_colwidth</span> <span class="o">=</span> <span class="mi">200</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Google Colab Adhoc Setting</span>
<span class="o">!</span>nvidia-smi
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">([</span><span class="s1">&#39;gutenberg&#39;</span><span class="p">,</span><span class="s1">&#39;punkt&#39;</span><span class="p">,</span><span class="s1">&#39;stopwords&#39;</span><span class="p">])</span>
<span class="o">!</span>pip show spacy
<span class="c1">#!pip install --upgrade spacy</span>
<span class="c1">#!python -m spacy download en_core_web_trf</span>
<span class="c1">#!python -m spacy download en_core_web_lg</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Thu Mar 11 09:53:31 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   60C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[nltk_data] Downloading package gutenberg to /root/nltk_data...
[nltk_data]   Package gutenberg is already up-to-date!
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Name: spacy
Version: 2.2.4
Summary: Industrial-strength Natural Language Processing (NLP) in Python
Home-page: https://spacy.io
Author: Explosion
Author-email: contact@explosion.ai
License: MIT
Location: /usr/local/lib/python3.7/dist-packages
Requires: cymem, catalogue, tqdm, blis, plac, murmurhash, wasabi, preshed, requests, numpy, thinc, setuptools, srsly
Required-by: fastai, en-core-web-sm
</pre></div>
</div>
</div>
</div>
<div class="section" id="sample-corpus-of-text-documents">
<h2>Sample corpus of text documents<a class="headerlink" href="#sample-corpus-of-text-documents" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;The sky is blue and beautiful.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Love this blue and beautiful sky!&#39;</span><span class="p">,</span>
          <span class="s1">&#39;The quick brown fox jumps over the lazy dog.&#39;</span><span class="p">,</span>
          <span class="s2">&quot;A king&#39;s breakfast has sausages, ham, bacon, eggs, toast and beans&quot;</span><span class="p">,</span>
          <span class="s1">&#39;I love green eggs, ham, sausages and bacon!&#39;</span><span class="p">,</span>
          <span class="s1">&#39;The brown fox is quick and the blue dog is lazy!&#39;</span><span class="p">,</span>
          <span class="s1">&#39;The sky is very blue and the sky is very beautiful today&#39;</span><span class="p">,</span>
          <span class="s1">&#39;The dog is lazy but the brown fox is quick!&#39;</span>    
<span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">,</span> <span class="s1">&#39;weather&#39;</span><span class="p">,</span> <span class="s1">&#39;animals&#39;</span><span class="p">]</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">corpus_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Document&#39;</span><span class="p">:</span> <span class="n">corpus</span><span class="p">,</span> 
                          <span class="s1">&#39;Category&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">})</span>
<span class="n">corpus_df</span> <span class="o">=</span> <span class="n">corpus_df</span><span class="p">[[</span><span class="s1">&#39;Document&#39;</span><span class="p">,</span> <span class="s1">&#39;Category&#39;</span><span class="p">]]</span>
<span class="n">corpus_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Document</th>
      <th>Category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The sky is blue and beautiful.</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Love this blue and beautiful sky!</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>2</th>
      <td>The quick brown fox jumps over the lazy dog.</td>
      <td>animals</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>
      <td>food</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I love green eggs, ham, sausages and bacon!</td>
      <td>food</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The brown fox is quick and the blue dog is lazy!</td>
      <td>animals</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The sky is very blue and the sky is very beautiful today</td>
      <td>weather</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The dog is lazy but the brown fox is quick!</td>
      <td>animals</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="simple-text-pre-processing">
<h2>Simple text pre-processing<a class="headerlink" href="#simple-text-pre-processing" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wpt</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">WordPunctTokenizer</span><span class="p">()</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">normalize_document</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="c1"># lower case and remove special characters\whitespaces</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-zA-Z\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">I</span><span class="o">|</span><span class="n">re</span><span class="o">.</span><span class="n">A</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="c1"># tokenize document</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">wpt</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="c1"># filter stopwords out of document</span>
    <span class="n">filtered_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="c1"># re-create document from filtered tokens</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filtered_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>

<span class="n">normalize_corpus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">normalize_document</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">norm_corpus</span> <span class="o">=</span> <span class="n">normalize_corpus</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">norm_corpus</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;sky blue beautiful&#39;, &#39;love blue beautiful sky&#39;,
       &#39;quick brown fox jumps lazy dog&#39;,
       &#39;kings breakfast sausages ham bacon eggs toast beans&#39;,
       &#39;love green eggs ham sausages bacon&#39;,
       &#39;brown fox quick blue dog lazy&#39;, &#39;sky blue sky beautiful today&#39;,
       &#39;dog lazy brown fox quick&#39;], dtype=&#39;&lt;U51&#39;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load-up-sample-corpus-bible">
<h2>Load up sample corpus - Bible<a class="headerlink" href="#load-up-sample-corpus-bible" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">gutenberg</span>
<span class="kn">from</span> <span class="nn">string</span> <span class="kn">import</span> <span class="n">punctuation</span>

<span class="n">bible</span> <span class="o">=</span> <span class="n">gutenberg</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="s1">&#39;bible-kjv.txt&#39;</span><span class="p">)</span> 
<span class="n">remove_terms</span> <span class="o">=</span> <span class="n">punctuation</span> <span class="o">+</span> <span class="s1">&#39;0123456789&#39;</span>

<span class="n">norm_bible</span> <span class="o">=</span> <span class="p">[[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">remove_terms</span><span class="p">]</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">bible</span><span class="p">]</span>
<span class="n">norm_bible</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tok_sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok_sent</span> <span class="ow">in</span> <span class="n">norm_bible</span><span class="p">]</span>
<span class="n">norm_bible</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize_corpus</span><span class="p">(</span><span class="n">norm_bible</span><span class="p">))</span>
<span class="n">norm_bible</span> <span class="o">=</span> <span class="p">[</span><span class="n">tok_sent</span> <span class="k">for</span> <span class="n">tok_sent</span> <span class="ow">in</span> <span class="n">norm_bible</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tok_sent</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total lines:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bible</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Sample line:&#39;</span><span class="p">,</span> <span class="n">bible</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Processed line:&#39;</span><span class="p">,</span> <span class="n">norm_bible</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total lines: 30103

Sample line: [&#39;1&#39;, &#39;:&#39;, &#39;6&#39;, &#39;And&#39;, &#39;God&#39;, &#39;said&#39;, &#39;,&#39;, &#39;Let&#39;, &#39;there&#39;, &#39;be&#39;, &#39;a&#39;, &#39;firmament&#39;, &#39;in&#39;, &#39;the&#39;, &#39;midst&#39;, &#39;of&#39;, &#39;the&#39;, &#39;waters&#39;, &#39;,&#39;, &#39;and&#39;, &#39;let&#39;, &#39;it&#39;, &#39;divide&#39;, &#39;the&#39;, &#39;waters&#39;, &#39;from&#39;, &#39;the&#39;, &#39;waters&#39;, &#39;.&#39;]

Processed line: god said let firmament midst waters let divide waters waters
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implementing-a-word2vec-model-using-a-cbow-continuous-bag-of-words-neural-network-architecture">
<h2>Implementing a word2vec model using a CBOW (Continuous Bag of Words) neural network architecture<a class="headerlink" href="#implementing-a-word2vec-model-using-a-cbow-continuous-bag-of-words-neural-network-architecture" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/text-tokenization-embedding.gif" /></p>
<p><img alt="" src="../_images/word2vec-text-to-sequences.gif" /></p>
<div class="section" id="build-vocabulary">
<h3>Build Vocabulary<a class="headerlink" href="#build-vocabulary" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">text</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">sequence</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">norm_bible</span><span class="p">)</span>
<span class="n">word2id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>

<span class="n">word2id</span><span class="p">[</span><span class="s1">&#39;PAD&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">id2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="c1"># wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]</span>
<span class="n">wids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">norm_bible</span><span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2id</span><span class="p">)</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Size:&#39;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Sample:&#39;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">word2id</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary Size: 12425
Vocabulary Sample: [(&#39;shall&#39;, 1), (&#39;unto&#39;, 2), (&#39;lord&#39;, 3), (&#39;thou&#39;, 4), (&#39;thy&#39;, 5), (&#39;god&#39;, 6), (&#39;ye&#39;, 7), (&#39;said&#39;, 8), (&#39;thee&#39;, 9), (&#39;upon&#39;, 10)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="build-context-words-target-word-pair-generator">
<h3>Build (context_words, target_word) pair generator<a class="headerlink" href="#build-context-words-target-word-pair-generator" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_context_word_pairs</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="n">context_length</span> <span class="o">=</span> <span class="n">window_size</span><span class="o">*</span><span class="mi">2</span>
    <span class="c1"># go through each unit in the corpus </span>
    <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="n">sentence_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        <span class="c1">## extract context-word pairs</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
            <span class="n">context_words</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">label_word</span>   <span class="o">=</span> <span class="p">[]</span>            
            <span class="n">start</span> <span class="o">=</span> <span class="n">index</span> <span class="o">-</span> <span class="n">window_size</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span>
            
            <span class="n">context_words</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
                                 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> 
                                 <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">sentence_length</span> 
                                 <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">index</span><span class="p">])</span>
            <span class="n">label_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">context_length</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">label_word</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">generate_context_word_pairs</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">wids</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">):</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Context (X):&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">id2word</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="s1">&#39;-&gt; Target (Y):&#39;</span><span class="p">,</span> <span class="n">id2word</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
    
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Context (X): [&#39;old&#39;, &#39;testament&#39;, &#39;james&#39;, &#39;bible&#39;] -&gt; Target (Y): king
Context (X): [&#39;first&#39;, &#39;book&#39;, &#39;called&#39;, &#39;genesis&#39;] -&gt; Target (Y): moses
Context (X): [&#39;beginning&#39;, &#39;god&#39;, &#39;heaven&#39;, &#39;earth&#39;] -&gt; Target (Y): created
Context (X): [&#39;earth&#39;, &#39;without&#39;, &#39;void&#39;, &#39;darkness&#39;] -&gt; Target (Y): form
Context (X): [&#39;without&#39;, &#39;form&#39;, &#39;darkness&#39;, &#39;upon&#39;] -&gt; Target (Y): void
Context (X): [&#39;form&#39;, &#39;void&#39;, &#39;upon&#39;, &#39;face&#39;] -&gt; Target (Y): darkness
Context (X): [&#39;void&#39;, &#39;darkness&#39;, &#39;face&#39;, &#39;deep&#39;] -&gt; Target (Y): upon
Context (X): [&#39;spirit&#39;, &#39;god&#39;, &#39;upon&#39;, &#39;face&#39;] -&gt; Target (Y): moved
Context (X): [&#39;god&#39;, &#39;moved&#39;, &#39;face&#39;, &#39;waters&#39;] -&gt; Target (Y): upon
Context (X): [&#39;god&#39;, &#39;said&#39;, &#39;light&#39;, &#39;light&#39;] -&gt; Target (Y): let
Context (X): [&#39;god&#39;, &#39;saw&#39;, &#39;good&#39;, &#39;god&#39;] -&gt; Target (Y): light
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="build-cbow-deep-network-model">
<h3>Build CBOW Deep Network Model<a class="headerlink" href="#build-cbow-deep-network-model" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/word2vec-cbow.gif" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Lambda</span>

<span class="n">cbow</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">cbow</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">window_size</span><span class="o">*</span><span class="mi">2</span><span class="p">))</span>
<span class="n">cbow</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,)))</span>
<span class="n">cbow</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">cbow</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cbow</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 4, 100)            1242500   
_________________________________________________________________
lambda (Lambda)              (None, 100)               0         
_________________________________________________________________
dense (Dense)                (None, 12425)             1254925   
=================================================================
Total params: 2,497,425
Trainable params: 2,497,425
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">cbow</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/text-vec-embedding-keras_22_0.png" src="../_images/text-vec-embedding-keras_22_0.png" />
</div>
</div>
</div>
<div class="section" id="train-model-for-5-epochs">
<h3>Train model for 5 epochs<a class="headerlink" href="#train-model-for-5-epochs" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">generate_context_word_pairs</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">wids</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">cbow</span><span class="o">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Processed </span><span class="si">{}</span><span class="s1"> (context, word) pairs&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch:&#39;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Loss:&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processed 100000 (context, word) pairs
Processed 200000 (context, word) pairs
Processed 300000 (context, word) pairs
Epoch: 1 	Loss: 4277719.899074742

Processed 100000 (context, word) pairs
Processed 200000 (context, word) pairs
Processed 300000 (context, word) pairs
Epoch: 2 	Loss: 5539841.925134436

Processed 100000 (context, word) pairs
Processed 200000 (context, word) pairs
Processed 300000 (context, word) pairs
Epoch: 3 	Loss: 5825754.4951127

Processed 100000 (context, word) pairs
Processed 200000 (context, word) pairs
Processed 300000 (context, word) pairs
Epoch: 4 	Loss: 5513488.361097297

Processed 100000 (context, word) pairs
Processed 200000 (context, word) pairs
Processed 300000 (context, word) pairs
Epoch: 5 	Loss: 4947481.834239027

CPU times: user 4h 11min 20s, sys: 22min 32s, total: 4h 33min 52s
Wall time: 3h 41min 2s
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="get-word-embeddings">
<h3>Get word embeddings<a class="headerlink" href="#get-word-embeddings" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">cbow</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">id2word</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12424, 100)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>...</th>
      <th>60</th>
      <th>61</th>
      <th>62</th>
      <th>63</th>
      <th>64</th>
      <th>65</th>
      <th>66</th>
      <th>67</th>
      <th>68</th>
      <th>69</th>
      <th>70</th>
      <th>71</th>
      <th>72</th>
      <th>73</th>
      <th>74</th>
      <th>75</th>
      <th>76</th>
      <th>77</th>
      <th>78</th>
      <th>79</th>
      <th>80</th>
      <th>81</th>
      <th>82</th>
      <th>83</th>
      <th>84</th>
      <th>85</th>
      <th>86</th>
      <th>87</th>
      <th>88</th>
      <th>89</th>
      <th>90</th>
      <th>91</th>
      <th>92</th>
      <th>93</th>
      <th>94</th>
      <th>95</th>
      <th>96</th>
      <th>97</th>
      <th>98</th>
      <th>99</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>unto</th>
      <td>2.493178</td>
      <td>2.280065</td>
      <td>-2.507939</td>
      <td>2.784425</td>
      <td>2.006123</td>
      <td>2.579235</td>
      <td>-2.849923</td>
      <td>2.374675</td>
      <td>2.379350</td>
      <td>2.764584</td>
      <td>-2.106564</td>
      <td>2.382879</td>
      <td>-2.914682</td>
      <td>-2.583828</td>
      <td>2.959634</td>
      <td>2.622565</td>
      <td>-2.466287</td>
      <td>-2.794176</td>
      <td>-2.622673</td>
      <td>2.524034</td>
      <td>1.591839</td>
      <td>2.579571</td>
      <td>2.417117</td>
      <td>-2.163602</td>
      <td>2.794503</td>
      <td>2.502496</td>
      <td>-2.224811</td>
      <td>2.355314</td>
      <td>2.530450</td>
      <td>-2.663880</td>
      <td>3.096247</td>
      <td>2.273305</td>
      <td>2.601277</td>
      <td>2.577739</td>
      <td>2.660791</td>
      <td>2.132764</td>
      <td>-2.210376</td>
      <td>2.771165</td>
      <td>-2.546790</td>
      <td>2.334303</td>
      <td>...</td>
      <td>-2.860587</td>
      <td>2.945470</td>
      <td>2.765243</td>
      <td>-2.629065</td>
      <td>-2.362266</td>
      <td>-2.944373</td>
      <td>-2.517269</td>
      <td>2.669332</td>
      <td>2.519333</td>
      <td>2.746240</td>
      <td>2.500954</td>
      <td>-2.480806</td>
      <td>-2.742043</td>
      <td>2.722529</td>
      <td>2.869028</td>
      <td>-2.695514</td>
      <td>2.439688</td>
      <td>2.394013</td>
      <td>2.290642</td>
      <td>3.030485</td>
      <td>1.894302</td>
      <td>2.278019</td>
      <td>-2.496936</td>
      <td>-2.347033</td>
      <td>2.240549</td>
      <td>-2.622205</td>
      <td>2.696797</td>
      <td>-3.005467</td>
      <td>-2.500668</td>
      <td>2.900511</td>
      <td>2.633258</td>
      <td>2.670748</td>
      <td>-2.447536</td>
      <td>2.422442</td>
      <td>2.593393</td>
      <td>1.753121</td>
      <td>2.608407</td>
      <td>-2.430226</td>
      <td>2.335080</td>
      <td>-2.505792</td>
    </tr>
    <tr>
      <th>lord</th>
      <td>2.237636</td>
      <td>1.920727</td>
      <td>-2.005896</td>
      <td>2.395445</td>
      <td>1.500550</td>
      <td>2.143541</td>
      <td>-2.076647</td>
      <td>2.127654</td>
      <td>2.074989</td>
      <td>1.840716</td>
      <td>-2.196392</td>
      <td>2.013716</td>
      <td>-2.227787</td>
      <td>-2.011690</td>
      <td>2.512754</td>
      <td>2.074346</td>
      <td>-1.994763</td>
      <td>-2.062951</td>
      <td>-2.112778</td>
      <td>2.267396</td>
      <td>1.563455</td>
      <td>1.983125</td>
      <td>2.062977</td>
      <td>-1.970129</td>
      <td>1.645113</td>
      <td>2.587614</td>
      <td>-2.049644</td>
      <td>2.144177</td>
      <td>1.973272</td>
      <td>-2.115422</td>
      <td>2.073282</td>
      <td>2.104995</td>
      <td>1.939169</td>
      <td>2.134913</td>
      <td>2.189713</td>
      <td>2.205002</td>
      <td>-1.674901</td>
      <td>2.079604</td>
      <td>-2.612578</td>
      <td>1.696025</td>
      <td>...</td>
      <td>-2.253058</td>
      <td>2.557268</td>
      <td>2.488352</td>
      <td>-1.716908</td>
      <td>-2.283367</td>
      <td>-1.699466</td>
      <td>-2.066367</td>
      <td>2.286157</td>
      <td>2.445107</td>
      <td>1.971202</td>
      <td>1.687662</td>
      <td>-2.219192</td>
      <td>-2.281904</td>
      <td>2.097086</td>
      <td>2.332855</td>
      <td>-2.203106</td>
      <td>2.295070</td>
      <td>1.904054</td>
      <td>2.069417</td>
      <td>2.250185</td>
      <td>1.373652</td>
      <td>1.837680</td>
      <td>-1.860649</td>
      <td>-1.820732</td>
      <td>1.938252</td>
      <td>-1.923746</td>
      <td>2.405630</td>
      <td>-2.344059</td>
      <td>-1.837343</td>
      <td>2.585225</td>
      <td>2.478138</td>
      <td>2.077151</td>
      <td>-2.026983</td>
      <td>2.146083</td>
      <td>2.061859</td>
      <td>1.643556</td>
      <td>2.195096</td>
      <td>-1.747171</td>
      <td>1.937860</td>
      <td>-2.189663</td>
    </tr>
    <tr>
      <th>thou</th>
      <td>1.852034</td>
      <td>1.608304</td>
      <td>-1.915088</td>
      <td>2.129377</td>
      <td>1.333169</td>
      <td>2.108295</td>
      <td>-2.040878</td>
      <td>1.774977</td>
      <td>1.841824</td>
      <td>1.885826</td>
      <td>-1.522025</td>
      <td>2.160235</td>
      <td>-2.075351</td>
      <td>-2.213708</td>
      <td>2.388963</td>
      <td>1.665148</td>
      <td>-1.550717</td>
      <td>-1.533588</td>
      <td>-2.147773</td>
      <td>2.215923</td>
      <td>1.473649</td>
      <td>2.037037</td>
      <td>2.247571</td>
      <td>-1.738790</td>
      <td>2.029315</td>
      <td>1.945095</td>
      <td>-1.702343</td>
      <td>2.062509</td>
      <td>2.078270</td>
      <td>-2.007159</td>
      <td>2.226472</td>
      <td>1.921827</td>
      <td>1.626323</td>
      <td>1.864058</td>
      <td>2.077960</td>
      <td>2.003502</td>
      <td>-1.675634</td>
      <td>1.847600</td>
      <td>-1.864253</td>
      <td>1.811313</td>
      <td>...</td>
      <td>-2.175801</td>
      <td>1.748633</td>
      <td>2.516579</td>
      <td>-1.621244</td>
      <td>-2.471605</td>
      <td>-1.754455</td>
      <td>-2.023512</td>
      <td>1.833258</td>
      <td>2.172236</td>
      <td>1.827036</td>
      <td>1.609966</td>
      <td>-1.997724</td>
      <td>-2.191856</td>
      <td>1.798254</td>
      <td>2.252249</td>
      <td>-2.006888</td>
      <td>2.209270</td>
      <td>1.905152</td>
      <td>1.799428</td>
      <td>2.023958</td>
      <td>1.435888</td>
      <td>1.952867</td>
      <td>-2.001946</td>
      <td>-1.460272</td>
      <td>1.910215</td>
      <td>-1.952791</td>
      <td>2.003366</td>
      <td>-2.298617</td>
      <td>-1.700005</td>
      <td>2.020771</td>
      <td>2.276090</td>
      <td>1.801864</td>
      <td>-2.064566</td>
      <td>1.771908</td>
      <td>1.822864</td>
      <td>1.432321</td>
      <td>1.536148</td>
      <td>-1.821058</td>
      <td>2.106123</td>
      <td>-2.161884</td>
    </tr>
    <tr>
      <th>thy</th>
      <td>2.531327</td>
      <td>2.025131</td>
      <td>-2.041817</td>
      <td>2.218701</td>
      <td>1.789895</td>
      <td>2.136183</td>
      <td>-2.185168</td>
      <td>2.030367</td>
      <td>2.466120</td>
      <td>2.127433</td>
      <td>-1.583739</td>
      <td>1.597496</td>
      <td>-2.756582</td>
      <td>-2.362903</td>
      <td>2.764671</td>
      <td>2.397500</td>
      <td>-1.974481</td>
      <td>-2.190022</td>
      <td>-2.444972</td>
      <td>2.530881</td>
      <td>1.579834</td>
      <td>2.377707</td>
      <td>2.391319</td>
      <td>-2.078337</td>
      <td>2.714062</td>
      <td>2.153903</td>
      <td>-2.091292</td>
      <td>1.877012</td>
      <td>2.532566</td>
      <td>-2.382282</td>
      <td>2.860591</td>
      <td>2.033803</td>
      <td>1.934266</td>
      <td>2.025002</td>
      <td>2.743492</td>
      <td>2.118318</td>
      <td>-1.832843</td>
      <td>2.321450</td>
      <td>-2.591767</td>
      <td>1.957423</td>
      <td>...</td>
      <td>-2.645058</td>
      <td>2.261010</td>
      <td>2.772206</td>
      <td>-2.138278</td>
      <td>-2.247943</td>
      <td>-2.777539</td>
      <td>-2.508750</td>
      <td>2.500719</td>
      <td>2.317946</td>
      <td>2.624919</td>
      <td>2.312386</td>
      <td>-2.521078</td>
      <td>-2.345290</td>
      <td>2.389005</td>
      <td>2.709480</td>
      <td>-2.333166</td>
      <td>2.431406</td>
      <td>2.359063</td>
      <td>2.120807</td>
      <td>2.260307</td>
      <td>1.859112</td>
      <td>2.350191</td>
      <td>-2.309731</td>
      <td>-2.134687</td>
      <td>2.032670</td>
      <td>-2.485084</td>
      <td>2.549423</td>
      <td>-2.468534</td>
      <td>-2.203936</td>
      <td>2.339925</td>
      <td>2.163095</td>
      <td>2.343931</td>
      <td>-2.544316</td>
      <td>2.022240</td>
      <td>2.183741</td>
      <td>1.703355</td>
      <td>2.328748</td>
      <td>-2.440206</td>
      <td>1.934747</td>
      <td>-2.370390</td>
    </tr>
    <tr>
      <th>god</th>
      <td>2.123617</td>
      <td>2.473450</td>
      <td>-2.375682</td>
      <td>2.494669</td>
      <td>1.866841</td>
      <td>1.800023</td>
      <td>-2.114289</td>
      <td>2.143275</td>
      <td>2.013142</td>
      <td>2.055153</td>
      <td>-2.077431</td>
      <td>2.238841</td>
      <td>-2.655066</td>
      <td>-2.193072</td>
      <td>2.406302</td>
      <td>2.133641</td>
      <td>-1.826285</td>
      <td>-2.165294</td>
      <td>-2.263628</td>
      <td>2.472098</td>
      <td>1.602052</td>
      <td>2.229868</td>
      <td>2.025896</td>
      <td>-2.102528</td>
      <td>2.093714</td>
      <td>2.298667</td>
      <td>-2.200569</td>
      <td>1.815781</td>
      <td>2.166676</td>
      <td>-2.464071</td>
      <td>2.685121</td>
      <td>1.923397</td>
      <td>2.278466</td>
      <td>1.930879</td>
      <td>2.454482</td>
      <td>2.107688</td>
      <td>-2.122718</td>
      <td>2.122227</td>
      <td>-2.026627</td>
      <td>2.453970</td>
      <td>...</td>
      <td>-2.367663</td>
      <td>2.196561</td>
      <td>2.341169</td>
      <td>-2.038842</td>
      <td>-2.001653</td>
      <td>-2.226016</td>
      <td>-1.711143</td>
      <td>1.863317</td>
      <td>1.959173</td>
      <td>2.127438</td>
      <td>1.927445</td>
      <td>-2.562122</td>
      <td>-2.437272</td>
      <td>2.149189</td>
      <td>2.321251</td>
      <td>-2.520710</td>
      <td>2.555057</td>
      <td>2.183687</td>
      <td>1.724928</td>
      <td>2.322994</td>
      <td>1.547610</td>
      <td>2.080300</td>
      <td>-2.134106</td>
      <td>-2.047226</td>
      <td>2.072769</td>
      <td>-2.247808</td>
      <td>2.015329</td>
      <td>-2.278941</td>
      <td>-2.261706</td>
      <td>2.498932</td>
      <td>2.687076</td>
      <td>1.856894</td>
      <td>-1.947362</td>
      <td>1.629364</td>
      <td>2.204880</td>
      <td>1.967651</td>
      <td>2.241149</td>
      <td>-2.265152</td>
      <td>2.188684</td>
      <td>-2.091099</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 100 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="build-a-distance-matrix-to-view-the-most-similar-words-contextually">
<h3>Build a distance matrix to view the most similar words (contextually)<a class="headerlink" href="#build-a-distance-matrix-to-view-the-most-similar-words-contextually" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>

<span class="c1"># compute pairwise distance matrix</span>
<span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">distance_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># view contextually similar words</span>
<span class="n">similar_words</span> <span class="o">=</span> <span class="p">{</span><span class="n">search_term</span><span class="p">:</span> <span class="p">[</span><span class="n">id2word</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">distance_matrix</span><span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">search_term</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> 
                   <span class="k">for</span> <span class="n">search_term</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;god&#39;</span><span class="p">,</span> <span class="s1">&#39;jesus&#39;</span><span class="p">,</span> <span class="s1">&#39;noah&#39;</span><span class="p">,</span> <span class="s1">&#39;egypt&#39;</span><span class="p">,</span> <span class="s1">&#39;john&#39;</span><span class="p">,</span> <span class="s1">&#39;gospel&#39;</span><span class="p">,</span> <span class="s1">&#39;moses&#39;</span><span class="p">,</span><span class="s1">&#39;famine&#39;</span><span class="p">]}</span>

<span class="n">similar_words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12424, 12424)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;egypt&#39;: [&#39;night&#39;, &#39;smote&#39;, &#39;strong&#39;, &#39;judgment&#39;, &#39;could&#39;],
 &#39;famine&#39;: [&#39;grave&#39;, &#39;fields&#39;, &#39;flee&#39;, &#39;consumed&#39;, &#39;daily&#39;],
 &#39;god&#39;: [&#39;may&#39;, &#39;also&#39;, &#39;lord&#39;, &#39;ye&#39;, &#39;great&#39;],
 &#39;gospel&#39;: [&#39;gentiles&#39;, &#39;hearts&#39;, &#39;sound&#39;, &#39;entered&#39;, &#39;saints&#39;],
 &#39;jesus&#39;: [&#39;heaven&#39;, &#39;sea&#39;, &#39;dead&#39;, &#39;spirit&#39;, &#39;round&#39;],
 &#39;john&#39;: [&#39;disciples&#39;, &#39;peter&#39;, &#39;entered&#39;, &#39;new&#39;, &#39;must&#39;],
 &#39;moses&#39;: [&#39;lay&#39;, &#39;nothing&#39;, &#39;kept&#39;, &#39;sins&#39;, &#39;knew&#39;],
 &#39;noah&#39;: [&#39;uncleanness&#39;, &#39;used&#39;, &#39;tarried&#39;, &#39;birds&#39;, &#39;willing&#39;]}
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="implementing-a-word2vec-model-using-a-skip-gram-neural-network-architecture">
<h2>Implementing a word2vec model using a skip-gram neural network architecture<a class="headerlink" href="#implementing-a-word2vec-model-using-a-skip-gram-neural-network-architecture" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Build Vocabulary<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">text</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">norm_bible</span><span class="p">)</span>

<span class="n">word2id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="n">id2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2id</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> 
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]</span>

<span class="n">wids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">norm_bible</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Size:&#39;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Sample:&#39;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">word2id</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary Size: 12425
Vocabulary Sample: [(&#39;shall&#39;, 1), (&#39;unto&#39;, 2), (&#39;lord&#39;, 3), (&#39;thou&#39;, 4), (&#39;thy&#39;, 5), (&#39;god&#39;, 6), (&#39;ye&#39;, 7), (&#39;said&#39;, 8), (&#39;thee&#39;, 9), (&#39;upon&#39;, 10)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="build-and-view-sample-skip-grams-word1-word2-relevancy">
<h3>Build and View sample skip grams ((word1, word2) -&gt; relevancy)<a class="headerlink" href="#build-and-view-sample-skip-grams-word1-word2-relevancy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">skipgrams</span>

<span class="c1"># generate skip-grams</span>
<span class="n">skip_grams</span> <span class="o">=</span> <span class="p">[</span><span class="n">skipgrams</span><span class="p">(</span><span class="n">wid</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">wid</span> <span class="ow">in</span> <span class="n">wids</span><span class="p">]</span>

<span class="c1"># view sample skip-grams</span>
<span class="n">pairs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">skip_grams</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_grams</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;(</span><span class="si">{:s}</span><span class="s2"> (</span><span class="si">{:d}</span><span class="s2">), </span><span class="si">{:s}</span><span class="s2"> (</span><span class="si">{:d}</span><span class="s2">)) -&gt; </span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">id2word</span><span class="p">[</span><span class="n">pairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">pairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
          <span class="n">id2word</span><span class="p">[</span><span class="n">pairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span> <span class="n">pairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
          <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(james (1154), lovely (5395)) -&gt; 0
(bible (5766), openest (7782)) -&gt; 0
(james (1154), bible (5766)) -&gt; 1
(bible (5766), king (13)) -&gt; 1
(king (13), zechariah (1353)) -&gt; 0
(james (1154), king (13)) -&gt; 1
(bible (5766), withal (1531)) -&gt; 0
(king (13), james (1154)) -&gt; 1
(james (1154), gopher (8490)) -&gt; 0
(king (13), pigeons (3296)) -&gt; 0
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="build-skip-gram-deep-network-model">
<h3>Build Skip-gram Deep Network Model<a class="headerlink" href="#build-skip-gram-deep-network-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dot</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Reshape</span>
<span class="kn">from</span> <span class="nn">keras.layers.embeddings</span> <span class="kn">import</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">word_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">word_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span>
                         <span class="n">embeddings_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span>
                         <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">word_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">((</span><span class="n">embed_size</span><span class="p">,</span> <span class="p">)))</span>

<span class="n">context_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">context_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span>
                  <span class="n">embeddings_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span>
                  <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">context_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">((</span><span class="n">embed_size</span><span class="p">,)))</span>

<span class="n">model_arch</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)([</span><span class="n">word_model</span><span class="o">.</span><span class="n">output</span><span class="p">,</span> <span class="n">context_model</span><span class="o">.</span><span class="n">output</span><span class="p">])</span>
<span class="n">model_arch</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">model_arch</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">word_model</span><span class="o">.</span><span class="n">input</span><span class="p">,</span><span class="n">context_model</span><span class="o">.</span><span class="n">input</span><span class="p">],</span> <span class="n">model_arch</span><span class="p">)</span>
<span class="c1">#model.add(Merge([word_model, context_model], mode=&quot;dot&quot;))</span>
<span class="c1">#model.add(Dense(1, kernel_initializer=&quot;glorot_uniform&quot;, activation=&quot;sigmoid&quot;))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
embedding_1_input (InputLayer)  [(None, 1)]          0                                            
__________________________________________________________________________________________________
embedding_2_input (InputLayer)  [(None, 1)]          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 1, 100)       1242500     embedding_1_input[0][0]          
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 1, 100)       1242500     embedding_2_input[0][0]          
__________________________________________________________________________________________________
reshape (Reshape)               (None, 100)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 100)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
dot (Dot)                       (None, 1)            0           reshape[0][0]                    
                                                                 reshape_1[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            2           dot[0][0]                        
==================================================================================================
Total params: 2,485,002
Trainable params: 2,485,002
Non-trainable params: 0
__________________________________________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/text-vec-embedding-keras_36_0.png" src="../_images/text-vec-embedding-keras_36_0.png" />
</div>
</div>
</div>
<div class="section" id="train-the-model-for-5-epochs">
<h3>Train the model for 5 epochs<a class="headerlink" href="#train-the-model-for-5-epochs" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elem</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skip_grams</span><span class="p">):</span>
        <span class="n">pair_first_elem</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">elem</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
        <span class="n">pair_second_elem</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">elem</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">elem</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair_first_elem</span><span class="p">,</span> <span class="n">pair_second_elem</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Processed </span><span class="si">{}</span><span class="s1"> (skip_first, skip_second, relevance) pairs&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>  

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch:&#39;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;Loss:&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processed 0 (skip_first, skip_second, relevance) pairs
Processed 10000 (skip_first, skip_second, relevance) pairs
Processed 20000 (skip_first, skip_second, relevance) pairs
Epoch: 1 Loss: 4378.33015979873
Processed 0 (skip_first, skip_second, relevance) pairs
Processed 10000 (skip_first, skip_second, relevance) pairs
Processed 20000 (skip_first, skip_second, relevance) pairs
Epoch: 2 Loss: 3713.269076963654
Processed 0 (skip_first, skip_second, relevance) pairs
Processed 10000 (skip_first, skip_second, relevance) pairs
Processed 20000 (skip_first, skip_second, relevance) pairs
Epoch: 3 Loss: 3700.8700029423344
Processed 0 (skip_first, skip_second, relevance) pairs
Processed 10000 (skip_first, skip_second, relevance) pairs
Processed 20000 (skip_first, skip_second, relevance) pairs
Epoch: 4 Loss: 3675.715855676419
Processed 0 (skip_first, skip_second, relevance) pairs
Processed 10000 (skip_first, skip_second, relevance) pairs
Processed 20000 (skip_first, skip_second, relevance) pairs
Epoch: 5 Loss: 3578.9555171455722
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id2">
<h3>Get word embeddings<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_embed_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">word_embed_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">id2word</span><span class="o">.</span><span class="n">values</span><span class="p">())</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12424, 100)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>...</th>
      <th>60</th>
      <th>61</th>
      <th>62</th>
      <th>63</th>
      <th>64</th>
      <th>65</th>
      <th>66</th>
      <th>67</th>
      <th>68</th>
      <th>69</th>
      <th>70</th>
      <th>71</th>
      <th>72</th>
      <th>73</th>
      <th>74</th>
      <th>75</th>
      <th>76</th>
      <th>77</th>
      <th>78</th>
      <th>79</th>
      <th>80</th>
      <th>81</th>
      <th>82</th>
      <th>83</th>
      <th>84</th>
      <th>85</th>
      <th>86</th>
      <th>87</th>
      <th>88</th>
      <th>89</th>
      <th>90</th>
      <th>91</th>
      <th>92</th>
      <th>93</th>
      <th>94</th>
      <th>95</th>
      <th>96</th>
      <th>97</th>
      <th>98</th>
      <th>99</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>shall</th>
      <td>-0.230191</td>
      <td>-0.012980</td>
      <td>-0.119776</td>
      <td>-0.214139</td>
      <td>-0.139326</td>
      <td>-0.005782</td>
      <td>-0.000499</td>
      <td>-0.012986</td>
      <td>-0.129972</td>
      <td>-0.134485</td>
      <td>0.073344</td>
      <td>-0.004424</td>
      <td>0.051141</td>
      <td>-0.086042</td>
      <td>-0.023183</td>
      <td>-0.073094</td>
      <td>0.003325</td>
      <td>-0.095509</td>
      <td>0.117723</td>
      <td>0.087508</td>
      <td>0.194871</td>
      <td>0.067518</td>
      <td>0.146130</td>
      <td>-0.152153</td>
      <td>-0.008002</td>
      <td>-0.070856</td>
      <td>-0.360937</td>
      <td>0.031479</td>
      <td>0.002801</td>
      <td>0.059270</td>
      <td>0.023433</td>
      <td>0.040273</td>
      <td>-0.098525</td>
      <td>0.056835</td>
      <td>0.097193</td>
      <td>0.036708</td>
      <td>-0.040750</td>
      <td>0.063268</td>
      <td>0.029116</td>
      <td>0.100384</td>
      <td>...</td>
      <td>0.059290</td>
      <td>-0.082546</td>
      <td>0.073031</td>
      <td>0.011758</td>
      <td>0.024530</td>
      <td>0.104860</td>
      <td>-0.058176</td>
      <td>-0.100977</td>
      <td>-0.046878</td>
      <td>-0.028711</td>
      <td>-0.011928</td>
      <td>0.043426</td>
      <td>-0.063566</td>
      <td>0.027842</td>
      <td>-0.073853</td>
      <td>0.037065</td>
      <td>0.106886</td>
      <td>-0.235693</td>
      <td>0.068956</td>
      <td>0.041608</td>
      <td>0.090529</td>
      <td>-0.073773</td>
      <td>-0.191838</td>
      <td>-0.024634</td>
      <td>0.113254</td>
      <td>-0.018184</td>
      <td>0.062602</td>
      <td>0.004019</td>
      <td>0.001023</td>
      <td>0.080665</td>
      <td>-0.014163</td>
      <td>0.079130</td>
      <td>-0.033400</td>
      <td>0.013658</td>
      <td>-0.113731</td>
      <td>-0.074399</td>
      <td>-0.005767</td>
      <td>0.324398</td>
      <td>-0.168895</td>
      <td>-0.038513</td>
    </tr>
    <tr>
      <th>unto</th>
      <td>0.033917</td>
      <td>-0.033548</td>
      <td>0.002460</td>
      <td>0.051957</td>
      <td>-0.037578</td>
      <td>-0.058962</td>
      <td>0.045944</td>
      <td>-0.033817</td>
      <td>-0.167547</td>
      <td>-0.029642</td>
      <td>0.052118</td>
      <td>0.034987</td>
      <td>-0.019723</td>
      <td>-0.248761</td>
      <td>-0.027972</td>
      <td>-0.130536</td>
      <td>-0.017513</td>
      <td>-0.080016</td>
      <td>0.169364</td>
      <td>0.050531</td>
      <td>0.089165</td>
      <td>0.094314</td>
      <td>0.022516</td>
      <td>-0.026207</td>
      <td>0.070197</td>
      <td>-0.037644</td>
      <td>-0.465807</td>
      <td>0.003047</td>
      <td>-0.035397</td>
      <td>-0.020680</td>
      <td>-0.050190</td>
      <td>-0.044418</td>
      <td>-0.055332</td>
      <td>0.163345</td>
      <td>0.122113</td>
      <td>-0.019833</td>
      <td>-0.055745</td>
      <td>0.068905</td>
      <td>0.019674</td>
      <td>0.133357</td>
      <td>...</td>
      <td>0.080152</td>
      <td>-0.039098</td>
      <td>0.125916</td>
      <td>0.018961</td>
      <td>-0.052088</td>
      <td>0.046908</td>
      <td>-0.234005</td>
      <td>-0.029314</td>
      <td>-0.042654</td>
      <td>-0.043847</td>
      <td>-0.185238</td>
      <td>0.047657</td>
      <td>0.011895</td>
      <td>0.030421</td>
      <td>-0.212477</td>
      <td>-0.100483</td>
      <td>0.076776</td>
      <td>-0.284910</td>
      <td>-0.001194</td>
      <td>0.152445</td>
      <td>-0.023135</td>
      <td>-0.073265</td>
      <td>-0.036882</td>
      <td>0.054470</td>
      <td>0.123800</td>
      <td>0.007911</td>
      <td>0.130739</td>
      <td>-0.011884</td>
      <td>0.183925</td>
      <td>0.087923</td>
      <td>-0.153560</td>
      <td>-0.007753</td>
      <td>-0.069279</td>
      <td>0.063225</td>
      <td>-0.002554</td>
      <td>0.023410</td>
      <td>-0.052896</td>
      <td>0.167541</td>
      <td>-0.144698</td>
      <td>0.059140</td>
    </tr>
    <tr>
      <th>lord</th>
      <td>-0.095424</td>
      <td>-0.007718</td>
      <td>-0.093306</td>
      <td>-0.064070</td>
      <td>-0.167174</td>
      <td>-0.115682</td>
      <td>0.010716</td>
      <td>-0.037788</td>
      <td>-0.056763</td>
      <td>-0.035718</td>
      <td>0.134224</td>
      <td>-0.057177</td>
      <td>0.092480</td>
      <td>-0.105964</td>
      <td>-0.060721</td>
      <td>-0.068378</td>
      <td>-0.016322</td>
      <td>0.001394</td>
      <td>0.070993</td>
      <td>0.082885</td>
      <td>0.186197</td>
      <td>0.079952</td>
      <td>0.059199</td>
      <td>0.026122</td>
      <td>0.055251</td>
      <td>-0.068530</td>
      <td>-0.180406</td>
      <td>0.068527</td>
      <td>0.080998</td>
      <td>0.014038</td>
      <td>-0.079342</td>
      <td>0.041935</td>
      <td>-0.266223</td>
      <td>0.016576</td>
      <td>0.172890</td>
      <td>-0.072076</td>
      <td>-0.151022</td>
      <td>0.124902</td>
      <td>-0.050274</td>
      <td>-0.029396</td>
      <td>...</td>
      <td>-0.011206</td>
      <td>0.012265</td>
      <td>0.008988</td>
      <td>0.002559</td>
      <td>0.063905</td>
      <td>0.138360</td>
      <td>-0.215831</td>
      <td>0.045561</td>
      <td>-0.178735</td>
      <td>-0.049412</td>
      <td>-0.087286</td>
      <td>0.129855</td>
      <td>-0.022569</td>
      <td>-0.022720</td>
      <td>-0.092201</td>
      <td>0.029446</td>
      <td>0.011717</td>
      <td>-0.180359</td>
      <td>-0.032985</td>
      <td>0.106314</td>
      <td>0.087757</td>
      <td>-0.013804</td>
      <td>-0.125697</td>
      <td>-0.022454</td>
      <td>0.018215</td>
      <td>-0.016422</td>
      <td>0.073527</td>
      <td>-0.048009</td>
      <td>0.087053</td>
      <td>0.053596</td>
      <td>-0.220106</td>
      <td>-0.048350</td>
      <td>-0.115681</td>
      <td>0.027153</td>
      <td>-0.065599</td>
      <td>-0.040915</td>
      <td>-0.000288</td>
      <td>-0.003145</td>
      <td>-0.221902</td>
      <td>-0.052453</td>
    </tr>
    <tr>
      <th>thou</th>
      <td>-0.015430</td>
      <td>-0.015494</td>
      <td>-0.014965</td>
      <td>0.118186</td>
      <td>-0.042467</td>
      <td>0.015224</td>
      <td>-0.168478</td>
      <td>-0.025783</td>
      <td>0.049552</td>
      <td>-0.047795</td>
      <td>0.035379</td>
      <td>-0.002506</td>
      <td>0.052654</td>
      <td>-0.178429</td>
      <td>0.019827</td>
      <td>-0.140220</td>
      <td>-0.013930</td>
      <td>-0.063914</td>
      <td>-0.041961</td>
      <td>0.072887</td>
      <td>0.044044</td>
      <td>0.088513</td>
      <td>0.086183</td>
      <td>-0.076763</td>
      <td>0.003079</td>
      <td>-0.137509</td>
      <td>-0.500076</td>
      <td>-0.006353</td>
      <td>0.052944</td>
      <td>0.096665</td>
      <td>0.015243</td>
      <td>-0.038306</td>
      <td>-0.051829</td>
      <td>0.136486</td>
      <td>0.196201</td>
      <td>0.053081</td>
      <td>0.032940</td>
      <td>0.024769</td>
      <td>-0.084245</td>
      <td>-0.067825</td>
      <td>...</td>
      <td>-0.009351</td>
      <td>-0.064741</td>
      <td>0.072620</td>
      <td>-0.061677</td>
      <td>0.212665</td>
      <td>0.085871</td>
      <td>-0.114312</td>
      <td>-0.070660</td>
      <td>-0.064450</td>
      <td>-0.055936</td>
      <td>-0.012863</td>
      <td>0.109043</td>
      <td>0.283692</td>
      <td>0.067448</td>
      <td>-0.028274</td>
      <td>-0.015870</td>
      <td>0.034884</td>
      <td>-0.298618</td>
      <td>0.035293</td>
      <td>0.004853</td>
      <td>-0.079439</td>
      <td>0.004860</td>
      <td>0.021883</td>
      <td>-0.012389</td>
      <td>0.056918</td>
      <td>-0.066689</td>
      <td>0.041185</td>
      <td>-0.006304</td>
      <td>0.085801</td>
      <td>0.338103</td>
      <td>-0.231865</td>
      <td>-0.004770</td>
      <td>-0.046060</td>
      <td>0.008816</td>
      <td>-0.352311</td>
      <td>-0.081346</td>
      <td>0.007086</td>
      <td>0.007176</td>
      <td>-0.020751</td>
      <td>-0.022667</td>
    </tr>
    <tr>
      <th>thy</th>
      <td>-0.052541</td>
      <td>0.025602</td>
      <td>-0.057033</td>
      <td>0.021996</td>
      <td>-0.014574</td>
      <td>-0.163738</td>
      <td>-0.062247</td>
      <td>0.026953</td>
      <td>-0.088437</td>
      <td>-0.097307</td>
      <td>0.117441</td>
      <td>-0.024271</td>
      <td>0.048699</td>
      <td>-0.124647</td>
      <td>-0.212219</td>
      <td>-0.012106</td>
      <td>-0.024889</td>
      <td>-0.026840</td>
      <td>-0.007780</td>
      <td>-0.039188</td>
      <td>0.072413</td>
      <td>-0.013055</td>
      <td>0.069651</td>
      <td>-0.018947</td>
      <td>0.078843</td>
      <td>0.001588</td>
      <td>-0.338669</td>
      <td>0.083456</td>
      <td>-0.056253</td>
      <td>0.006093</td>
      <td>0.001902</td>
      <td>0.138836</td>
      <td>-0.113098</td>
      <td>0.050578</td>
      <td>0.096934</td>
      <td>-0.017088</td>
      <td>-0.255093</td>
      <td>0.013012</td>
      <td>-0.106204</td>
      <td>0.179721</td>
      <td>...</td>
      <td>0.066951</td>
      <td>-0.049823</td>
      <td>0.048732</td>
      <td>-0.045002</td>
      <td>-0.034193</td>
      <td>0.058281</td>
      <td>-0.230789</td>
      <td>0.098568</td>
      <td>-0.016107</td>
      <td>-0.121213</td>
      <td>-0.088523</td>
      <td>0.074341</td>
      <td>0.131518</td>
      <td>0.005541</td>
      <td>-0.176916</td>
      <td>0.162957</td>
      <td>-0.030527</td>
      <td>-0.178511</td>
      <td>0.079122</td>
      <td>0.028123</td>
      <td>-0.042759</td>
      <td>-0.151469</td>
      <td>-0.067745</td>
      <td>0.060250</td>
      <td>0.121428</td>
      <td>-0.056656</td>
      <td>0.027064</td>
      <td>0.009636</td>
      <td>0.035933</td>
      <td>0.171641</td>
      <td>-0.161448</td>
      <td>-0.029322</td>
      <td>-0.063280</td>
      <td>-0.009890</td>
      <td>-0.275462</td>
      <td>-0.040079</td>
      <td>0.198287</td>
      <td>0.132119</td>
      <td>0.116443</td>
      <td>-0.036876</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 100 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="id3">
<h3>Build a distance matrix to view the most similar words (contextually)<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>

<span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">distance_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">similar_words</span> <span class="o">=</span> <span class="p">{</span><span class="n">search_term</span><span class="p">:</span> <span class="p">[</span><span class="n">id2word</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">distance_matrix</span><span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">search_term</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> 
                   <span class="k">for</span> <span class="n">search_term</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;god&#39;</span><span class="p">,</span> <span class="s1">&#39;jesus&#39;</span><span class="p">,</span> <span class="s1">&#39;noah&#39;</span><span class="p">,</span> <span class="s1">&#39;egypt&#39;</span><span class="p">,</span> <span class="s1">&#39;john&#39;</span><span class="p">,</span> <span class="s1">&#39;gospel&#39;</span><span class="p">,</span> <span class="s1">&#39;moses&#39;</span><span class="p">,</span><span class="s1">&#39;famine&#39;</span><span class="p">]}</span>

<span class="n">similar_words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12424, 12424)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;egypt&#39;: [&#39;whither&#39;, &#39;fight&#39;, &#39;spoil&#39;, &#39;left&#39;, &#39;possess&#39;],
 &#39;famine&#39;: [&#39;pestilence&#39;, &#39;drive&#39;, &#39;former&#39;, &#39;increase&#39;, &#39;countries&#39;],
 &#39;god&#39;: [&#39;lord&#39;, &#39;us&#39;, &#39;nothing&#39;, &#39;spirit&#39;, &#39;might&#39;],
 &#39;gospel&#39;: [&#39;church&#39;, &#39;preached&#39;, &#39;preach&#39;, &#39;pilate&#39;, &#39;always&#39;],
 &#39;jesus&#39;: [&#39;christ&#39;, &#39;disciples&#39;, &#39;faith&#39;, &#39;peter&#39;, &#39;scribes&#39;],
 &#39;john&#39;: [&#39;galilee&#39;, &#39;peter&#39;, &#39;scribes&#39;, &#39;preached&#39;, &#39;asked&#39;],
 &#39;moses&#39;: [&#39;aaron&#39;, &#39;congregation&#39;, &#39;commanded&#39;, &#39;minister&#39;, &#39;command&#39;],
 &#39;noah&#39;: [&#39;shem&#39;, &#39;methuselah&#39;, &#39;ham&#39;, &#39;contentment&#39;, &#39;haran&#39;]}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-word-embeddings">
<h3>Visualize word embeddings<a class="headerlink" href="#visualize-word-embeddings" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="n">words</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">similar_words</span><span class="o">.</span><span class="n">items</span><span class="p">()],</span> <span class="p">[])</span>
<span class="n">words_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
<span class="n">word_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">weights</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">words_ids</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total words:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Word Embedding shapes:&#39;</span><span class="p">,</span> <span class="n">word_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">words</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total words: 48 	Word Embedding shapes: (48, 100)
</pre></div>
</div>
<img alt="../_images/text-vec-embedding-keras_44_1.png" src="../_images/text-vec-embedding-keras_44_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Sarkar (2020) Ch 4 Feature Engineering for Text Representation</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./temp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="text-vec-embedding.html" title="previous page">Word Embeddings</a>
    <a class='right-next' id="next-link" href="dl-statistical-language-model.html" title="next page">Statistical Language Model</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>