
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Attention and Transformers: Intuitions &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Sequence Model with Attention for Addition Learning" href="dl-seq-to-seq-attention-addition.html" />
    <link rel="prev" title="3. Word Embeddings" href="../nlp/text-vec-embedding.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../nlp/text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-algorithm.html">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-neural-network-from-scratch.html">
   1. Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-simple-case.html">
   2. Deep Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-sentiment-case.html">
   3. Deep Learning: Sentiment Analysis
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neural Language Model and Embeddings
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-sequence-models-intuition.html">
   1. Sequence Models Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-neural-language-model-primer.html">
   2. Neural Language Model: A Start
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/text-vec-embedding.html">
   3. Word Embeddings
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Seq2Seq, Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Attention and Transformers: Intuitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-seq-to-seq-attention-addition.html">
   2. Sequence Model with Attention for Addition Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-transformers-keras.html">
   3. Sentiment Classification with Transformer (Self-Study)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sentiment-analysis-using-bert-keras-movie-reviews.html">
   4. Transfer Learning With BERT (Self-Study)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   Assignment III: Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   Assignment IV: Chinese Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   Assignment V: Text Vectorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/6-machine-learning.html">
   Assignment VI: Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/7-topic-modeling.html">
   Assignment VII: Topic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/8-dl-chinese-name-gender.html">
   Assignment VIII: Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/9-sentiment-analysis-dl.html">
   Assignment IX: Sentiment Analysis Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/10-neural-language-model.html">
   Assignment X: Neural Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/11-word2vec.html">
   Assignment XI: Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/12-encoder-decoder.html">
   Assignment XII: Encoder-Decoder Sequence Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/13-attention.html">
   Assignment XIII: Attention
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/temp/dl-attention-transformer-intuition.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/temp/dl-attention-transformer-intuition.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/temp/dl-attention-transformer-intuition.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sequence-to-sequence-models">
   1.1. Sequence-to-Sequence Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vanilla-encoder-decoder-model">
   1.2. Vanilla Encoder-Decoder Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#teacher-forcing">
   1.3. Teacher Forcing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#peeky-encoder-decoder-model">
   1.4. Peeky Encoder-Decoder Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-based-encoder-decoder-model">
   1.5. Attention-based Encoder-Decoder Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-intuition">
     Attention Intuition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-weights">
     Attention Weights
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-layer-in-tensorflow-keras">
   1.6. Attention Layer in
   <code class="docutils literal notranslate">
    <span class="pre">
     tensorflow.keras
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#self-attention">
   1.7. Self-Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-self-attention-to-transformers">
   1.8. From Self-Attention to Transformers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#token-positions">
   1.9. Token Positions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-transformers-to-classifiers">
   1.10. From Transformers to Classifiers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transfer-learning">
   1.11. Transfer Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning">
     Machine Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning-intuition">
     Transfer Learning Intuition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-transfer-learning-in-nlp">
     Examples of Transfer Learning in NLP
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#famous-transformers-based-models">
   1.12. Famous Transformers-based Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bert-bi-directional-encoder-representations-from-transformers">
     BERT (Bi-directional Encoder Representations from Transformers)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#english-examples">
       English Examples
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#chinese-examples">
       Chinese examples
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpt-2-generative-pre-training-2">
     GPT-2 (Generative Pre-Training 2)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       English Examples
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#chinese">
       Chinese
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prospect">
   1.13. Prospect
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   1.14. References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="attention-and-transformers-intuitions">
<h1><span class="section-number">1. </span>Attention and Transformers: Intuitions<a class="headerlink" href="#attention-and-transformers-intuitions" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ## Run this notebook on Google Coalb</span>
<span class="c1"># ## Google Colab Setting</span>
<span class="c1"># !pip install -U transformers</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The state-of-the-art NLP features the use of <strong>Attention</strong> or its sophisticated application, <strong>Transformers</strong>.</p></li>
<li><p>The <strong>Attention Mechanism</strong> can be seen as an important architecture in deep learning (sequence models in particular) that allows the model to learn things from the <strong>co-occurring contexts</strong> of words.</p></li>
<li><p>Most importantly, this mechanism enables the network to effectively learn the <strong>long distance dependency</strong> relations in languages, which have long been a difficult task in traditional statistical NLP.</p></li>
<li><p>In this unit, we will provide an intuitive understanding of the Attention Mechanism and its extended application, Transformers, in deep learning.</p></li>
</ul>
<div class="section" id="sequence-to-sequence-models">
<h2><span class="section-number">1.1. </span>Sequence-to-Sequence Models<a class="headerlink" href="#sequence-to-sequence-models" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The Attention Mechanism shows its most effective power in Sequence-to-Sequence models, esp. when both the input and output sequences are of <strong>variable</strong> lengths.</p></li>
<li><p>A typical application of Sequence-to-Sequence model is <strong>machine translation</strong>.</p></li>
<li><p>This type of model is also referred to as <strong>Encoder-Decoder</strong> models, where Encoder and Decoder are two independent RNN’s trained at the same time.</p></li>
<li><p>In this unit, we will use Machine Translation as the example for illustration.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-vanilla-rnn.jpeg" /></p>
</div>
<div class="section" id="vanilla-encoder-decoder-model">
<h2><span class="section-number">1.2. </span>Vanilla Encoder-Decoder Model<a class="headerlink" href="#vanilla-encoder-decoder-model" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/seq2seq-enc-dec-1.gif" /></p>
<ul class="simple">
<li><p>In Vanilla Encoder-Decoder model, the Encoder processes one input token at a time and produces one hidden state (h) at each time step.</p></li>
<li><p>Each hidden state is passed to the next time step, along with the next input token (i.e., “recurrent”).</p></li>
<li><p>At the last time step of the Encoder, it passes the hidden state of the <strong>last</strong> time step to the Decoder.</p></li>
<li><p>Then the Decoder takes in the last hidden state from the Encoder and produces one hidden state at a time.</p></li>
<li><p>Each Decoder’s hidden state is passed to Dense Layer to get the output token.</p></li>
<li><p>The Decoder passes both the hidden state and the output token to the next time step and produces another hidden state and output token (i.e., “recurrent”).</p></li>
<li><p>The Decoder stops decoding when the stopping condition is reached.</p></li>
</ul>
</div>
<div class="section" id="teacher-forcing">
<h2><span class="section-number">1.3. </span>Teacher Forcing<a class="headerlink" href="#teacher-forcing" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Both Encoder and Decoder is an RNN cell, which takes two inputs at each time step: the <strong>input token vector</strong> of the current time step and the <strong>hidden state</strong> from the previous time step.</p></li>
<li><p>The training and testing for Decoder deserves more attention.</p></li>
</ul>
<ul class="simple">
<li><p>During the <strong>training</strong> stage, the Decoder takes the previous return state <span class="math notranslate nohighlight">\(h_{t-1}\)</span> and the current correct <span class="math notranslate nohighlight">\(y_t\)</span> as the input vector. This is referred to as <strong>teacher forcing</strong>.</p></li>
<li><p>During the <strong>testing</strong> stage, the Decoder would have to decode the output one at a time, taking the previous hidden state <span class="math notranslate nohighlight">\(h_{t-1}\)</span> and the previous predicted output vector <span class="math notranslate nohighlight">\(\hat{y}_{t-1}\)</span> as its inputs. That is, no <strong>teacher-forcing</strong> during the testing stage.
<img alt="" src="../_images/seq2seq-vanilla-rnn-teacher-forcing.jpeg" /></p></li>
</ul>
</div>
<div class="section" id="peeky-encoder-decoder-model">
<h2><span class="section-number">1.4. </span>Peeky Encoder-Decoder Model<a class="headerlink" href="#peeky-encoder-decoder-model" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/seq2seq-peeky.jpeg" /></p>
<ul class="simple">
<li><p>In the vanilla Encoder-Decoder model, Decoder can only access the <strong>last hidden state</strong> from Encoder.</p></li>
<li><p>A variant of the seq-to-seq model is to make available Encoder’s last hidden state <span class="math notranslate nohighlight">\(h_{t}\)</span> to Decoder at every decoding time step.</p></li>
<li><p>An intuitive understanding of this <strong>peeky</strong> approach is that during the decoding stage (i.e., translation), the <strong>contexts</strong> from the source input sequence are always available to all decoding steps.</p></li>
</ul>
</div>
<div class="section" id="attention-based-encoder-decoder-model">
<h2><span class="section-number">1.5. </span>Attention-based Encoder-Decoder Model<a class="headerlink" href="#attention-based-encoder-decoder-model" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Compared to Peeky Encoder-Decoder Model, the Attention-based Encoder-Decoder Model goes one step further by allowing Decoder to access not only Encoder’s last hidden state, but Encoder’s <strong>hidden states at all time steps</strong>.</p></li>
<li><p>This is where the Attention mechanism comes in.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-enc-dec-2.gif" /></p>
<ul class="simple">
<li><p>Attention Mechanism can be seen as a much more sophisticated design of the Peeky approach.</p></li>
<li><p>The key is how Decoder makes use of Encoder’s hidden states.</p></li>
<li><p>The Attention Mechanism takes care of this important step.</p></li>
</ul>
<div class="section" id="attention-intuition">
<h3>Attention Intuition<a class="headerlink" href="#attention-intuition" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>When Decoder is translating/decoding, we would expect Decoder to focus on the parts of the input that are relevant to this decoding time step.</p></li>
<li><p>That is, when decoding the translation of <span class="math notranslate nohighlight">\(\hat{Y}_{t}\)</span>, it is very likely that its translation is more relevant to some of the input words and less relevant to the others.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-enc-dec-attn.gif" /></p>
<ul class="simple">
<li><p>With the Attention Mechanism, Decoder is capable of accessing the hidden states (<span class="math notranslate nohighlight">\([h_1, h_2, ...,h_t]\)</span>) from all the time steps of the Encoder.</p></li>
<li><p>We only need to decide which hidden state <strong><span class="math notranslate nohighlight">\(h_t\)</span></strong> is more relevant to the decoding step (i.e., we need some <strong>attention weights</strong>).</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-enc-dec-attn.gif" /></p>
<ul class="simple">
<li><p>The Attention Mechanism does just that:</p>
<ul>
<li><p>First the Attention Layer determines the “relevance” of each Encoder’s hidden state <strong><span class="math notranslate nohighlight">\(h_t\)</span></strong> to the Decoder’s previous hidden state.</p></li>
<li><p>Second, the Attention Layer transforms all the hidden states of the Encoder into a <strong>Context Vector</strong> by taking the weighted sum of all the Encoder’s hidden states.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/seq2seq-enc-dec-attn.gif" /></p>
<ul class="simple">
<li><p>Finally, to decode the next output token, we can utilize both the Decoder’s hidden state and the Context vector for next-word translation.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-attention-weights.jpeg" /></p>
</div>
<div class="section" id="attention-weights">
<h3>Attention Weights<a class="headerlink" href="#attention-weights" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>There are many proposals regarding how to compute the attention weights.</p></li>
<li><p>In the current Tensorflow implementation, there are three types of <a class="reference external" href="https://keras.io/api/layers/attention_layers/">Attention layers</a>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Attention</span></code> Layer: Luong’s style attention (i.e., simple dot-product) <a class="reference external" href="https://arxiv.org/pdf/1508.4025.pdf">Luong et al., 2015</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AdditiveAttention</span></code> Layer: Bahdanau’s style attention <a class="reference external" href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau et al., 2015</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code> Layer: transformer’s style attention <a class="reference external" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">“Attention is All you Need” (Vaswani, et al., 2017)</a></p></li>
</ul>
</li>
<li><p>The <strong>Attention</strong> layer then will transform all Encoder’s hidden states into one <strong>Context Vector</strong>, indicating how relevant the decoding step is to all the Encoder’s hidden states respectively.</p></li>
<li><p>In short, the Context Vector is a weighted sum of the Encoder’s hidden states, using the <strong>Attention Weights</strong>.</p></li>
</ul>
</div>
</div>
<div class="section" id="attention-layer-in-tensorflow-keras">
<h2><span class="section-number">1.6. </span>Attention Layer in <code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code><a class="headerlink" href="#attention-layer-in-tensorflow-keras" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> has implemented three types of Attention layers:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Attention</span></code>: Dot-product attention layer, a.k.a. Luong-style attention.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AdditiveAttention</span></code>: Additive attention layer, a.k.a. Bahdanau-style attention.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code>: Multi-head attention.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>When using the <code class="docutils literal notranslate"><span class="pre">Attention</span></code> layer, we need to specify which tensor (<strong>query</strong>) is attending to which tensor (<strong>key</strong>):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">query</span></code> tensor: The tensor which is compared to every other vector to establish the weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">value</span></code> tensor: The tensor which is used to compute the weighted sum of the Attention output, i.e., the Context Vector.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">key</span></code> tensor: By default, it is the same as <code class="docutils literal notranslate"><span class="pre">value</span></code> tensor.</p></li>
</ul>
</li>
<li><p>In our Decoder-Encoder Model, the <strong>query</strong> tensor is the Decoder’s hidden state at the decoding time step; the <strong>key</strong> and <strong>value</strong> tensors are Encoder’s hidden states (at all time steps).</p></li>
<li><p>The Attention layer returns a <strong>Context Vector</strong>, whose shape is the same as the <strong>query</strong> tensor.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In Self-Attention layers, the Query is all the input vectors, and the Key is also the input vectors.</p>
</div>
</div>
<div class="section" id="self-attention">
<h2><span class="section-number">1.7. </span>Self-Attention<a class="headerlink" href="#self-attention" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Self-Attention is an extended application of the <strong>Attention Mechansim</strong>.</p></li>
<li><p>Given an input sequence, [<span class="math notranslate nohighlight">\(x_1, x_2, ..., x_t\)</span>], we can also check how each token is connected to each other, i.e., the pair-wise similarity in-between tokens.</p></li>
<li><p>This is the idea of <strong>Self-Attention</strong>.</p></li>
</ul>
<ul class="simple">
<li><p>Self-attention operation is fundamental to the state-of-the-art NLP models.</p></li>
<li><p>It is a simple <strong>sequence-to-sequence</strong> (same lengths) operation: a sequence of vectors (<strong>input vectors</strong>) goes in, and a sequence of vectors comes out.</p></li>
<li><p>The self-attention operation builds upon the assumption that among all the input vectors, some are more connected to each other (e.g., syntactic/semantic long-distance dependency in “<em>The cat walks on the street</em>”).</p></li>
<li><p>Therefore, when the Self-Attention layer transforms an input vector, it may give more weights to those input vectors that are more similar to this input vector.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-self-atten1.gif" /></p>
<ul class="simple">
<li><p>How do we know which input vectors are more similar or more connected to each other? The simplest way is to compute the <strong>dot-product</strong> of the two vectors (i.e., similar to Cosine Similarity).</p></li>
<li><p>Therefore, in Self-Attention, each input vector (<strong>Query</strong>) is compared to all the other input vectors (<strong>Keys</strong>) to get the weights or similarity measures.</p></li>
<li><p>And each output vector is a weighted sum over all the input vectors, weighted by the similarity measures (the pairwise dot-products of the input vectors).</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-self-atten1.gif" /></p>
<ul class="simple">
<li><p>For instance, in the following example, the word <span class="math notranslate nohighlight">\(walks\)</span> may be more relevant to <em>who</em> is doing the walking (i.e., <span class="math notranslate nohighlight">\(cats\)</span>), or, <em>where</em> the agent is walking (i.e, <span class="math notranslate nohighlight">\(street\)</span>), and less relevant to grammatical words like <span class="math notranslate nohighlight">\(the\)</span>.</p></li>
<li><p>Therefore, an effective Self-Attention layer should transform the output vector of <span class="math notranslate nohighlight">\(walks\)</span> (i.e., the weighted sum) by assigning higher weights on these relevant tokens (as indicated by the widths of the arrows) and lower weights on those irrelevant tokens.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-self-atten1.gif" /></p>
<ul class="simple">
<li><p>Simply put, the Self-Attention layer transforms each input vector into the output vector by taking into consideration how each input vector (<strong>query</strong>) is connected to the rest of the input vectors (<strong>keys</strong> and <strong>values</strong>).</p></li>
<li><p>Each transformed vector in the Self-Attention output is a weighted sum of all the input vectors.</p></li>
</ul>
</div>
<div class="section" id="from-self-attention-to-transformers">
<h2><span class="section-number">1.8. </span>From Self-Attention to Transformers<a class="headerlink" href="#from-self-attention-to-transformers" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Now we see <strong>Self-Attention</strong> can be a powerful and effective mechanism to automatically model the long-distance dependency relationships in-between the tokens of an input sequence.</p></li>
<li><p>This turns out to be an important building block for <strong>Transformers</strong>.</p></li>
</ul>
<ul>
<li><p>A <strong>transformer</strong> is an architecture that builds upon self-attention layers.</p></li>
<li><p>Peter Bloem’s definition of transformers:</p>
<p>“<em>Any architecture designed to process a connected set of units–such as the tokens in a sequence or the pixels in an image–where the only interaction between units is through self-attention.</em>”</p>
</li>
</ul>
<p><img alt="" src="../_images/transformer-block.svg" />
(Source: <a class="reference external" href="http://peterbloem.nl/blog/transformers">http://peterbloem.nl/blog/transformers</a>)</p>
<ul class="simple">
<li><p>A transformer block combines the self-attention layer with a local feedforward network and add normalization and residual connections.</p></li>
<li><p>Normalization and residual connections are standard tricks used to help neural network train faster and more accurately.</p></li>
<li><p>A transformer block can also have <strong>multiheaded attention layers</strong> to keep track of different types of long-distance relationships between input tokens.</p></li>
</ul>
</div>
<div class="section" id="token-positions">
<h2><span class="section-number">1.9. </span>Token Positions<a class="headerlink" href="#token-positions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The above operation of Transformers (or Self-Attention) does not take into account the relative positions of tokens in each sequence.</p></li>
<li><p>The output sequence may therefore be the same no matter how the tokens of the input sequence vary in order. (The model is <strong>permutation invariant</strong>).</p></li>
<li><p>To fix this, most transformers models create <strong>position embeddings</strong> or <strong>position encodings</strong> for each token of the sequence to represent the position of the word/token in the current sequence.</p></li>
<li><p>The position embeddings are added to word/token embeddings via concatenation.</p></li>
</ul>
<p><img alt="" src="../_images/transformers-classifier.svg" />
(Source: <a class="reference external" href="http://peterbloem.nl/blog/transformers">http://peterbloem.nl/blog/transformers</a>)</p>
</div>
<div class="section" id="from-transformers-to-classifiers">
<h2><span class="section-number">1.10. </span>From Transformers to Classifiers<a class="headerlink" href="#from-transformers-to-classifiers" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/transformers-classifier.svg" />
(Source: <a class="reference external" href="http://peterbloem.nl/blog/transformers">http://peterbloem.nl/blog/transformers</a>)</p>
<ul class="simple">
<li><p>With the transformer blocks, the most common way to build a classifier is to have an architecture consisting of <strong>a large chain of transformer blocks</strong>.</p></li>
<li><p>All we need to do is work out how to feed the input sequences into the architecture and how to transform the final output sequence into a single <strong>classification</strong>.</p></li>
</ul>
<ul class="simple">
<li><p>The trick in the classifier is to apply <strong>global average pooling</strong> to the final output sequence, and map the result to a <strong>softmaxed</strong> class vector.</p>
<ul>
<li><p>The output sequence is averaged to produce a single vector (similar to the <em>document embeddings</em>).</p></li>
<li><p>This vector is then projected down to a vector with one element per class and softmaxed into probabilities.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="transfer-learning">
<h2><span class="section-number">1.11. </span>Transfer Learning<a class="headerlink" href="#transfer-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="machine-learning">
<h3>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A traditional machine learning model is trained for performance on a specific task.</p></li>
<li><p>To deal with a different task requires another set of labeled data and another round of training and optimization.</p></li>
<li><p>Therefore, every new task requires a sufficient amount of labeled data.</p></li>
</ul>
</div>
<div class="section" id="transfer-learning-intuition">
<h3>Transfer Learning Intuition<a class="headerlink" href="#transfer-learning-intuition" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Transfer Learning</strong> is the concept of learning a <strong>fundamental representation of the data</strong> that can be adapted to different tasks.</p></li>
<li><p>The learning of this fundamental general-purpose representation often relies on a large amount of data that are available relatively cheaply.</p></li>
<li><p>Two important considerations for effective transfer learning:</p>
<ul>
<li><p>The knowledge distillation step, called <strong>pre-training</strong>, should be based on an abundant amount of data.</p></li>
<li><p>Adaptation, often called <strong>fine-tuning</strong>, should be done with data that shares similarities with the data used for pre-training.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="examples-of-transfer-learning-in-nlp">
<h3>Examples of Transfer Learning in NLP<a class="headerlink" href="#examples-of-transfer-learning-in-nlp" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We can build the sentiment classifier using the pre-trained word embeddings (e.g., GloVe, Fasttext).</p></li>
<li><p>We can apply advanced language model (i.e., BERT) for sentiment classification.</p></li>
<li><p>This two-step sequential learning of <strong>pre-training</strong> and <strong>fine-tuning</strong> is the most common form of transfer learning.</p></li>
</ul>
<ul class="simple">
<li><p>Important considerations:</p>
<ul>
<li><p>When applying the pre-trained models to your data, always check the proportion of the unknown/unseen tokens.</p></li>
<li><p>The weights learned by the pre-trained model can be <strong>frozen</strong> during the fine-tuning of the task-specific model or those weights can be updated or <strong>fine-tuned</strong>. This is a non-trivial decision.</p></li>
<li><p>Generally, fine-tuning the weights of the pre-trained model is only recommended when the source and target tasks are similar.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="famous-transformers-based-models">
<h2><span class="section-number">1.12. </span>Famous Transformers-based Models<a class="headerlink" href="#famous-transformers-based-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bert-bi-directional-encoder-representations-from-transformers">
<h3>BERT (Bi-directional Encoder Representations from Transformers)<a class="headerlink" href="#bert-bi-directional-encoder-representations-from-transformers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Prior embeddings models are mostly context-free; BERT, however, is claimed to be considering contexts in its language model.</p></li>
<li><p>BERT was developed by Google Research in May 2019.</p></li>
<li><p>The paper: <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>.</p></li>
<li><p>BERT refers to Bi-directional Encoder Representations from Transformers.</p></li>
<li><p>BERT consists of a simple stacks of <strong>transformer</strong> blocks.</p></li>
<li><p>It is pre-trained on a large general-domain corpus consisting of 800M words from English books and 2.5B words of Wikipedia articles.</p></li>
</ul>
<ul class="simple">
<li><p>BERT is a neural network built to accomplish two language-modeling tasks:</p>
<ul>
<li><p><strong>Masking</strong>: A certain number of words in the input sequences are randomly masked out and the model is to learn to predict which words have been modified and what the original words are for each input sequence.</p></li>
<li><p><strong>Next Sequence Classification</strong>: Two sequences (around 256 words) are sampled from the corpus which may follow each other directly in the corpus, or are taken from random places. The model needs to learn which case it would be.</p></li>
</ul>
</li>
<li><p>BERT utilizes <strong>WordPiece</strong> tokenization. Each token is somewhere in between word-level and character level sequences.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Don&#39;t like it!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;don&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;like&#39;, &#39;it&#39;, &#39;!&#39;]
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="../_images/bert-tokenizer.jpeg" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In <code class="docutils literal notranslate"><span class="pre">transformers</span></code>, if you want to build a BERT-based sequence classifier, you may need to not only tokenize input texts into sequences but also get the attention masks of each input text. This can be done via the function call <code class="docutils literal notranslate"><span class="pre">BertTokenizer.encode_plus()</span></code>.</p>
</div>
<ul class="simple">
<li><p>With this pretrained BERT, we can add signle task-specific layer after the stack of transformer blocks, which maps the general purpose representation to a task specific output (e.g., binary classification).</p></li>
<li><p>The model can then be fine-tuned for that particular task at hand. (i.e., <strong>transfer learning</strong>)</p></li>
</ul>
<ul class="simple">
<li><p>Statistics of the large BERT model:</p>
<ul>
<li><p>Transformer blocks: 24</p></li>
<li><p>Sequence length: 256(?) Word-pieces</p></li>
<li><p>Embedding dimension: 1024</p></li>
<li><p>Attention heads: 16</p></li>
<li><p>Parameter number: 340M</p></li>
</ul>
</li>
</ul>
<div class="section" id="english-examples">
<h4>English Examples<a class="headerlink" href="#english-examples" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>In this example, we load the pre-trained English BERT model using <code class="docutils literal notranslate"><span class="pre">transformers</span></code>, which is the go-to package for transformer-based NLP models in Python.</p></li>
<li><p>Depending on the architecture of the network, BERT comes in many different variants. <code class="docutils literal notranslate"><span class="pre">transformers</span></code> allows the users to access a lot of pre-trained language models available on its official <a class="reference external" href="https://huggingface.co/models">Hugging Face website</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">textwrap</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForMaskedLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer_dbert</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-cased&quot;</span><span class="p">)</span>
<span class="n">model_dbert</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-cased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;China has been very </span><span class="si">{</span><span class="n">tokenizer_dbert</span><span class="o">.</span><span class="n">mask_token</span><span class="si">}</span><span class="s2"> toward Taiwan.&quot;</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">tokenizer_dbert</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="nb">input</span> <span class="o">==</span> <span class="n">tokenizer_dbert</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">token_logits</span> <span class="o">=</span> <span class="n">model_dbert</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
<span class="n">mask_token_logits</span> <span class="o">=</span> <span class="n">token_logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask_token_index</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">top_5_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">mask_token_logits</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 53.7 ms, sys: 1.16 ms, total: 54.8 ms
Wall time: 56.4 ms
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top_5_tokens</span><span class="p">:</span>
<span class="o">...</span>     <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tokenizer_dbert</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span> <span class="n">tokenizer_dbert</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>China has been very hostile toward Taiwan.
China has been very aggressive toward Taiwan.
China has been very favorable toward Taiwan.
China has been very positive toward Taiwan.
China has been very tolerant toward Taiwan.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="chinese-examples">
<h4>Chinese examples<a class="headerlink" href="#chinese-examples" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForMaskedLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer_zh_albert</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;ckiplab/albert-tiny-chinese&quot;</span><span class="p">)</span>
<span class="n">model_zh_albert</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;ckiplab/albert-tiny-chinese&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;疫情持續</span><span class="si">{</span><span class="n">tokenizer_zh_albert</span><span class="o">.</span><span class="n">mask_token</span><span class="si">}</span><span class="s2">，考驗北市醫療量能。&quot;</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">tokenizer_zh_albert</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="nb">input</span> <span class="o">==</span> <span class="n">tokenizer_zh_albert</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">token_logits</span> <span class="o">=</span> <span class="n">model_zh_albert</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
<span class="n">mask_token_logits</span> <span class="o">=</span> <span class="n">token_logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask_token_index</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">top_5_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">mask_token_logits</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 15.6 ms, sys: 852 µs, total: 16.5 ms
Wall time: 16.4 ms
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top_5_tokens</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tokenizer_zh_albert</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span>
                     <span class="n">tokenizer_zh_albert</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>疫情持續大，考驗北市醫療量能。
疫情持續熱，考驗北市醫療量能。
疫情持續[UNK]，考驗北市醫療量能。
疫情持續高，考驗北市醫療量能。
疫情持續了，考驗北市醫療量能。
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="gpt-2-generative-pre-training-2">
<h3>GPT-2 (Generative Pre-Training 2)<a class="headerlink" href="#gpt-2-generative-pre-training-2" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>GPT-2 is famous (notorious) in the news media as the “<a class="reference external" href="https://www.bbc.com/news/technology-47249163">malicious writing AI</a>”.</p></li>
<li><p>Different from BERT, GPT-2 is fundamentally a language <strong>generation</strong> model (i.e., a Decoder-oriented generative model).</p></li>
<li><p>Compared to BERT, GPT-2 features its the linguistic diversity of their training data (e.g., posts and links via the social media site <em>Reddit</em> with a minimum level of social support, i.e., 按讚數).</p></li>
<li><p>Statistics of GPT-2:</p>
<ul>
<li><p>Transformer blocks: 48</p></li>
<li><p>Sequence length: 1024</p></li>
<li><p>Ebmedding dimension: 1600</p></li>
<li><p>Attention heads: 36</p></li>
<li><p>Parameter number: 1.5B</p></li>
</ul>
</li>
</ul>
<div class="section" id="id1">
<h4>English Examples<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Let’s look at a simple example of English GPT-2, where we use the model to create a sample texts based on a short prompt.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelWithLMHead</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer_en_gpt2</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model_en_gpt2</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
                                                     <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Taiwan and China are two independent countries&quot;</span>

<span class="c1"># Tokenize the input string</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">tokenizer_en_gpt2</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># Run the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_en_gpt2</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
                                <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
                                <span class="n">top_k</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="c1"># Print the output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="n">tokenizer_en_gpt2</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">40</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Taiwan and China are two independent
countries in the Americas. In recent
years, the two countries have become a
major trading partner for each other
thanks to the global financial system.
According to a recent report from the
World Bank, China&#39;s GDP per head was
more than $100 per person back in 2006
and $85 in 2010, a growth rate which
grew to double by 2016. India&#39;s growth
was even more evident as it per head per
capita, up 11.8 percent to $67
CPU times: user 5.14 s, sys: 263 ms, total: 5.4 s
Wall time: 5.4 s
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="chinese">
<h4>Chinese<a class="headerlink" href="#chinese" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelWithLMHead</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer_zh_gpt2</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ckiplab/gpt2-base-chinese&quot;</span><span class="p">)</span>

<span class="c1">#model = AutoModelWithLMHead.from_pretrained(&quot;gpt2-xl&quot;)</span>
<span class="n">model_zh_gpt2</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ckiplab/gpt2-base-chinese&quot;</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#text = &quot;老太太把眼鏡往下一移，從眼鏡上面朝屋內四處張望了一圈，然後又把眼鏡往上抬著，從眼鏡下面往屋外瞧。她平時很少、甚至從來沒有透過眼鏡去找像一個小男孩這樣小傢伙。對她來說，自己這副做工考究的眼鏡是地位的象徵，它的裝飾價值遠遠超出了實用價值，其實，即使戴上兩片爐蓋也照樣看得一清二楚。&quot;</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;最近疫情逐漸升溫，許多地方出錯破口，政府&quot;</span>
<span class="c1"># Tokenize the input string</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">tokenizer_zh_gpt2</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># Run the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_zh_gpt2</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> 
                                <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">+</span> <span class="n">tokenizer_zh_gpt2</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">40</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 最 近 疫 情 逐 漸 升 溫 ， 許 多 地 方 出 錯 破 口 ， 政 府
亦 有 不 少 地 方 被 出 錯 破 口 ， 但 卻 有 部 分 地 方 的
缺 口 ， 且 沒 有 設 防 疫 口 ， 並 且 都 遭 到 部 分 區 域
中 的 區 域 中 不 得 不 的 地 方 出 錯 的 影 響 。 但 是 ，
為 提 高 警 覺 ， 疫 情 在 當 地 的 不 滿 ， 部 分 區 域 亦
有 部 分 發 生 錯 誤 ， 政 府 對 於 區 域 中 所 有 區 域 均
未 設 防 疫 口 ， 且 其 有 些 區 域 皆 沒 有 防 疫 口 。 雖
然 世 界 衛 生 組 織 對 於 區 域 中 一 致 性 的 反 應 非 常
有 限 ， 但 是 並 無 法 制 定 出 有 關 警 戒 的 規 定 ， 但
防 疫 部 門 指 出 一 般 而 言 區 域 內 一 致 性 防 疫 的 狀
況 不 適 合 出 現 。 在 2010 年 ， 有 學 者 表 示 當 地 的
大 部 分 市 區 因 為 沒 有 任 何 地 方 被 出 錯 的 反 應 ，
有 可 能 發 生 錯 誤 ， 但 也 有 人 質 疑 區 域 中 的 不 安
全 及 可 怕 的 不 確 定 性 ， 但 也 有 人 認 為 事 件 不 是
地 方 政 府 能 否 同 意 。 當 地 一 致 性 防 疫 是 一 個 非
常 重 要 的 環 節 ， 因 為 不 是 傳 統 的 一 種 區 域 ， 因
此 不 應 被 視 為 是 一 個 非 常 重 要 的 保 護 措 施 ， 例
如 「 區 域 中 出 錯 」 的 問 題 。 但 政 府 未 能 制 定 出
一 致 性 的 防 疫 措 施 。 自 2001 年 以 來 ， 美 國 境 內
各 地 區 的 大 部 分 區 域 都 被 出 錯 。 當 地 的 國 民 ，
可 以 在 發 生 錯 誤 的 情 況 下 在 一 定 時 間 內 出 錯 。
當 地 出 錯 ， 可 以 在 一 定 時 間 內 出 錯 ， 而 國 內 出
錯 的 狀 況 可 以 被 視 為 無 害 的 。 這 種 現 象 也 有 一
些 是 人 力 資 源 與 環 境 汙 染 。 為 了 防 範 大 規 模 人
口 感 染 ， 區 域 裡 的 不 同 情 況 通 常 不 適 合 出 錯 。
另 外
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="prospect">
<h2><span class="section-number">1.13. </span>Prospect<a class="headerlink" href="#prospect" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Now it seems that the more complex the transformer-based model is, the more effective. (cf. <a class="reference external" href="https://arxiv.org/abs/1901.02860">Transformer-XL</a>)</p></li>
<li><p>In other words, the current performance limit seems to be purely in the <strong>hardware</strong>.</p></li>
<li><p>Transformers are generic, waiting to be exploited in many more fields.</p></li>
<li><p>Many linguists are still waiting with bated breath for how to tease apart the morpho-syntactic and semantic long-distance dependencies learned in these transformer blocks.</p></li>
</ul>
</div>
<div class="section" id="references">
<h2><span class="section-number">1.14. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Important Works:</p>
<ul>
<li><p>Sequence to Sequence Models</p>
<ul>
<li><p>Sutskever et al. 2014. <a class="reference external" href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">Sequence to sequence learning with neural network</a>.</p></li>
<li><p>Cho et al. 2014. <a class="reference external" href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Learning Phrase Representations using RNN Encoder–Decoderfor Statistical Machine Translation</a>.</p></li>
</ul>
</li>
<li><p>Attention</p>
<ul>
<li><p>Bahdanau et al. 2014. <a class="reference external" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>.</p></li>
<li><p>Luong et al. 2015. <a class="reference external" href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a>.</p></li>
<li><p>Vaswani et al. 2017. <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p>This lecture is Peter Bloem’s blog post: <a class="reference external" href="http://peterbloem.nl/blog/transformers">Transformers from Scratch</a>.</p></li>
<li><p>Jay Alammar’s blog post: <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model</a></p></li>
<li><p>Jay Alammar’s blog post: <a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li>
<li><p>Jay Alammar’s blog post: <a class="reference external" href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></p></li>
<li><p>Please see a very nice review of Lilian Weng’s <a class="reference external" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Attention? Attention!</a>.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./temp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../nlp/text-vec-embedding.html" title="previous page"><span class="section-number">3. </span>Word Embeddings</a>
    <a class='right-next' id="next-link" href="dl-seq-to-seq-attention-addition.html" title="next page"><span class="section-number">2. </span>Sequence Model with Attention for Addition Learning</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>