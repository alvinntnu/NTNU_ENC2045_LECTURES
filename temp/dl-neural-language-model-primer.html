
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Language Model: A Start &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../nlp/text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-algorithm.html">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-nlp-case.html">
   3. Machine Learning: NLP Tasks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-neural-network-from-scratch.html">
   Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="text-vec-embedding.html">
   Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="text-vec-embedding-keras.html">
   Word Embedding Using Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-statistical-language-model.html">
   Statistical Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sequence-models-intuition.html">
   Sequence Models Intuition
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-seq-to-seq-types.html">
   Attention: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-transformers-intuition.html">
   Transformers: Intuition
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   1. Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   2. Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   3. Assignment III: Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-3-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-3-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   4. Assignment IV: Chinese Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   5. Assignment V: Text Vectorization
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/temp/dl-neural-language-model-primer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/temp/dl-neural-language-model-primer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/temp/dl-neural-language-model-primer.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflow-of-neural-language-model">
   Workflow of Neural Language Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bigram-model">
   Bigram Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     Tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-to-sequences-and-training-data">
     Text-to-Sequences and Training Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-representation-of-the-next-word">
     One-hot Representation of the Next-Word
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-rnn-language-model">
     Define RNN Language Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#beam-search">
   Beam Search
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#searching-in-nlp">
     Searching in NLP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beam-search-decoding">
     Beam Search Decoding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-language-model-a-start">
<h1>Neural Language Model: A Start<a class="headerlink" href="#neural-language-model-a-start" title="Permalink to this headline">¶</a></h1>
<p>(<a class="reference external" href="https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/">https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/</a>)</p>
<ul class="simple">
<li><p>English texts</p></li>
<li><p>Three methods to build a neural language model:</p>
<ul>
<li><p>bigram</p></li>
<li><p>trigram</p></li>
<li><p>line-based</p></li>
</ul>
</li>
</ul>
<div class="section" id="workflow-of-neural-language-model">
<h2>Workflow of Neural Language Model<a class="headerlink" href="#workflow-of-neural-language-model" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/neural-language-model-flowchart.png" /></p>
</div>
<div class="section" id="bigram-model">
<h2>Bigram Model<a class="headerlink" href="#bigram-model" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span><span class="p">,</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Tokenizer()</span></code>:</p>
<ul class="simple">
<li><p>By default, all punctuation is removed, turning the texts into space-separated sequences of words</p></li>
<li><p>Word-index <code class="docutils literal notranslate"><span class="pre">0</span></code> is a reserved index that won’t be assigned to any word.</p></li>
<li><p>Important Arguments:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">num_words</span></code>: the maximum number of words to keep, based on word frequency. Only the most common <code class="docutils literal notranslate"><span class="pre">num_words-1</span></code> words will be kept.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">filters</span></code>: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the <code class="docutils literal notranslate"><span class="pre">'</span></code> character.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lower</span></code>: boolean. Whether to convert the texts to lowercase.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">split</span></code>: str. Separator for word splitting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">char_level</span></code>: if True, every character will be treated as a token.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">oov_token</span></code>: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># source text</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot; Jack and Jill went up the hill</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">		To fetch a pail of water</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">		Jack fell down and broke his crown</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">		And Jill came tumbling after</span><span class="se">\n</span><span class="s2"> &quot;&quot;&quot;</span>
<span class="c1"># integer encode text</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>

<span class="c1"># now the data consists of a sequence of word index integers</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">data</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># determine the vocabulary size</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Size: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary Size: 22
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="text-to-sequences-and-training-data">
<h3>Text-to-Sequences and Training Data<a class="headerlink" href="#text-to-sequences-and-training-data" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create word -&gt; word sequences</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)):</span>
    <span class="n">sequence</span> <span class="o">=</span> <span class="n">encoded</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Sequences: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total Sequences: 24
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[2, 1],
       [1, 3],
       [3, 4],
       [4, 5],
       [5, 6]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>For bigram models, the first word is the input <em>X</em> and the second word is the expected output <em>y</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split into X and y elements</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 2,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  2, 14, 15,  1,
       16, 17, 18,  1,  3, 19, 20])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  2, 14, 15,  1, 16,
       17, 18,  1,  3, 19, 20, 21])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="one-hot-representation-of-the-next-word">
<h3>One-hot Representation of the Next-Word<a class="headerlink" href="#one-hot-representation-of-the-next-word" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># one hot encode outputs</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(24, 22)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-rnn-language-model">
<h3>Define RNN Language Model<a class="headerlink" href="#define-rnn-language-model" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># embedding dimension</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span> <span class="c1"># LSTM Complexity</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 1, 10)             220       
_________________________________________________________________
lstm (LSTM)                  (None, 50)                12200     
_________________________________________________________________
dense (Dense)                (None, 22)                1122      
=================================================================
Total params: 13,542
Trainable params: 13,542
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compile network</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="c1"># fit network</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/500
1/1 - 0s - loss: 3.0912 - accuracy: 0.0000e+00
Epoch 2/500
1/1 - 0s - loss: 3.0905 - accuracy: 0.1250
Epoch 3/500
1/1 - 0s - loss: 3.0897 - accuracy: 0.1667
Epoch 4/500
1/1 - 0s - loss: 3.0890 - accuracy: 0.1667
Epoch 5/500
1/1 - 0s - loss: 3.0882 - accuracy: 0.1667
Epoch 6/500
1/1 - 0s - loss: 3.0875 - accuracy: 0.1250
Epoch 7/500
1/1 - 0s - loss: 3.0868 - accuracy: 0.1250
Epoch 8/500
1/1 - 0s - loss: 3.0860 - accuracy: 0.1250
Epoch 9/500
1/1 - 0s - loss: 3.0852 - accuracy: 0.1250
Epoch 10/500
1/1 - 0s - loss: 3.0845 - accuracy: 0.1250
Epoch 11/500
1/1 - 0s - loss: 3.0837 - accuracy: 0.1250
Epoch 12/500
1/1 - 0s - loss: 3.0829 - accuracy: 0.1250
Epoch 13/500
1/1 - 0s - loss: 3.0821 - accuracy: 0.1250
Epoch 14/500
1/1 - 0s - loss: 3.0813 - accuracy: 0.1250
Epoch 15/500
1/1 - 0s - loss: 3.0804 - accuracy: 0.1250
Epoch 16/500
1/1 - 0s - loss: 3.0796 - accuracy: 0.1250
Epoch 17/500
1/1 - 0s - loss: 3.0787 - accuracy: 0.1250
Epoch 18/500
1/1 - 0s - loss: 3.0778 - accuracy: 0.1250
Epoch 19/500
1/1 - 0s - loss: 3.0769 - accuracy: 0.1250
Epoch 20/500
1/1 - 0s - loss: 3.0759 - accuracy: 0.1250
Epoch 21/500
1/1 - 0s - loss: 3.0750 - accuracy: 0.1250
Epoch 22/500
1/1 - 0s - loss: 3.0740 - accuracy: 0.1250
Epoch 23/500
1/1 - 0s - loss: 3.0730 - accuracy: 0.1250
Epoch 24/500
1/1 - 0s - loss: 3.0719 - accuracy: 0.1250
Epoch 25/500
1/1 - 0s - loss: 3.0709 - accuracy: 0.1250
Epoch 26/500
1/1 - 0s - loss: 3.0698 - accuracy: 0.1250
Epoch 27/500
1/1 - 0s - loss: 3.0686 - accuracy: 0.1250
Epoch 28/500
1/1 - 0s - loss: 3.0675 - accuracy: 0.1250
Epoch 29/500
1/1 - 0s - loss: 3.0663 - accuracy: 0.1250
Epoch 30/500
1/1 - 0s - loss: 3.0650 - accuracy: 0.1250
Epoch 31/500
1/1 - 0s - loss: 3.0637 - accuracy: 0.1250
Epoch 32/500
1/1 - 0s - loss: 3.0624 - accuracy: 0.1250
Epoch 33/500
1/1 - 0s - loss: 3.0611 - accuracy: 0.1250
Epoch 34/500
1/1 - 0s - loss: 3.0597 - accuracy: 0.1250
Epoch 35/500
1/1 - 0s - loss: 3.0583 - accuracy: 0.1250
Epoch 36/500
1/1 - 0s - loss: 3.0568 - accuracy: 0.1250
Epoch 37/500
1/1 - 0s - loss: 3.0552 - accuracy: 0.1250
Epoch 38/500
1/1 - 0s - loss: 3.0537 - accuracy: 0.1250
Epoch 39/500
1/1 - 0s - loss: 3.0521 - accuracy: 0.1250
Epoch 40/500
1/1 - 0s - loss: 3.0504 - accuracy: 0.1250
Epoch 41/500
1/1 - 0s - loss: 3.0487 - accuracy: 0.1250
Epoch 42/500
1/1 - 0s - loss: 3.0469 - accuracy: 0.1250
Epoch 43/500
1/1 - 0s - loss: 3.0450 - accuracy: 0.1250
Epoch 44/500
1/1 - 0s - loss: 3.0431 - accuracy: 0.1250
Epoch 45/500
1/1 - 0s - loss: 3.0412 - accuracy: 0.1250
Epoch 46/500
1/1 - 0s - loss: 3.0392 - accuracy: 0.1250
Epoch 47/500
1/1 - 0s - loss: 3.0371 - accuracy: 0.1250
Epoch 48/500
1/1 - 0s - loss: 3.0350 - accuracy: 0.1250
Epoch 49/500
1/1 - 0s - loss: 3.0327 - accuracy: 0.1250
Epoch 50/500
1/1 - 0s - loss: 3.0305 - accuracy: 0.1250
Epoch 51/500
1/1 - 0s - loss: 3.0281 - accuracy: 0.1250
Epoch 52/500
1/1 - 0s - loss: 3.0257 - accuracy: 0.1250
Epoch 53/500
1/1 - 0s - loss: 3.0232 - accuracy: 0.1250
Epoch 54/500
1/1 - 0s - loss: 3.0206 - accuracy: 0.1250
Epoch 55/500
1/1 - 0s - loss: 3.0179 - accuracy: 0.1250
Epoch 56/500
1/1 - 0s - loss: 3.0152 - accuracy: 0.1250
Epoch 57/500
1/1 - 0s - loss: 3.0124 - accuracy: 0.1250
Epoch 58/500
1/1 - 0s - loss: 3.0095 - accuracy: 0.1250
Epoch 59/500
1/1 - 0s - loss: 3.0065 - accuracy: 0.1250
Epoch 60/500
1/1 - 0s - loss: 3.0034 - accuracy: 0.1250
Epoch 61/500
1/1 - 0s - loss: 3.0002 - accuracy: 0.1250
Epoch 62/500
1/1 - 0s - loss: 2.9969 - accuracy: 0.1250
Epoch 63/500
1/1 - 0s - loss: 2.9935 - accuracy: 0.1250
Epoch 64/500
1/1 - 0s - loss: 2.9900 - accuracy: 0.1250
Epoch 65/500
1/1 - 0s - loss: 2.9864 - accuracy: 0.1250
Epoch 66/500
1/1 - 0s - loss: 2.9827 - accuracy: 0.1250
Epoch 67/500
1/1 - 0s - loss: 2.9789 - accuracy: 0.1250
Epoch 68/500
1/1 - 0s - loss: 2.9750 - accuracy: 0.1250
Epoch 69/500
1/1 - 0s - loss: 2.9710 - accuracy: 0.1250
Epoch 70/500
1/1 - 0s - loss: 2.9668 - accuracy: 0.1250
Epoch 71/500
1/1 - 0s - loss: 2.9626 - accuracy: 0.1250
Epoch 72/500
1/1 - 0s - loss: 2.9582 - accuracy: 0.1250
Epoch 73/500
1/1 - 0s - loss: 2.9536 - accuracy: 0.1250
Epoch 74/500
1/1 - 0s - loss: 2.9490 - accuracy: 0.1250
Epoch 75/500
1/1 - 0s - loss: 2.9442 - accuracy: 0.1250
Epoch 76/500
1/1 - 0s - loss: 2.9393 - accuracy: 0.2083
Epoch 77/500
1/1 - 0s - loss: 2.9342 - accuracy: 0.2083
Epoch 78/500
1/1 - 0s - loss: 2.9291 - accuracy: 0.2083
Epoch 79/500
1/1 - 0s - loss: 2.9237 - accuracy: 0.2083
Epoch 80/500
1/1 - 0s - loss: 2.9182 - accuracy: 0.2083
Epoch 81/500
1/1 - 0s - loss: 2.9126 - accuracy: 0.2083
Epoch 82/500
1/1 - 0s - loss: 2.9068 - accuracy: 0.2083
Epoch 83/500
1/1 - 0s - loss: 2.9009 - accuracy: 0.2083
Epoch 84/500
1/1 - 0s - loss: 2.8948 - accuracy: 0.2083
Epoch 85/500
1/1 - 0s - loss: 2.8886 - accuracy: 0.2083
Epoch 86/500
1/1 - 0s - loss: 2.8822 - accuracy: 0.2083
Epoch 87/500
1/1 - 0s - loss: 2.8757 - accuracy: 0.2083
Epoch 88/500
1/1 - 0s - loss: 2.8689 - accuracy: 0.2083
Epoch 89/500
1/1 - 0s - loss: 2.8621 - accuracy: 0.2083
Epoch 90/500
1/1 - 0s - loss: 2.8550 - accuracy: 0.2083
Epoch 91/500
1/1 - 0s - loss: 2.8478 - accuracy: 0.2083
Epoch 92/500
1/1 - 0s - loss: 2.8404 - accuracy: 0.2083
Epoch 93/500
1/1 - 0s - loss: 2.8329 - accuracy: 0.2083
Epoch 94/500
1/1 - 0s - loss: 2.8251 - accuracy: 0.2083
Epoch 95/500
1/1 - 0s - loss: 2.8172 - accuracy: 0.2083
Epoch 96/500
1/1 - 0s - loss: 2.8092 - accuracy: 0.2083
Epoch 97/500
1/1 - 0s - loss: 2.8009 - accuracy: 0.2083
Epoch 98/500
1/1 - 0s - loss: 2.7925 - accuracy: 0.2083
Epoch 99/500
1/1 - 0s - loss: 2.7839 - accuracy: 0.2083
Epoch 100/500
1/1 - 0s - loss: 2.7751 - accuracy: 0.2083
Epoch 101/500
1/1 - 0s - loss: 2.7661 - accuracy: 0.2083
Epoch 102/500
1/1 - 0s - loss: 2.7570 - accuracy: 0.2083
Epoch 103/500
1/1 - 0s - loss: 2.7477 - accuracy: 0.2083
Epoch 104/500
1/1 - 0s - loss: 2.7382 - accuracy: 0.2083
Epoch 105/500
1/1 - 0s - loss: 2.7285 - accuracy: 0.2083
Epoch 106/500
1/1 - 0s - loss: 2.7187 - accuracy: 0.2083
Epoch 107/500
1/1 - 0s - loss: 2.7087 - accuracy: 0.2083
Epoch 108/500
1/1 - 0s - loss: 2.6985 - accuracy: 0.2083
Epoch 109/500
1/1 - 0s - loss: 2.6881 - accuracy: 0.2083
Epoch 110/500
1/1 - 0s - loss: 2.6776 - accuracy: 0.2083
Epoch 111/500
1/1 - 0s - loss: 2.6669 - accuracy: 0.2083
Epoch 112/500
1/1 - 0s - loss: 2.6561 - accuracy: 0.2083
Epoch 113/500
1/1 - 0s - loss: 2.6451 - accuracy: 0.2083
Epoch 114/500
1/1 - 0s - loss: 2.6339 - accuracy: 0.2083
Epoch 115/500
1/1 - 0s - loss: 2.6226 - accuracy: 0.2083
Epoch 116/500
1/1 - 0s - loss: 2.6111 - accuracy: 0.2083
Epoch 117/500
1/1 - 0s - loss: 2.5995 - accuracy: 0.2083
Epoch 118/500
1/1 - 0s - loss: 2.5877 - accuracy: 0.2083
Epoch 119/500
1/1 - 0s - loss: 2.5758 - accuracy: 0.2083
Epoch 120/500
1/1 - 0s - loss: 2.5637 - accuracy: 0.2083
Epoch 121/500
1/1 - 0s - loss: 2.5516 - accuracy: 0.2083
Epoch 122/500
1/1 - 0s - loss: 2.5393 - accuracy: 0.2083
Epoch 123/500
1/1 - 0s - loss: 2.5268 - accuracy: 0.2083
Epoch 124/500
1/1 - 0s - loss: 2.5143 - accuracy: 0.2083
Epoch 125/500
1/1 - 0s - loss: 2.5016 - accuracy: 0.2083
Epoch 126/500
1/1 - 0s - loss: 2.4889 - accuracy: 0.2083
Epoch 127/500
1/1 - 0s - loss: 2.4760 - accuracy: 0.2083
Epoch 128/500
1/1 - 0s - loss: 2.4631 - accuracy: 0.2083
Epoch 129/500
1/1 - 0s - loss: 2.4501 - accuracy: 0.2083
Epoch 130/500
1/1 - 0s - loss: 2.4369 - accuracy: 0.2083
Epoch 131/500
1/1 - 0s - loss: 2.4237 - accuracy: 0.2083
Epoch 132/500
1/1 - 0s - loss: 2.4105 - accuracy: 0.2083
Epoch 133/500
1/1 - 0s - loss: 2.3971 - accuracy: 0.2083
Epoch 134/500
1/1 - 0s - loss: 2.3837 - accuracy: 0.2083
Epoch 135/500
1/1 - 0s - loss: 2.3703 - accuracy: 0.2083
Epoch 136/500
1/1 - 0s - loss: 2.3568 - accuracy: 0.2083
Epoch 137/500
1/1 - 0s - loss: 2.3432 - accuracy: 0.2083
Epoch 138/500
1/1 - 0s - loss: 2.3296 - accuracy: 0.2083
Epoch 139/500
1/1 - 0s - loss: 2.3160 - accuracy: 0.2083
Epoch 140/500
1/1 - 0s - loss: 2.3024 - accuracy: 0.2083
Epoch 141/500
1/1 - 0s - loss: 2.2887 - accuracy: 0.2083
Epoch 142/500
1/1 - 0s - loss: 2.2750 - accuracy: 0.2083
Epoch 143/500
1/1 - 0s - loss: 2.2613 - accuracy: 0.2083
Epoch 144/500
1/1 - 0s - loss: 2.2475 - accuracy: 0.2083
Epoch 145/500
1/1 - 0s - loss: 2.2338 - accuracy: 0.2083
Epoch 146/500
1/1 - 0s - loss: 2.2200 - accuracy: 0.2083
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 147/500
1/1 - 0s - loss: 2.2062 - accuracy: 0.2083
Epoch 148/500
1/1 - 0s - loss: 2.1925 - accuracy: 0.2083
Epoch 149/500
1/1 - 0s - loss: 2.1787 - accuracy: 0.2083
Epoch 150/500
1/1 - 0s - loss: 2.1649 - accuracy: 0.2083
Epoch 151/500
1/1 - 0s - loss: 2.1511 - accuracy: 0.2083
Epoch 152/500
1/1 - 0s - loss: 2.1374 - accuracy: 0.2500
Epoch 153/500
1/1 - 0s - loss: 2.1236 - accuracy: 0.2500
Epoch 154/500
1/1 - 0s - loss: 2.1098 - accuracy: 0.2917
Epoch 155/500
1/1 - 0s - loss: 2.0961 - accuracy: 0.2917
Epoch 156/500
1/1 - 0s - loss: 2.0823 - accuracy: 0.2917
Epoch 157/500
1/1 - 0s - loss: 2.0686 - accuracy: 0.3333
Epoch 158/500
1/1 - 0s - loss: 2.0549 - accuracy: 0.3333
Epoch 159/500
1/1 - 0s - loss: 2.0411 - accuracy: 0.3333
Epoch 160/500
1/1 - 0s - loss: 2.0274 - accuracy: 0.3750
Epoch 161/500
1/1 - 0s - loss: 2.0137 - accuracy: 0.3750
Epoch 162/500
1/1 - 0s - loss: 2.0000 - accuracy: 0.4167
Epoch 163/500
1/1 - 0s - loss: 1.9863 - accuracy: 0.4583
Epoch 164/500
1/1 - 0s - loss: 1.9726 - accuracy: 0.4583
Epoch 165/500
1/1 - 0s - loss: 1.9590 - accuracy: 0.4583
Epoch 166/500
1/1 - 0s - loss: 1.9453 - accuracy: 0.4583
Epoch 167/500
1/1 - 0s - loss: 1.9316 - accuracy: 0.4583
Epoch 168/500
1/1 - 0s - loss: 1.9180 - accuracy: 0.4583
Epoch 169/500
1/1 - 0s - loss: 1.9044 - accuracy: 0.5000
Epoch 170/500
1/1 - 0s - loss: 1.8907 - accuracy: 0.5417
Epoch 171/500
1/1 - 0s - loss: 1.8771 - accuracy: 0.5417
Epoch 172/500
1/1 - 0s - loss: 1.8635 - accuracy: 0.5417
Epoch 173/500
1/1 - 0s - loss: 1.8499 - accuracy: 0.5417
Epoch 174/500
1/1 - 0s - loss: 1.8363 - accuracy: 0.5417
Epoch 175/500
1/1 - 0s - loss: 1.8227 - accuracy: 0.5417
Epoch 176/500
1/1 - 0s - loss: 1.8092 - accuracy: 0.5417
Epoch 177/500
1/1 - 0s - loss: 1.7956 - accuracy: 0.5417
Epoch 178/500
1/1 - 0s - loss: 1.7821 - accuracy: 0.5417
Epoch 179/500
1/1 - 0s - loss: 1.7686 - accuracy: 0.5417
Epoch 180/500
1/1 - 0s - loss: 1.7551 - accuracy: 0.5833
Epoch 181/500
1/1 - 0s - loss: 1.7416 - accuracy: 0.6250
Epoch 182/500
1/1 - 0s - loss: 1.7281 - accuracy: 0.6250
Epoch 183/500
1/1 - 0s - loss: 1.7147 - accuracy: 0.6250
Epoch 184/500
1/1 - 0s - loss: 1.7012 - accuracy: 0.6250
Epoch 185/500
1/1 - 0s - loss: 1.6878 - accuracy: 0.6250
Epoch 186/500
1/1 - 0s - loss: 1.6744 - accuracy: 0.6667
Epoch 187/500
1/1 - 0s - loss: 1.6610 - accuracy: 0.6667
Epoch 188/500
1/1 - 0s - loss: 1.6477 - accuracy: 0.6667
Epoch 189/500
1/1 - 0s - loss: 1.6344 - accuracy: 0.6667
Epoch 190/500
1/1 - 0s - loss: 1.6211 - accuracy: 0.6667
Epoch 191/500
1/1 - 0s - loss: 1.6078 - accuracy: 0.6667
Epoch 192/500
1/1 - 0s - loss: 1.5946 - accuracy: 0.6667
Epoch 193/500
1/1 - 0s - loss: 1.5814 - accuracy: 0.7083
Epoch 194/500
1/1 - 0s - loss: 1.5682 - accuracy: 0.7083
Epoch 195/500
1/1 - 0s - loss: 1.5550 - accuracy: 0.7083
Epoch 196/500
1/1 - 0s - loss: 1.5419 - accuracy: 0.7083
Epoch 197/500
1/1 - 0s - loss: 1.5289 - accuracy: 0.7500
Epoch 198/500
1/1 - 0s - loss: 1.5158 - accuracy: 0.7500
Epoch 199/500
1/1 - 0s - loss: 1.5028 - accuracy: 0.7500
Epoch 200/500
1/1 - 0s - loss: 1.4899 - accuracy: 0.7500
Epoch 201/500
1/1 - 0s - loss: 1.4770 - accuracy: 0.7917
Epoch 202/500
1/1 - 0s - loss: 1.4641 - accuracy: 0.7917
Epoch 203/500
1/1 - 0s - loss: 1.4513 - accuracy: 0.7917
Epoch 204/500
1/1 - 0s - loss: 1.4385 - accuracy: 0.7917
Epoch 205/500
1/1 - 0s - loss: 1.4258 - accuracy: 0.7917
Epoch 206/500
1/1 - 0s - loss: 1.4131 - accuracy: 0.7917
Epoch 207/500
1/1 - 0s - loss: 1.4004 - accuracy: 0.7917
Epoch 208/500
1/1 - 0s - loss: 1.3879 - accuracy: 0.7917
Epoch 209/500
1/1 - 0s - loss: 1.3753 - accuracy: 0.7917
Epoch 210/500
1/1 - 0s - loss: 1.3629 - accuracy: 0.7917
Epoch 211/500
1/1 - 0s - loss: 1.3504 - accuracy: 0.7917
Epoch 212/500
1/1 - 0s - loss: 1.3381 - accuracy: 0.7917
Epoch 213/500
1/1 - 0s - loss: 1.3258 - accuracy: 0.7917
Epoch 214/500
1/1 - 0s - loss: 1.3135 - accuracy: 0.7917
Epoch 215/500
1/1 - 0s - loss: 1.3014 - accuracy: 0.7917
Epoch 216/500
1/1 - 0s - loss: 1.2892 - accuracy: 0.7917
Epoch 217/500
1/1 - 0s - loss: 1.2772 - accuracy: 0.7917
Epoch 218/500
1/1 - 0s - loss: 1.2652 - accuracy: 0.7917
Epoch 219/500
1/1 - 0s - loss: 1.2533 - accuracy: 0.7917
Epoch 220/500
1/1 - 0s - loss: 1.2414 - accuracy: 0.7917
Epoch 221/500
1/1 - 0s - loss: 1.2297 - accuracy: 0.7917
Epoch 222/500
1/1 - 0s - loss: 1.2180 - accuracy: 0.7917
Epoch 223/500
1/1 - 0s - loss: 1.2063 - accuracy: 0.7917
Epoch 224/500
1/1 - 0s - loss: 1.1947 - accuracy: 0.7917
Epoch 225/500
1/1 - 0s - loss: 1.1833 - accuracy: 0.7917
Epoch 226/500
1/1 - 0s - loss: 1.1718 - accuracy: 0.7917
Epoch 227/500
1/1 - 0s - loss: 1.1605 - accuracy: 0.7917
Epoch 228/500
1/1 - 0s - loss: 1.1492 - accuracy: 0.7917
Epoch 229/500
1/1 - 0s - loss: 1.1380 - accuracy: 0.7917
Epoch 230/500
1/1 - 0s - loss: 1.1269 - accuracy: 0.7917
Epoch 231/500
1/1 - 0s - loss: 1.1159 - accuracy: 0.7917
Epoch 232/500
1/1 - 0s - loss: 1.1049 - accuracy: 0.7917
Epoch 233/500
1/1 - 0s - loss: 1.0941 - accuracy: 0.7917
Epoch 234/500
1/1 - 0s - loss: 1.0833 - accuracy: 0.7917
Epoch 235/500
1/1 - 0s - loss: 1.0726 - accuracy: 0.7917
Epoch 236/500
1/1 - 0s - loss: 1.0620 - accuracy: 0.7917
Epoch 237/500
1/1 - 0s - loss: 1.0514 - accuracy: 0.7917
Epoch 238/500
1/1 - 0s - loss: 1.0410 - accuracy: 0.7917
Epoch 239/500
1/1 - 0s - loss: 1.0306 - accuracy: 0.7917
Epoch 240/500
1/1 - 0s - loss: 1.0203 - accuracy: 0.7917
Epoch 241/500
1/1 - 0s - loss: 1.0101 - accuracy: 0.7917
Epoch 242/500
1/1 - 0s - loss: 1.0000 - accuracy: 0.7917
Epoch 243/500
1/1 - 0s - loss: 0.9900 - accuracy: 0.7917
Epoch 244/500
1/1 - 0s - loss: 0.9801 - accuracy: 0.7917
Epoch 245/500
1/1 - 0s - loss: 0.9702 - accuracy: 0.7917
Epoch 246/500
1/1 - 0s - loss: 0.9604 - accuracy: 0.7917
Epoch 247/500
1/1 - 0s - loss: 0.9508 - accuracy: 0.7917
Epoch 248/500
1/1 - 0s - loss: 0.9412 - accuracy: 0.7917
Epoch 249/500
1/1 - 0s - loss: 0.9317 - accuracy: 0.7917
Epoch 250/500
1/1 - 0s - loss: 0.9223 - accuracy: 0.8333
Epoch 251/500
1/1 - 0s - loss: 0.9130 - accuracy: 0.8333
Epoch 252/500
1/1 - 0s - loss: 0.9038 - accuracy: 0.8333
Epoch 253/500
1/1 - 0s - loss: 0.8947 - accuracy: 0.8333
Epoch 254/500
1/1 - 0s - loss: 0.8856 - accuracy: 0.8333
Epoch 255/500
1/1 - 0s - loss: 0.8767 - accuracy: 0.8333
Epoch 256/500
1/1 - 0s - loss: 0.8679 - accuracy: 0.8333
Epoch 257/500
1/1 - 0s - loss: 0.8591 - accuracy: 0.8333
Epoch 258/500
1/1 - 0s - loss: 0.8505 - accuracy: 0.8333
Epoch 259/500
1/1 - 0s - loss: 0.8419 - accuracy: 0.8333
Epoch 260/500
1/1 - 0s - loss: 0.8334 - accuracy: 0.8333
Epoch 261/500
1/1 - 0s - loss: 0.8250 - accuracy: 0.8333
Epoch 262/500
1/1 - 0s - loss: 0.8168 - accuracy: 0.8333
Epoch 263/500
1/1 - 0s - loss: 0.8086 - accuracy: 0.8333
Epoch 264/500
1/1 - 0s - loss: 0.8005 - accuracy: 0.8333
Epoch 265/500
1/1 - 0s - loss: 0.7925 - accuracy: 0.8333
Epoch 266/500
1/1 - 0s - loss: 0.7846 - accuracy: 0.8333
Epoch 267/500
1/1 - 0s - loss: 0.7768 - accuracy: 0.8333
Epoch 268/500
1/1 - 0s - loss: 0.7691 - accuracy: 0.8333
Epoch 269/500
1/1 - 0s - loss: 0.7614 - accuracy: 0.8333
Epoch 270/500
1/1 - 0s - loss: 0.7539 - accuracy: 0.8333
Epoch 271/500
1/1 - 0s - loss: 0.7465 - accuracy: 0.8333
Epoch 272/500
1/1 - 0s - loss: 0.7391 - accuracy: 0.8333
Epoch 273/500
1/1 - 0s - loss: 0.7319 - accuracy: 0.8333
Epoch 274/500
1/1 - 0s - loss: 0.7248 - accuracy: 0.8333
Epoch 275/500
1/1 - 0s - loss: 0.7177 - accuracy: 0.8333
Epoch 276/500
1/1 - 0s - loss: 0.7107 - accuracy: 0.8333
Epoch 277/500
1/1 - 0s - loss: 0.7039 - accuracy: 0.8333
Epoch 278/500
1/1 - 0s - loss: 0.6971 - accuracy: 0.8333
Epoch 279/500
1/1 - 0s - loss: 0.6904 - accuracy: 0.8333
Epoch 280/500
1/1 - 0s - loss: 0.6838 - accuracy: 0.8333
Epoch 281/500
1/1 - 0s - loss: 0.6774 - accuracy: 0.8333
Epoch 282/500
1/1 - 0s - loss: 0.6710 - accuracy: 0.8333
Epoch 283/500
1/1 - 0s - loss: 0.6646 - accuracy: 0.8333
Epoch 284/500
1/1 - 0s - loss: 0.6584 - accuracy: 0.8333
Epoch 285/500
1/1 - 0s - loss: 0.6523 - accuracy: 0.8333
Epoch 286/500
1/1 - 0s - loss: 0.6463 - accuracy: 0.8333
Epoch 287/500
1/1 - 0s - loss: 0.6403 - accuracy: 0.8333
Epoch 288/500
1/1 - 0s - loss: 0.6345 - accuracy: 0.8333
Epoch 289/500
1/1 - 0s - loss: 0.6287 - accuracy: 0.8333
Epoch 290/500
1/1 - 0s - loss: 0.6230 - accuracy: 0.8333
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 291/500
1/1 - 0s - loss: 0.6174 - accuracy: 0.8333
Epoch 292/500
1/1 - 0s - loss: 0.6119 - accuracy: 0.8333
Epoch 293/500
1/1 - 0s - loss: 0.6065 - accuracy: 0.8333
Epoch 294/500
1/1 - 0s - loss: 0.6011 - accuracy: 0.8333
Epoch 295/500
1/1 - 0s - loss: 0.5959 - accuracy: 0.8333
Epoch 296/500
1/1 - 0s - loss: 0.5907 - accuracy: 0.8750
Epoch 297/500
1/1 - 0s - loss: 0.5856 - accuracy: 0.8750
Epoch 298/500
1/1 - 0s - loss: 0.5806 - accuracy: 0.8750
Epoch 299/500
1/1 - 0s - loss: 0.5757 - accuracy: 0.8750
Epoch 300/500
1/1 - 0s - loss: 0.5708 - accuracy: 0.8750
Epoch 301/500
1/1 - 0s - loss: 0.5661 - accuracy: 0.8750
Epoch 302/500
1/1 - 0s - loss: 0.5614 - accuracy: 0.8750
Epoch 303/500
1/1 - 0s - loss: 0.5568 - accuracy: 0.8750
Epoch 304/500
1/1 - 0s - loss: 0.5522 - accuracy: 0.8750
Epoch 305/500
1/1 - 0s - loss: 0.5477 - accuracy: 0.8750
Epoch 306/500
1/1 - 0s - loss: 0.5434 - accuracy: 0.8750
Epoch 307/500
1/1 - 0s - loss: 0.5390 - accuracy: 0.8750
Epoch 308/500
1/1 - 0s - loss: 0.5348 - accuracy: 0.8750
Epoch 309/500
1/1 - 0s - loss: 0.5306 - accuracy: 0.8750
Epoch 310/500
1/1 - 0s - loss: 0.5265 - accuracy: 0.8750
Epoch 311/500
1/1 - 0s - loss: 0.5225 - accuracy: 0.8750
Epoch 312/500
1/1 - 0s - loss: 0.5185 - accuracy: 0.8750
Epoch 313/500
1/1 - 0s - loss: 0.5146 - accuracy: 0.8750
Epoch 314/500
1/1 - 0s - loss: 0.5108 - accuracy: 0.8750
Epoch 315/500
1/1 - 0s - loss: 0.5070 - accuracy: 0.8750
Epoch 316/500
1/1 - 0s - loss: 0.5033 - accuracy: 0.8750
Epoch 317/500
1/1 - 0s - loss: 0.4996 - accuracy: 0.8750
Epoch 318/500
1/1 - 0s - loss: 0.4960 - accuracy: 0.8750
Epoch 319/500
1/1 - 0s - loss: 0.4925 - accuracy: 0.8750
Epoch 320/500
1/1 - 0s - loss: 0.4890 - accuracy: 0.8750
Epoch 321/500
1/1 - 0s - loss: 0.4856 - accuracy: 0.8750
Epoch 322/500
1/1 - 0s - loss: 0.4823 - accuracy: 0.8750
Epoch 323/500
1/1 - 0s - loss: 0.4790 - accuracy: 0.8750
Epoch 324/500
1/1 - 0s - loss: 0.4758 - accuracy: 0.8750
Epoch 325/500
1/1 - 0s - loss: 0.4726 - accuracy: 0.8750
Epoch 326/500
1/1 - 0s - loss: 0.4694 - accuracy: 0.8750
Epoch 327/500
1/1 - 0s - loss: 0.4664 - accuracy: 0.8750
Epoch 328/500
1/1 - 0s - loss: 0.4633 - accuracy: 0.8750
Epoch 329/500
1/1 - 0s - loss: 0.4604 - accuracy: 0.8750
Epoch 330/500
1/1 - 0s - loss: 0.4574 - accuracy: 0.8750
Epoch 331/500
1/1 - 0s - loss: 0.4546 - accuracy: 0.8750
Epoch 332/500
1/1 - 0s - loss: 0.4517 - accuracy: 0.8750
Epoch 333/500
1/1 - 0s - loss: 0.4489 - accuracy: 0.8750
Epoch 334/500
1/1 - 0s - loss: 0.4462 - accuracy: 0.8750
Epoch 335/500
1/1 - 0s - loss: 0.4435 - accuracy: 0.8750
Epoch 336/500
1/1 - 0s - loss: 0.4409 - accuracy: 0.8750
Epoch 337/500
1/1 - 0s - loss: 0.4382 - accuracy: 0.8750
Epoch 338/500
1/1 - 0s - loss: 0.4357 - accuracy: 0.8750
Epoch 339/500
1/1 - 0s - loss: 0.4332 - accuracy: 0.8750
Epoch 340/500
1/1 - 0s - loss: 0.4307 - accuracy: 0.8750
Epoch 341/500
1/1 - 0s - loss: 0.4282 - accuracy: 0.8750
Epoch 342/500
1/1 - 0s - loss: 0.4258 - accuracy: 0.8750
Epoch 343/500
1/1 - 0s - loss: 0.4235 - accuracy: 0.8750
Epoch 344/500
1/1 - 0s - loss: 0.4211 - accuracy: 0.8750
Epoch 345/500
1/1 - 0s - loss: 0.4188 - accuracy: 0.8750
Epoch 346/500
1/1 - 0s - loss: 0.4166 - accuracy: 0.8750
Epoch 347/500
1/1 - 0s - loss: 0.4144 - accuracy: 0.8750
Epoch 348/500
1/1 - 0s - loss: 0.4122 - accuracy: 0.8750
Epoch 349/500
1/1 - 0s - loss: 0.4100 - accuracy: 0.8750
Epoch 350/500
1/1 - 0s - loss: 0.4079 - accuracy: 0.8750
Epoch 351/500
1/1 - 0s - loss: 0.4058 - accuracy: 0.8750
Epoch 352/500
1/1 - 0s - loss: 0.4037 - accuracy: 0.8750
Epoch 353/500
1/1 - 0s - loss: 0.4017 - accuracy: 0.8750
Epoch 354/500
1/1 - 0s - loss: 0.3997 - accuracy: 0.8750
Epoch 355/500
1/1 - 0s - loss: 0.3977 - accuracy: 0.8750
Epoch 356/500
1/1 - 0s - loss: 0.3958 - accuracy: 0.8750
Epoch 357/500
1/1 - 0s - loss: 0.3939 - accuracy: 0.8750
Epoch 358/500
1/1 - 0s - loss: 0.3920 - accuracy: 0.8750
Epoch 359/500
1/1 - 0s - loss: 0.3901 - accuracy: 0.8750
Epoch 360/500
1/1 - 0s - loss: 0.3883 - accuracy: 0.8750
Epoch 361/500
1/1 - 0s - loss: 0.3865 - accuracy: 0.8750
Epoch 362/500
1/1 - 0s - loss: 0.3847 - accuracy: 0.8750
Epoch 363/500
1/1 - 0s - loss: 0.3829 - accuracy: 0.8750
Epoch 364/500
1/1 - 0s - loss: 0.3812 - accuracy: 0.8750
Epoch 365/500
1/1 - 0s - loss: 0.3794 - accuracy: 0.8750
Epoch 366/500
1/1 - 0s - loss: 0.3777 - accuracy: 0.8750
Epoch 367/500
1/1 - 0s - loss: 0.3761 - accuracy: 0.8750
Epoch 368/500
1/1 - 0s - loss: 0.3744 - accuracy: 0.8750
Epoch 369/500
1/1 - 0s - loss: 0.3728 - accuracy: 0.8750
Epoch 370/500
1/1 - 0s - loss: 0.3712 - accuracy: 0.8750
Epoch 371/500
1/1 - 0s - loss: 0.3696 - accuracy: 0.8750
Epoch 372/500
1/1 - 0s - loss: 0.3680 - accuracy: 0.8750
Epoch 373/500
1/1 - 0s - loss: 0.3664 - accuracy: 0.8750
Epoch 374/500
1/1 - 0s - loss: 0.3649 - accuracy: 0.8750
Epoch 375/500
1/1 - 0s - loss: 0.3634 - accuracy: 0.8750
Epoch 376/500
1/1 - 0s - loss: 0.3619 - accuracy: 0.8750
Epoch 377/500
1/1 - 0s - loss: 0.3604 - accuracy: 0.8750
Epoch 378/500
1/1 - 0s - loss: 0.3589 - accuracy: 0.8750
Epoch 379/500
1/1 - 0s - loss: 0.3575 - accuracy: 0.8750
Epoch 380/500
1/1 - 0s - loss: 0.3560 - accuracy: 0.8750
Epoch 381/500
1/1 - 0s - loss: 0.3546 - accuracy: 0.8750
Epoch 382/500
1/1 - 0s - loss: 0.3532 - accuracy: 0.8750
Epoch 383/500
1/1 - 0s - loss: 0.3518 - accuracy: 0.8750
Epoch 384/500
1/1 - 0s - loss: 0.3504 - accuracy: 0.8750
Epoch 385/500
1/1 - 0s - loss: 0.3491 - accuracy: 0.8750
Epoch 386/500
1/1 - 0s - loss: 0.3477 - accuracy: 0.8750
Epoch 387/500
1/1 - 0s - loss: 0.3464 - accuracy: 0.8750
Epoch 388/500
1/1 - 0s - loss: 0.3450 - accuracy: 0.8750
Epoch 389/500
1/1 - 0s - loss: 0.3437 - accuracy: 0.8750
Epoch 390/500
1/1 - 0s - loss: 0.3424 - accuracy: 0.8750
Epoch 391/500
1/1 - 0s - loss: 0.3411 - accuracy: 0.8750
Epoch 392/500
1/1 - 0s - loss: 0.3399 - accuracy: 0.8750
Epoch 393/500
1/1 - 0s - loss: 0.3386 - accuracy: 0.8750
Epoch 394/500
1/1 - 0s - loss: 0.3373 - accuracy: 0.8750
Epoch 395/500
1/1 - 0s - loss: 0.3361 - accuracy: 0.8750
Epoch 396/500
1/1 - 0s - loss: 0.3348 - accuracy: 0.8750
Epoch 397/500
1/1 - 0s - loss: 0.3336 - accuracy: 0.8750
Epoch 398/500
1/1 - 0s - loss: 0.3324 - accuracy: 0.8750
Epoch 399/500
1/1 - 0s - loss: 0.3312 - accuracy: 0.8750
Epoch 400/500
1/1 - 0s - loss: 0.3300 - accuracy: 0.8750
Epoch 401/500
1/1 - 0s - loss: 0.3288 - accuracy: 0.8750
Epoch 402/500
1/1 - 0s - loss: 0.3276 - accuracy: 0.8750
Epoch 403/500
1/1 - 0s - loss: 0.3265 - accuracy: 0.8750
Epoch 404/500
1/1 - 0s - loss: 0.3253 - accuracy: 0.8750
Epoch 405/500
1/1 - 0s - loss: 0.3241 - accuracy: 0.8750
Epoch 406/500
1/1 - 0s - loss: 0.3230 - accuracy: 0.8750
Epoch 407/500
1/1 - 0s - loss: 0.3219 - accuracy: 0.8750
Epoch 408/500
1/1 - 0s - loss: 0.3207 - accuracy: 0.8750
Epoch 409/500
1/1 - 0s - loss: 0.3196 - accuracy: 0.8750
Epoch 410/500
1/1 - 0s - loss: 0.3185 - accuracy: 0.8750
Epoch 411/500
1/1 - 0s - loss: 0.3174 - accuracy: 0.8750
Epoch 412/500
1/1 - 0s - loss: 0.3163 - accuracy: 0.8750
Epoch 413/500
1/1 - 0s - loss: 0.3152 - accuracy: 0.8750
Epoch 414/500
1/1 - 0s - loss: 0.3142 - accuracy: 0.8750
Epoch 415/500
1/1 - 0s - loss: 0.3131 - accuracy: 0.8750
Epoch 416/500
1/1 - 0s - loss: 0.3120 - accuracy: 0.8750
Epoch 417/500
1/1 - 0s - loss: 0.3110 - accuracy: 0.8750
Epoch 418/500
1/1 - 0s - loss: 0.3099 - accuracy: 0.8750
Epoch 419/500
1/1 - 0s - loss: 0.3089 - accuracy: 0.8750
Epoch 420/500
1/1 - 0s - loss: 0.3078 - accuracy: 0.8750
Epoch 421/500
1/1 - 0s - loss: 0.3068 - accuracy: 0.8750
Epoch 422/500
1/1 - 0s - loss: 0.3058 - accuracy: 0.8750
Epoch 423/500
1/1 - 0s - loss: 0.3048 - accuracy: 0.8750
Epoch 424/500
1/1 - 0s - loss: 0.3038 - accuracy: 0.8750
Epoch 425/500
1/1 - 0s - loss: 0.3028 - accuracy: 0.8750
Epoch 426/500
1/1 - 0s - loss: 0.3018 - accuracy: 0.8750
Epoch 427/500
1/1 - 0s - loss: 0.3008 - accuracy: 0.8750
Epoch 428/500
1/1 - 0s - loss: 0.2998 - accuracy: 0.8750
Epoch 429/500
1/1 - 0s - loss: 0.2989 - accuracy: 0.8750
Epoch 430/500
1/1 - 0s - loss: 0.2979 - accuracy: 0.8750
Epoch 431/500
1/1 - 0s - loss: 0.2970 - accuracy: 0.8750
Epoch 432/500
1/1 - 0s - loss: 0.2960 - accuracy: 0.8750
Epoch 433/500
1/1 - 0s - loss: 0.2951 - accuracy: 0.8750
Epoch 434/500
1/1 - 0s - loss: 0.2942 - accuracy: 0.8750
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 435/500
1/1 - 0s - loss: 0.2932 - accuracy: 0.8750
Epoch 436/500
1/1 - 0s - loss: 0.2923 - accuracy: 0.8750
Epoch 437/500
1/1 - 0s - loss: 0.2914 - accuracy: 0.8750
Epoch 438/500
1/1 - 0s - loss: 0.2905 - accuracy: 0.8750
Epoch 439/500
1/1 - 0s - loss: 0.2896 - accuracy: 0.8750
Epoch 440/500
1/1 - 0s - loss: 0.2888 - accuracy: 0.8750
Epoch 441/500
1/1 - 0s - loss: 0.2879 - accuracy: 0.8750
Epoch 442/500
1/1 - 0s - loss: 0.2870 - accuracy: 0.8750
Epoch 443/500
1/1 - 0s - loss: 0.2862 - accuracy: 0.8750
Epoch 444/500
1/1 - 0s - loss: 0.2853 - accuracy: 0.8750
Epoch 445/500
1/1 - 0s - loss: 0.2845 - accuracy: 0.8750
Epoch 446/500
1/1 - 0s - loss: 0.2836 - accuracy: 0.8750
Epoch 447/500
1/1 - 0s - loss: 0.2828 - accuracy: 0.8750
Epoch 448/500
1/1 - 0s - loss: 0.2820 - accuracy: 0.8750
Epoch 449/500
1/1 - 0s - loss: 0.2812 - accuracy: 0.8750
Epoch 450/500
1/1 - 0s - loss: 0.2804 - accuracy: 0.8750
Epoch 451/500
1/1 - 0s - loss: 0.2796 - accuracy: 0.8750
Epoch 452/500
1/1 - 0s - loss: 0.2788 - accuracy: 0.8750
Epoch 453/500
1/1 - 0s - loss: 0.2780 - accuracy: 0.8750
Epoch 454/500
1/1 - 0s - loss: 0.2772 - accuracy: 0.8750
Epoch 455/500
1/1 - 0s - loss: 0.2765 - accuracy: 0.8750
Epoch 456/500
1/1 - 0s - loss: 0.2757 - accuracy: 0.8750
Epoch 457/500
1/1 - 0s - loss: 0.2750 - accuracy: 0.8750
Epoch 458/500
1/1 - 0s - loss: 0.2742 - accuracy: 0.8750
Epoch 459/500
1/1 - 0s - loss: 0.2735 - accuracy: 0.8750
Epoch 460/500
1/1 - 0s - loss: 0.2728 - accuracy: 0.8750
Epoch 461/500
1/1 - 0s - loss: 0.2721 - accuracy: 0.8750
Epoch 462/500
1/1 - 0s - loss: 0.2714 - accuracy: 0.8750
Epoch 463/500
1/1 - 0s - loss: 0.2707 - accuracy: 0.8750
Epoch 464/500
1/1 - 0s - loss: 0.2700 - accuracy: 0.8750
Epoch 465/500
1/1 - 0s - loss: 0.2693 - accuracy: 0.8750
Epoch 466/500
1/1 - 0s - loss: 0.2686 - accuracy: 0.8750
Epoch 467/500
1/1 - 0s - loss: 0.2679 - accuracy: 0.8750
Epoch 468/500
1/1 - 0s - loss: 0.2673 - accuracy: 0.8750
Epoch 469/500
1/1 - 0s - loss: 0.2666 - accuracy: 0.8750
Epoch 470/500
1/1 - 0s - loss: 0.2660 - accuracy: 0.8750
Epoch 471/500
1/1 - 0s - loss: 0.2653 - accuracy: 0.8750
Epoch 472/500
1/1 - 0s - loss: 0.2647 - accuracy: 0.8750
Epoch 473/500
1/1 - 0s - loss: 0.2641 - accuracy: 0.8750
Epoch 474/500
1/1 - 0s - loss: 0.2635 - accuracy: 0.8750
Epoch 475/500
1/1 - 0s - loss: 0.2629 - accuracy: 0.8750
Epoch 476/500
1/1 - 0s - loss: 0.2623 - accuracy: 0.8750
Epoch 477/500
1/1 - 0s - loss: 0.2617 - accuracy: 0.8750
Epoch 478/500
1/1 - 0s - loss: 0.2611 - accuracy: 0.8750
Epoch 479/500
1/1 - 0s - loss: 0.2605 - accuracy: 0.8750
Epoch 480/500
1/1 - 0s - loss: 0.2599 - accuracy: 0.8750
Epoch 481/500
1/1 - 0s - loss: 0.2594 - accuracy: 0.8750
Epoch 482/500
1/1 - 0s - loss: 0.2588 - accuracy: 0.8750
Epoch 483/500
1/1 - 0s - loss: 0.2583 - accuracy: 0.8750
Epoch 484/500
1/1 - 0s - loss: 0.2577 - accuracy: 0.8750
Epoch 485/500
1/1 - 0s - loss: 0.2572 - accuracy: 0.8750
Epoch 486/500
1/1 - 0s - loss: 0.2567 - accuracy: 0.8750
Epoch 487/500
1/1 - 0s - loss: 0.2561 - accuracy: 0.8750
Epoch 488/500
1/1 - 0s - loss: 0.2556 - accuracy: 0.8750
Epoch 489/500
1/1 - 0s - loss: 0.2551 - accuracy: 0.8750
Epoch 490/500
1/1 - 0s - loss: 0.2546 - accuracy: 0.8750
Epoch 491/500
1/1 - 0s - loss: 0.2541 - accuracy: 0.8750
Epoch 492/500
1/1 - 0s - loss: 0.2536 - accuracy: 0.8750
Epoch 493/500
1/1 - 0s - loss: 0.2531 - accuracy: 0.8750
Epoch 494/500
1/1 - 0s - loss: 0.2526 - accuracy: 0.8750
Epoch 495/500
1/1 - 0s - loss: 0.2522 - accuracy: 0.8750
Epoch 496/500
1/1 - 0s - loss: 0.2517 - accuracy: 0.8750
Epoch 497/500
1/1 - 0s - loss: 0.2512 - accuracy: 0.8750
Epoch 498/500
1/1 - 0s - loss: 0.2508 - accuracy: 0.8750
Epoch 499/500
1/1 - 0s - loss: 0.2503 - accuracy: 0.8750
Epoch 500/500
1/1 - 0s - loss: 0.2499 - accuracy: 0.8750
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x7fe772ea72e8&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-language-model-primer_22_0.png" src="../_images/dl-neural-language-model-primer_22_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a sequence from the model</span>
<span class="k">def</span> <span class="nf">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
    <span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">seed_text</span>
    <span class="c1"># generate a fixed number of words</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="c1"># encode the text as integer</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">in_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="c1"># predict a word in the vocabulary</span>
        <span class="c1">#yhat = model.predict_classes(encoded, verbose=0)</span>
        <span class="n">yhat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># map predicted word index to word</span>
        <span class="n">out_word</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">:</span>
                <span class="n">out_word</span> <span class="o">=</span> <span class="n">word</span>
                <span class="k">break</span>
        <span class="c1"># append to input</span>
        <span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">out_word</span><span class="p">,</span> <span class="n">result</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">out_word</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When we generate the output sequence, we use a <strong>greedy search</strong>, which selects the most likely word at each time step in the output sequence. While this approach features its efficiency, the quality of the final output sequences may not be necessarily optimal.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s1">&#39;Jill&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jill went up the hill to
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="beam-search">
<h2>Beam Search<a class="headerlink" href="#beam-search" title="Permalink to this headline">¶</a></h2>
<div class="section" id="searching-in-nlp">
<h3>Searching in NLP<a class="headerlink" href="#searching-in-nlp" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In the previous demonstration, when we generate the predicted next word, we adopt a naive approach, i.e., always choosing the word of the highest probability.</p></li>
<li><p>It is common in NLP for models to output a probability distribution over words in the vocabulary.</p></li>
<li><p>This step involves searching through all the possible output sequences based on their likelihood.</p></li>
<li><p>Choosing the next word of highest probability does not guarantee us the most optimal sequence.</p></li>
<li><p>The search problem is exponential in the length of the output sequence given the large size of vocabulary.</p></li>
</ul>
</div>
<div class="section" id="beam-search-decoding">
<h3>Beam Search Decoding<a class="headerlink" href="#beam-search-decoding" title="Permalink to this headline">¶</a></h3>
<p>The beam search expands all possible next steps and keeps the <strong><span class="math notranslate nohighlight">\(k\)</span></strong> most likely, where <strong><span class="math notranslate nohighlight">\(k\)</span></strong> is a researcher-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.</p>
<p>The search process can stop for each candidate independently either by:</p>
<ul class="simple">
<li><p>reaching a maximum length</p></li>
<li><p>reaching an end-of-sequence token</p></li>
<li><p>reaching a threshold likelihood</p></li>
</ul>
<div class="highlight-{note] notranslate"><div class="highlight"><pre><span></span>Please see Jason Brownlee&#39;s blog post [How to Implement a Beam Search Decoder for Natural Language Processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/) for the python implementation.

The following codes are based on Jason&#39;s code.
</pre></div>
</div>
<p>Not sure if the following code works ….Needs checking.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a sequence from the model</span>
<span class="k">def</span> <span class="nf">generate_seq_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">in_text</span> <span class="o">=</span> <span class="n">seed_text</span> 
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[[[</span><span class="n">in_text</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">]]</span>
    <span class="c1"># prepare id_2_word map</span>
    <span class="n">id_2_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">([(</span><span class="nb">id</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">id</span><span class="p">)</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
    
    <span class="c1"># start next-word generating</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="n">all_candidates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>        
        <span class="c1">#print(&quot;Next word &quot;, _+1)</span>
        <span class="c1"># temp list to hold all possible candidates</span>
        <span class="c1"># `sequence + next words`</span>

        
        <span class="c1"># for each existing sentence</span>
        <span class="c1"># take the last word of the sequence</span>
        <span class="c1"># find probs of all words in the next position</span>
        <span class="c1"># save the top k</span>
        <span class="c1"># all_candidates should have 3 * 22 = 66 candidates</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)):</span>
            <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># next word probablity distribution</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
            <span class="n">model_pred_prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            
            <span class="c1"># try each of `sequence + nextword`</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_pred_prob</span><span class="p">)):</span>
                <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">id_2_word</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">score</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_pred_prob</span><span class="p">[</span><span class="n">j</span><span class="p">])]</span>
                <span class="n">all_candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>
            
            <span class="c1"># order all candidates (seqence + nextword) by score</span>
            <span class="c1">#print(&quot;all_condidates length:&quot;, len(all_candidates))</span>
            <span class="n">ordered</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_candidates</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># default ascending</span>
            <span class="c1"># select k best</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="n">ordered</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="c1">## choose top k</span>
    <span class="k">return</span> <span class="n">sequences</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_seq_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s1">&#39;Jill&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[[&#39;Jill&#39;, &#39;up&#39;, &#39;hill&#39;, &#39;fetch&#39;, &#39;pail&#39;, &#39;water&#39;], 1.0160576105117798],
 [[&#39;Jill&#39;, &#39;up&#39;, &#39;hill&#39;, &#39;fetch&#39;, &#39;pail&#39;, &#39;broke&#39;], 5.479756973683834],
 [[&#39;Jill&#39;, &#39;up&#39;, &#39;hill&#39;, &#39;fetch&#39;, &#39;pail&#39;, &#39;broke&#39;, &#39;crown&#39;], 5.51508317515254],
 [[&#39;Jill&#39;, &#39;up&#39;, &#39;hill&#39;, &#39;fetch&#39;, &#39;pail&#39;, &#39;broke&#39;, &#39;crown&#39;, &#39;jack&#39;],
  5.551574371755123],
 [[&#39;Jill&#39;, &#39;up&#39;, &#39;hill&#39;, &#39;fetch&#39;, &#39;pail&#39;, &#39;crown&#39;], 5.742704056203365]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># topk_ind = np.argpartition(model_pred_prob, -k, axis=None)[-k:]</span>
<span class="c1">#         topk_prob = model_pred_prob[topk_ind]</span>
<span class="c1">#         topk_word = [id_2_word.get(id) for id in topk_ind]</span>

<span class="c1">#     print(topk_prob[np.argsort(-topk_prob)])</span>
<span class="c1">#     print(topk_ind[np.argsort(-topk_prob)])</span>
<span class="c1"># np.argmax(model.predict(encoded))</span>

<span class="c1"># yhat=np.argmax(model.predict(encoded), axis=-1) # in `np.argmax`, `axis = -1` uses flatten array</span>
<span class="c1">#         # map predicted word index to word</span>
<span class="c1"># out_word = &#39;&#39;</span>
<span class="c1"># for word, index in tokenizer.word_index.items():</span>
<span class="c1">#     if index == yhat:</span>
<span class="c1">#         out_word = word</span>
<span class="c1">#         break</span>
<span class="c1"># print(out_word)</span>
<span class="c1"># model_pred_prob = model.predict(encoded).flatten()</span>
<span class="c1"># topk_ind = np.argpartition(model_pred_prob, -k, axis=None)[-k:]</span>
<span class="c1"># print(topk_ind)</span>

<span class="c1"># topk_prob = model_pred_prob[topk_ind]</span>
<span class="c1"># print(topk_prob)</span>
<span class="c1"># print(topk_prob[np.argsort(-topk_prob)])</span>
<span class="c1"># print(topk_ind[np.argsort(-topk_prob)])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>This tutorial is based on Jason Brownlee’s blog post <a class="reference external" href="https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/">How to Develop Word-Based Neural Language Models in Python with Keras</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./temp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>