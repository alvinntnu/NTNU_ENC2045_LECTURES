
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sequence Model with Attention for Addition Learning &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transformers: Intuition" href="dl-transformers-intuition.html" />
    <link rel="prev" title="Attention: Intuition" href="dl-seq-to-seq-types.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../nlp/text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../nlp/google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-algorithm.html">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-neural-network-from-scratch.html">
   1. Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-simple-case.html">
   2. Deep Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-sentiment-case.html">
   3. Deep Learning: Sentiment Analysis
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neural Language Model and Embeddings
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-sequence-models-intuition.html">
   1. Sequence Models Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/dl-neural-language-model-primer.html">
   2. Neural Language Model: A Start
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp/text-vec-embedding.html">
   3. Word Embeddings
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Seq2Seq, Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-seq-to-seq-types.html">
   Attention: Intuition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Sequence Model with Attention for Addition Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-transformers-intuition.html">
   Transformers: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-transformers-keras.html">
   Text Classification with Transformer
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   Assignment III: Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   Assignment IV: Chinese Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   Assignment V: Text Vectorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/6-machine-learning.html">
   Assignment VI: Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/midterm-exam.html">
   Midterm Exam
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/7-topic-modeling.html">
   Assignment VII: Topic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/8-dl-chinese-name-gender.html">
   Assignment VIII: Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/9-sentiment-analysis-dl.html">
   Assignment IX: Sentiment Analysis Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/10-neural-language-model.html">
   Assignment X: Neural Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/11-word2vec.html">
   Assignment XI: Word Embeddings
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/temp/dl-seq-to-seq-attention-addition.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/temp/dl-seq-to-seq-attention-addition.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/temp/dl-seq-to-seq-attention-addition.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#set-up-dependencies">
   Set up Dependencies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning-hyperparameters">
   Deep Learning Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preprocessing">
   Data Preprocessing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-test-split">
   Train-Test Split
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Data Preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-to-sequences">
     Text to Sequences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequences-to-one-hot-encoding">
     Sequences to One-Hot Encoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#token-indices">
     Token Indices
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-model-architecture">
   Define Model Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-1-vanilla-rnn">
   Model 1 (Vanilla RNN)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-model">
     Define Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-2-gru">
   Model 2 (GRU)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Define Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-3-birdirectional">
   Model 3 (Birdirectional)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Define Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-4-peeky-decoder">
   Model 4 (Peeky Decoder)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Define Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-5-attention">
   Model 5 (Attention)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     Define Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#save-models">
   Save Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interim-comparison">
   Interim Comparison
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-model-analysis">
   Attention Model Analysis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-encoder">
     Inference Encoder
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-decoder">
     Inference Decoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plotting-attention">
   Plotting Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-on-testing-data">
   Evaluation on Testing Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="sequence-model-with-attention-for-addition-learning">
<h1>Sequence Model with Attention for Addition Learning<a class="headerlink" href="#sequence-model-with-attention-for-addition-learning" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>In this lecture note, we demonstrate a simple sequence-to-sequence model for addition learning.</p></li>
<li><p>The inputs are sequences of two numbers adding together; the outputs are the correct answers, i.e., the sum of the two numbers.</p></li>
<li><p>This task is to simulate the machine translation task.</p></li>
<li><p>In particular, we demonstrate the uses of two types of Attention layers from <code class="docutils literal notranslate"><span class="pre">tensorflow.keras.layers</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Attention</span></code> (Luong attention)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AdditiveAttention</span></code> (Bahdanau attention)</p></li>
</ul>
</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul class="simple">
<li><p>Please update <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> to the most recent version (v2+).</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">tensorflow-nightly</span></code>, <code class="docutils literal notranslate"><span class="pre">MultiheadedAttention</span></code> is available.</p></li>
</ul>
</div>
<div class="section" id="set-up-dependencies">
<h2>Set up Dependencies<a class="headerlink" href="#set-up-dependencies" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">tensorflow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span><span class="p">,</span> <span class="n">SimpleRNN</span><span class="p">,</span> <span class="n">Bidirectional</span><span class="p">,</span> <span class="n">Concatenate</span><span class="p">,</span> <span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">AdditiveAttention</span><span class="p">,</span> <span class="n">GlobalAveragePooling1D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tensorflow Version: &#39;</span><span class="p">,</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensorflow Version:  2.4.1
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="deep-learning-hyperparameters">
<h2>Deep Learning Hyperparameters<a class="headerlink" href="#deep-learning-hyperparameters" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># Batch size for training.</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of epochs to train for.</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># Latent dimensionality of the encoding space.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-preprocessing">
<h2>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The dataset is from the book, Deep Learning 2: 用Python進行自然語言處理的理論實作.</p></li>
<li><p>Several preprocessing steps have been done by the data provider:</p>
<ul>
<li><p>All the input sequences have been padded to unigram lengths.</p></li>
<li><p>All the target sequences have been added with sequence-initial and sequence-ending characters <code class="docutils literal notranslate"><span class="pre">_</span></code>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;../../../RepositoryData/data/deep-learning-2/addition.txt&#39;</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
<span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>

<span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">sent</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">target_texts</span><span class="p">]</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_texts</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">inds</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">input_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Data Size:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_texts</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;16+75  &#39;, &#39;52+607 &#39;, &#39;75+22  &#39;, &#39;63+22  &#39;, &#39;795+3  &#39;]
[&#39;_91_&#39;, &#39;_659_&#39;, &#39;_97_&#39;, &#39;_85_&#39;, &#39;_798_&#39;]
Data Size: 50001
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-test-split">
<h2>Train-Test Split<a class="headerlink" href="#train-test-split" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_test_ratio</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_test_ratio</span><span class="p">))</span>
<span class="n">train_inds</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>
<span class="n">test_inds</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>

<span class="n">tr_input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">train_inds</span><span class="p">]</span>
<span class="n">tr_target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">train_inds</span><span class="p">]</span>

<span class="n">ts_input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">test_inds</span><span class="p">]</span>
<span class="n">ts_target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_texts</span><span class="p">[</span><span class="n">ti</span><span class="p">]</span> <span class="k">for</span> <span class="n">ti</span> <span class="ow">in</span> <span class="n">test_inds</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr_input_texts</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;27+673 &#39;,
 &#39;153+27 &#39;,
 &#39;93+901 &#39;,
 &#39;243+678&#39;,
 &#39;269+46 &#39;,
 &#39;235+891&#39;,
 &#39;46+290 &#39;,
 &#39;324+947&#39;,
 &#39;721+49 &#39;,
 &#39;535+7  &#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr_target_texts</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;_700_&#39;,
 &#39;_180_&#39;,
 &#39;_994_&#39;,
 &#39;_921_&#39;,
 &#39;_315_&#39;,
 &#39;_1126_&#39;,
 &#39;_336_&#39;,
 &#39;_1271_&#39;,
 &#39;_770_&#39;,
 &#39;_542_&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of Samples:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of Samples in Training:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of Samples in Testing:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ts_input_texts</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of Samples: 50001
Number of Samples in Training: 45001
Number of Samples in Testing: 5000
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2>Data Preprocessing<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/text-seq-onehot-embedding.jpeg" /></p>
<div class="section" id="text-to-sequences">
<h3>Text to Sequences<a class="headerlink" href="#text-to-sequences" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Tokenization of input and target texts invovles the following important steps:</p>
<ul>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code></p></li>
<li><p>Fit the <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> on the training sets</p></li>
<li><p>Tokenize input and target texts of the training set into sequences</p></li>
<li><p>Identify the maxlen of the input and target texts (Redundant for this dataset)</p></li>
<li><p>Pad input and text sequences to uniform lengths (Redundant for this dataset)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># &quot;&quot;&quot; Defining tokenizers &quot;&quot;&quot;</span>
<span class="n">input_tokenizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="n">oov_token</span><span class="o">=</span><span class="s1">&#39;UNK&#39;</span><span class="p">,</span>
                                                     <span class="n">char_level</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">input_tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">)</span>
<span class="n">encoder_input_sequences</span> <span class="o">=</span> <span class="n">input_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">tr_input_texts</span><span class="p">)</span>
<span class="n">input_maxlen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">encoder_input_sequences</span><span class="p">])</span>
<span class="n">encoder_input_sequences</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span>
                                        <span class="n">maxlen</span><span class="o">=</span><span class="n">input_maxlen</span><span class="p">)</span>

<span class="n">target_tokenizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="n">oov_token</span><span class="o">=</span><span class="s1">&#39;UNK&#39;</span><span class="p">,</span>
                                                      <span class="n">char_level</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target_tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">tr_target_texts</span><span class="p">)</span>
<span class="n">target_sequences</span> <span class="o">=</span> <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">tr_target_texts</span><span class="p">)</span>
<span class="n">target_maxlen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">target_sequences</span><span class="p">])</span>
<span class="n">target_sequences</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">,</span>
                                 <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span>
                                 <span class="n">maxlen</span><span class="o">=</span><span class="n">target_maxlen</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Shapes of Input and Target Sequences</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(45001, 7)
(45001, 6)
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The plus 1 for vocabulary size is to include the padding character, whose index is the reserved <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ### vocab size</span>
<span class="n">input_vsize</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">input_tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">target_vsize</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">target_tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">input_vsize</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14
13
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr_input_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;27+673 &#39;, &#39;153+27 &#39;, &#39;93+901 &#39;, &#39;243+678&#39;, &#39;269+46 &#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoder_input_sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[11, 12,  3,  9, 12,  7,  2],
       [10,  8,  7,  3, 11, 12,  2],
       [ 4,  7,  3,  4, 13, 10,  2],
       [11,  5,  7,  3,  9, 12,  6],
       [11,  9,  4,  3,  5,  9,  2]], dtype=int32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr_target_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;_700_&#39;, &#39;_180_&#39;, &#39;_994_&#39;, &#39;_921_&#39;, &#39;_315_&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2,  5, 12, 12,  2,  0],
       [ 2,  3, 11, 12,  2,  0],
       [ 2,  8,  8,  9,  2,  0],
       [ 2,  8,  4,  3,  2,  0],
       [ 2,  7,  3, 10,  2,  0]], dtype=int32)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Input and Output Sequences for Decoder</p>
<ul>
<li><p>Decoder input and output sequences have one timestep difference.</p></li>
<li><p>We create decoder input and output sequences as different sets of data.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">decoder_input_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">),</span> <span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">decoder_output_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">),</span> <span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">decoder_input_sequences</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">decoder_output_sequences</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">decoder_input_sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2.,  5., 12., 12.,  2.],
       [ 2.,  3., 11., 12.,  2.],
       [ 2.,  8.,  8.,  9.,  2.],
       [ 2.,  8.,  4.,  3.,  2.],
       [ 2.,  7.,  3., 10.,  2.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">decoder_output_sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 5., 12., 12.,  2.,  0.],
       [ 3., 11., 12.,  2.,  0.],
       [ 8.,  8.,  9.,  2.,  0.],
       [ 8.,  4.,  3.,  2.,  0.],
       [ 7.,  3., 10.,  2.,  0.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sequences-to-one-hot-encoding">
<h3>Sequences to One-Hot Encoding<a class="headerlink" href="#sequences-to-one-hot-encoding" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>This step converts the sequence token into a one-hot encoding.</p></li>
<li><p>This step would render the text representation of the entire training data from 2D (batch_size, sequence_length) to 3D tensors (batch_size, sequence_length, vocab_size).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For neural machine translations, the vocabulary sizes of the input and target languages are usually very large. It is more effective to implement an <strong>Embedding</strong> layer to convert sequences (integers) into embeddings, rather than one-hot encodings.</p>
<p>For this tutorial, we have a limited vocabulary size (only digits and math symbols). One-hot encodings should be fine.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_input_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_output_sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(45001, 7)
(45001, 5)
(45001, 5)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoder_input_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">encoder_input_sequences</span><span class="p">,</span>
                                      <span class="n">num_classes</span><span class="o">=</span><span class="n">input_vsize</span><span class="p">)</span>
<span class="n">decoder_input_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">decoder_input_sequences</span><span class="p">,</span>
                                      <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">)</span>
<span class="n">decoder_output_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">decoder_output_sequences</span><span class="p">,</span>
                                       <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoder_input_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_input_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_output_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(45001, 7, 14)
(45001, 5, 13)
(45001, 5, 13)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="token-indices">
<h3>Token Indices<a class="headerlink" href="#token-indices" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Index2word &quot;&quot;&quot;</span>
<span class="n">enc_index2word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">input_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
        <span class="n">input_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">dec_index2word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">target_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
        <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">enc_index2word</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;UNK&#39;,
 2: &#39; &#39;,
 3: &#39;+&#39;,
 4: &#39;9&#39;,
 5: &#39;4&#39;,
 6: &#39;8&#39;,
 7: &#39;3&#39;,
 8: &#39;5&#39;,
 9: &#39;6&#39;,
 10: &#39;1&#39;,
 11: &#39;2&#39;,
 12: &#39;7&#39;,
 13: &#39;0&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dec_index2word</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;UNK&#39;,
 2: &#39;_&#39;,
 3: &#39;1&#39;,
 4: &#39;2&#39;,
 5: &#39;7&#39;,
 6: &#39;6&#39;,
 7: &#39;3&#39;,
 8: &#39;9&#39;,
 9: &#39;4&#39;,
 10: &#39;5&#39;,
 11: &#39;8&#39;,
 12: &#39;0&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">150</span>


<span class="c1"># Plotting results</span>
<span class="k">def</span> <span class="nf">plot1</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">## Accuracy plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training acc&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation acc&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="c1">## Loss plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot2</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#plt.gca().set_ylim(0,1)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="define-model-architecture">
<h2>Define Model Architecture<a class="headerlink" href="#define-model-architecture" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Sequence-to-Sequence can go from simple RNNs to complex models with attention mechanisms.</p></li>
<li><p>In this tutorial, we will try the following:</p>
<ul>
<li><p>Sequence-to-sequence model with vanila RNN Encoder and Decoder</p></li>
<li><p>Sequence-to-sequence model with bidirectional RNN Encoder</p></li>
<li><p>Sequence-to-sequence model with peeky Decoder</p></li>
<li><p>Sequence-to-sequence model with attention-based Decoder</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="model-1-vanilla-rnn">
<h2>Model 1 (Vanilla RNN)<a class="headerlink" href="#model-1-vanilla-rnn" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/seq2seq-vanilla-rnn.jpeg" /></p>
<div class="section" id="define-model">
<h3>Define Model<a class="headerlink" href="#define-model" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In the training stage, we feed the decoder the correct answer at each timestep as the input sequence.</p></li>
<li><p>In the real testing stage, the decoder will take the hidden state from the previous timestep as the input sequence.</p></li>
<li><p>This type of training is referred to as <strong>teacher forcing</strong> learning strategy. This can help the model converge more effectively..</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder RNN</span>
<span class="n">encoder_rnn</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_rnn&#39;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state</span> <span class="o">=</span> <span class="n">encoder_rnn</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>
<span class="c1">## first return is the hidden states of all timesteps of encoder</span>
<span class="c1">## second return is the last hidden state of encoder</span>

<span class="c1"># Decoder RNN</span>
<span class="c1">## using `encoder_state` (last h) as initial state.</span>
<span class="c1">## using `decoder_inputs` for teacher forcing learning</span>
<span class="n">decoder_rnn</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_rnn&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_rnn</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model1</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_inputs (InputLayer)     [(None, 7, 14)]      0                                            
__________________________________________________________________________________________________
decoder_inputs (InputLayer)     [(None, 5, 13)]      0                                            
__________________________________________________________________________________________________
encoder_rnn (SimpleRNN)         [(None, 7, 256), (No 69376       encoder_inputs[0][0]             
__________________________________________________________________________________________________
decoder_rnn (SimpleRNN)         [(None, 5, 256), (No 69120       decoder_inputs[0][0]             
                                                                 encoder_rnn[0][1]                
__________________________________________________________________________________________________
time_distributed_layer (TimeDis (None, 5, 13)        3341        decoder_rnn[0][0]                
==================================================================================================
Total params: 141,837
Trainable params: 141,837
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model1</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_51_0.png" src="../_images/dl-seq-to-seq-attention-addition_51_0.png" />
</div>
</div>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model1</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history1</span> <span class="o">=</span> <span class="n">full_model1</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
141/141 [==============================] - 5s 30ms/step - loss: 1.5346 - accuracy: 0.4536 - val_loss: 1.2511 - val_accuracy: 0.5328
Epoch 2/20
141/141 [==============================] - 3s 22ms/step - loss: 1.2215 - accuracy: 0.5443 - val_loss: 1.1284 - val_accuracy: 0.5796
Epoch 3/20
141/141 [==============================] - 3s 22ms/step - loss: 1.0938 - accuracy: 0.5906 - val_loss: 1.0140 - val_accuracy: 0.6199
Epoch 4/20
141/141 [==============================] - 3s 21ms/step - loss: 0.9713 - accuracy: 0.6332 - val_loss: 0.8867 - val_accuracy: 0.6604
Epoch 5/20
141/141 [==============================] - 3s 22ms/step - loss: 0.8386 - accuracy: 0.6816 - val_loss: 0.7511 - val_accuracy: 0.7111
Epoch 6/20
141/141 [==============================] - 3s 22ms/step - loss: 0.7097 - accuracy: 0.7300 - val_loss: 0.6580 - val_accuracy: 0.7444
Epoch 7/20
141/141 [==============================] - 3s 22ms/step - loss: 0.6139 - accuracy: 0.7668 - val_loss: 0.5771 - val_accuracy: 0.7731
Epoch 8/20
141/141 [==============================] - 3s 22ms/step - loss: 0.5163 - accuracy: 0.8074 - val_loss: 0.4854 - val_accuracy: 0.8148
Epoch 9/20
141/141 [==============================] - 3s 21ms/step - loss: 0.4365 - accuracy: 0.8418 - val_loss: 0.4171 - val_accuracy: 0.8436
Epoch 10/20
141/141 [==============================] - 3s 21ms/step - loss: 0.3784 - accuracy: 0.8626 - val_loss: 0.3735 - val_accuracy: 0.8556
Epoch 11/20
141/141 [==============================] - 3s 21ms/step - loss: 0.3232 - accuracy: 0.8812 - val_loss: 0.3220 - val_accuracy: 0.8764
Epoch 12/20
141/141 [==============================] - 3s 22ms/step - loss: 0.2785 - accuracy: 0.8988 - val_loss: 0.2879 - val_accuracy: 0.8914
Epoch 13/20
141/141 [==============================] - 3s 22ms/step - loss: 0.2449 - accuracy: 0.9133 - val_loss: 0.2563 - val_accuracy: 0.9048
Epoch 14/20
141/141 [==============================] - 3s 22ms/step - loss: 0.2106 - accuracy: 0.9276 - val_loss: 0.2578 - val_accuracy: 0.9005
Epoch 15/20
141/141 [==============================] - 3s 22ms/step - loss: 0.1898 - accuracy: 0.9356 - val_loss: 0.2238 - val_accuracy: 0.9168
Epoch 16/20
141/141 [==============================] - 3s 22ms/step - loss: 0.1754 - accuracy: 0.9399 - val_loss: 0.1923 - val_accuracy: 0.9298
Epoch 17/20
141/141 [==============================] - 3s 21ms/step - loss: 0.1557 - accuracy: 0.9480 - val_loss: 0.1847 - val_accuracy: 0.9311
Epoch 18/20
141/141 [==============================] - 3s 22ms/step - loss: 0.1402 - accuracy: 0.9538 - val_loss: 0.1802 - val_accuracy: 0.9325
Epoch 19/20
141/141 [==============================] - 3s 22ms/step - loss: 0.1262 - accuracy: 0.9589 - val_loss: 0.1734 - val_accuracy: 0.9351
Epoch 20/20
141/141 [==============================] - 3s 22ms/step - loss: 0.1197 - accuracy: 0.9600 - val_loss: 0.1621 - val_accuracy: 0.9401
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-2-gru">
<h2>Model 2 (GRU)<a class="headerlink" href="#model-2-gru" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In Model 2, we replace vanilla RNN with GRU, which deals with the issue of long-distance dependencies between sequences.</p></li>
</ul>
<div class="section" id="id2">
<h3>Define Model<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Decoder GRU, using `encoder_state` (last h) as initial state.</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model2</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_inputs (InputLayer)     [(None, 7, 14)]      0                                            
__________________________________________________________________________________________________
decoder_inputs (InputLayer)     [(None, 5, 13)]      0                                            
__________________________________________________________________________________________________
encoder_gru (GRU)               [(None, 7, 256), (No 208896      encoder_inputs[0][0]             
__________________________________________________________________________________________________
decoder_gru (GRU)               [(None, 5, 256), (No 208128      decoder_inputs[0][0]             
                                                                 encoder_gru[0][1]                
__________________________________________________________________________________________________
time_distributed_layer (TimeDis (None, 5, 13)        3341        decoder_gru[0][0]                
==================================================================================================
Total params: 420,365
Trainable params: 420,365
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model2</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_58_0.png" src="../_images/dl-seq-to-seq-attention-addition_58_0.png" />
</div>
</div>
</div>
<div class="section" id="id3">
<h3>Training<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history2</span> <span class="o">=</span> <span class="n">full_model2</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
141/141 [==============================] - 14s 75ms/step - loss: 1.8559 - accuracy: 0.3679 - val_loss: 1.4187 - val_accuracy: 0.4730
Epoch 2/20
141/141 [==============================] - 10s 69ms/step - loss: 1.3998 - accuracy: 0.4763 - val_loss: 1.3718 - val_accuracy: 0.4817
Epoch 3/20
141/141 [==============================] - 10s 69ms/step - loss: 1.3620 - accuracy: 0.4856 - val_loss: 1.2945 - val_accuracy: 0.5063
Epoch 4/20
141/141 [==============================] - 10s 73ms/step - loss: 1.2671 - accuracy: 0.5196 - val_loss: 1.1589 - val_accuracy: 0.5615
Epoch 5/20
141/141 [==============================] - 11s 76ms/step - loss: 1.1301 - accuracy: 0.5712 - val_loss: 1.0611 - val_accuracy: 0.5915
Epoch 6/20
141/141 [==============================] - 10s 73ms/step - loss: 1.0370 - accuracy: 0.6047 - val_loss: 0.9906 - val_accuracy: 0.6226
Epoch 7/20
141/141 [==============================] - 10s 74ms/step - loss: 0.9763 - accuracy: 0.6299 - val_loss: 0.9368 - val_accuracy: 0.6464
Epoch 8/20
141/141 [==============================] - 11s 76ms/step - loss: 0.9172 - accuracy: 0.6543 - val_loss: 0.8715 - val_accuracy: 0.6670
Epoch 9/20
141/141 [==============================] - 11s 77ms/step - loss: 0.8471 - accuracy: 0.6793 - val_loss: 0.8013 - val_accuracy: 0.7001
Epoch 10/20
141/141 [==============================] - 11s 77ms/step - loss: 0.7872 - accuracy: 0.7039 - val_loss: 0.7564 - val_accuracy: 0.7163
Epoch 11/20
141/141 [==============================] - 10s 73ms/step - loss: 0.7365 - accuracy: 0.7261 - val_loss: 0.7114 - val_accuracy: 0.7344
Epoch 12/20
141/141 [==============================] - 11s 75ms/step - loss: 0.7002 - accuracy: 0.7402 - val_loss: 0.6816 - val_accuracy: 0.7429
Epoch 13/20
141/141 [==============================] - 10s 74ms/step - loss: 0.6704 - accuracy: 0.7496 - val_loss: 0.6684 - val_accuracy: 0.7414
Epoch 14/20
141/141 [==============================] - 11s 77ms/step - loss: 0.6410 - accuracy: 0.7597 - val_loss: 0.6540 - val_accuracy: 0.7429
Epoch 15/20
141/141 [==============================] - 11s 75ms/step - loss: 0.6186 - accuracy: 0.7655 - val_loss: 0.6089 - val_accuracy: 0.7640
Epoch 16/20
141/141 [==============================] - 11s 75ms/step - loss: 0.5984 - accuracy: 0.7736 - val_loss: 0.5958 - val_accuracy: 0.7677
Epoch 17/20
141/141 [==============================] - 11s 77ms/step - loss: 0.5708 - accuracy: 0.7827 - val_loss: 0.5853 - val_accuracy: 0.7696
Epoch 18/20
141/141 [==============================] - 11s 75ms/step - loss: 0.5494 - accuracy: 0.7914 - val_loss: 0.5962 - val_accuracy: 0.7580
Epoch 19/20
141/141 [==============================] - 11s 80ms/step - loss: 0.5425 - accuracy: 0.7908 - val_loss: 0.5496 - val_accuracy: 0.7813
Epoch 20/20
141/141 [==============================] - 11s 81ms/step - loss: 0.5314 - accuracy: 0.7950 - val_loss: 0.5245 - val_accuracy: 0.7927
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-3-birdirectional">
<h2>Model 3 (Birdirectional)<a class="headerlink" href="#model-3-birdirectional" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In Model 3, we implement a bi-directional sequence model for Decoder.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-bidirectional.jpeg" /></p>
<div class="section" id="id4">
<h3>Define Model<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span>
    <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Combine forward and backward state (last h&#39;s) from encoder</span>
<span class="n">encoder_state</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)([</span><span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span><span class="p">])</span>

<span class="c1"># Decoder GRU, using `encoder_state`, but the latent dim needs to be doubled because we use two last states from the bidirectional encoder</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model3</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model3</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_2&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_inputs (InputLayer)     [(None, 7, 14)]      0                                            
__________________________________________________________________________________________________
bidirectional (Bidirectional)   [(None, 7, 512), (No 417792      encoder_inputs[0][0]             
__________________________________________________________________________________________________
decoder_inputs (InputLayer)     [(None, 5, 13)]      0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 512)          0           bidirectional[0][1]              
                                                                 bidirectional[0][2]              
__________________________________________________________________________________________________
decoder_gru (GRU)               [(None, 5, 512), (No 809472      decoder_inputs[0][0]             
                                                                 concatenate[0][0]                
__________________________________________________________________________________________________
time_distributed_layer (TimeDis (None, 5, 13)        6669        decoder_gru[0][0]                
==================================================================================================
Total params: 1,233,933
Trainable params: 1,233,933
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model3</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_66_0.png" src="../_images/dl-seq-to-seq-attention-addition_66_0.png" />
</div>
</div>
</div>
<div class="section" id="id5">
<h3>Training<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model3</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history3</span> <span class="o">=</span> <span class="n">full_model3</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
141/141 [==============================] - 31s 191ms/step - loss: 1.7620 - accuracy: 0.4114 - val_loss: 1.4097 - val_accuracy: 0.4787
Epoch 2/20
141/141 [==============================] - 28s 198ms/step - loss: 1.3824 - accuracy: 0.4828 - val_loss: 1.3175 - val_accuracy: 0.5085
Epoch 3/20
141/141 [==============================] - 28s 199ms/step - loss: 1.2755 - accuracy: 0.5202 - val_loss: 1.1128 - val_accuracy: 0.5824
Epoch 4/20
141/141 [==============================] - 28s 196ms/step - loss: 1.0743 - accuracy: 0.5923 - val_loss: 0.9770 - val_accuracy: 0.6261
Epoch 5/20
141/141 [==============================] - 27s 193ms/step - loss: 0.9377 - accuracy: 0.6443 - val_loss: 0.8428 - val_accuracy: 0.6808
Epoch 6/20
141/141 [==============================] - 28s 198ms/step - loss: 0.8103 - accuracy: 0.6921 - val_loss: 0.7405 - val_accuracy: 0.7206
Epoch 7/20
141/141 [==============================] - 27s 193ms/step - loss: 0.7196 - accuracy: 0.7290 - val_loss: 0.6697 - val_accuracy: 0.7490
Epoch 8/20
141/141 [==============================] - 28s 196ms/step - loss: 0.6462 - accuracy: 0.7572 - val_loss: 0.5991 - val_accuracy: 0.7723
Epoch 9/20
141/141 [==============================] - 28s 197ms/step - loss: 0.5814 - accuracy: 0.7812 - val_loss: 0.5565 - val_accuracy: 0.7875
Epoch 10/20
141/141 [==============================] - 28s 199ms/step - loss: 0.5266 - accuracy: 0.8041 - val_loss: 0.4991 - val_accuracy: 0.8148
Epoch 11/20
141/141 [==============================] - 28s 200ms/step - loss: 0.4805 - accuracy: 0.8237 - val_loss: 0.4539 - val_accuracy: 0.8305
Epoch 12/20
141/141 [==============================] - 27s 191ms/step - loss: 0.4377 - accuracy: 0.8385 - val_loss: 0.4337 - val_accuracy: 0.8315
Epoch 13/20
141/141 [==============================] - 27s 188ms/step - loss: 0.4137 - accuracy: 0.8438 - val_loss: 0.3966 - val_accuracy: 0.8449
Epoch 14/20
141/141 [==============================] - 27s 191ms/step - loss: 0.3763 - accuracy: 0.8572 - val_loss: 0.3734 - val_accuracy: 0.8533
Epoch 15/20
141/141 [==============================] - 27s 190ms/step - loss: 0.3593 - accuracy: 0.8617 - val_loss: 0.3726 - val_accuracy: 0.8472
Epoch 16/20
141/141 [==============================] - 27s 191ms/step - loss: 0.3337 - accuracy: 0.8715 - val_loss: 0.3416 - val_accuracy: 0.8615
Epoch 17/20
141/141 [==============================] - 27s 192ms/step - loss: 0.3228 - accuracy: 0.8738 - val_loss: 0.3241 - val_accuracy: 0.8648
Epoch 18/20
141/141 [==============================] - 26s 187ms/step - loss: 0.2985 - accuracy: 0.8833 - val_loss: 0.3131 - val_accuracy: 0.8685
Epoch 19/20
141/141 [==============================] - 27s 193ms/step - loss: 0.3034 - accuracy: 0.8789 - val_loss: 0.2922 - val_accuracy: 0.8761
Epoch 20/20
141/141 [==============================] - 27s 192ms/step - loss: 0.2781 - accuracy: 0.8902 - val_loss: 0.2767 - val_accuracy: 0.8865
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-4-peeky-decoder">
<h2>Model 4 (Peeky Decoder)<a class="headerlink" href="#model-4-peeky-decoder" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In the previous models, Decoder only utilizes the last hidden state of Encoder for the decoding of the first output. As for the subsequent decoding timesteps, Decoder does not have the information from Encoder.</p></li>
<li><p>In Model 4, we implement a <strong>peeky</strong> Decoder. This strategy allows the Decoder to access the information (last hidden state) of the Encoder in the decoding of every timestep.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-peeky.jpeg" /></p>
<div class="section" id="id6">
<h3>Define Model<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Model Inputs</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>

<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span>
    <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Combine forward and backward state (last h&#39;s) from encoder</span>
<span class="n">encoder_state</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)([</span><span class="n">encoder_state_fwd</span><span class="p">,</span> <span class="n">encoder_state_bwd</span><span class="p">])</span>


<span class="c1"># Repeat the last-hidden-state of Encoder</span>
<span class="n">encoder_state_repeated</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)(</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1">## Concatenate every decoder input with the encoder_state</span>
<span class="n">decoder_inputs_peeky</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span>
    <span class="p">[</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">encoder_state_repeated</span><span class="p">])</span>

<span class="c1"># Decoder GRU, using `encoder_state`, but the latent dim needs to be doubled because we use two last states from the bidirectional encoder</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs_peeky</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_out</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model4</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>

<span class="n">full_model4</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_3&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_inputs (InputLayer)     [(None, 7, 14)]      0                                            
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) [(None, 7, 512), (No 417792      encoder_inputs[0][0]             
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            
                                                                 bidirectional_1[0][2]            
__________________________________________________________________________________________________
decoder_inputs (InputLayer)     [(None, 5, 13)]      0                                            
__________________________________________________________________________________________________
repeat_vector (RepeatVector)    (None, 5, 512)       0           concatenate_1[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 5, 525)       0           decoder_inputs[0][0]             
                                                                 repeat_vector[0][0]              
__________________________________________________________________________________________________
decoder_gru (GRU)               [(None, 5, 512), (No 1595904     concatenate_2[0][0]              
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
time_distributed_layer (TimeDis (None, 5, 13)        6669        decoder_gru[0][0]                
==================================================================================================
Total params: 2,020,365
Trainable params: 2,020,365
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model4</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_74_0.png" src="../_images/dl-seq-to-seq-attention-addition_74_0.png" />
</div>
</div>
</div>
<div class="section" id="id7">
<h3>Training<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model4</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history4</span> <span class="o">=</span> <span class="n">full_model4</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
141/141 [==============================] - 41s 262ms/step - loss: 1.7230 - accuracy: 0.3985 - val_loss: 1.3661 - val_accuracy: 0.4923
Epoch 2/20
141/141 [==============================] - 35s 245ms/step - loss: 1.3878 - accuracy: 0.4897 - val_loss: 1.2761 - val_accuracy: 0.5186
Epoch 3/20
141/141 [==============================] - 35s 248ms/step - loss: 1.2087 - accuracy: 0.5443 - val_loss: 1.0251 - val_accuracy: 0.6119
Epoch 4/20
141/141 [==============================] - 35s 247ms/step - loss: 0.9747 - accuracy: 0.6309 - val_loss: 0.8405 - val_accuracy: 0.6812
Epoch 5/20
141/141 [==============================] - 35s 250ms/step - loss: 0.8019 - accuracy: 0.6961 - val_loss: 0.6752 - val_accuracy: 0.7470
Epoch 6/20
141/141 [==============================] - 37s 260ms/step - loss: 0.6353 - accuracy: 0.7632 - val_loss: 0.5278 - val_accuracy: 0.7964
Epoch 7/20
141/141 [==============================] - 35s 249ms/step - loss: 0.4643 - accuracy: 0.8285 - val_loss: 0.3228 - val_accuracy: 0.8820
Epoch 8/20
141/141 [==============================] - 35s 251ms/step - loss: 0.2652 - accuracy: 0.9147 - val_loss: 0.2156 - val_accuracy: 0.9270
Epoch 9/20
141/141 [==============================] - 35s 245ms/step - loss: 0.1647 - accuracy: 0.9587 - val_loss: 0.1169 - val_accuracy: 0.9749
Epoch 10/20
141/141 [==============================] - 35s 248ms/step - loss: 0.1078 - accuracy: 0.9770 - val_loss: 0.1102 - val_accuracy: 0.9705
Epoch 11/20
141/141 [==============================] - 35s 248ms/step - loss: 0.0732 - accuracy: 0.9880 - val_loss: 0.0595 - val_accuracy: 0.9890
Epoch 12/20
141/141 [==============================] - 35s 248ms/step - loss: 0.0464 - accuracy: 0.9941 - val_loss: 0.0511 - val_accuracy: 0.9883
Epoch 13/20
141/141 [==============================] - 35s 249ms/step - loss: 0.0617 - accuracy: 0.9853 - val_loss: 0.0510 - val_accuracy: 0.9908
Epoch 14/20
141/141 [==============================] - 35s 249ms/step - loss: 0.0341 - accuracy: 0.9962 - val_loss: 0.0283 - val_accuracy: 0.9957
Epoch 15/20
141/141 [==============================] - 35s 250ms/step - loss: 0.0200 - accuracy: 0.9984 - val_loss: 0.0232 - val_accuracy: 0.9962
Epoch 16/20
141/141 [==============================] - 35s 247ms/step - loss: 0.0164 - accuracy: 0.9986 - val_loss: 0.0300 - val_accuracy: 0.9925
Epoch 17/20
141/141 [==============================] - 35s 251ms/step - loss: 0.0216 - accuracy: 0.9959 - val_loss: 0.0712 - val_accuracy: 0.9764
Epoch 18/20
141/141 [==============================] - 35s 246ms/step - loss: 0.0348 - accuracy: 0.9915 - val_loss: 0.0191 - val_accuracy: 0.9957
Epoch 19/20
141/141 [==============================] - 35s 247ms/step - loss: 0.0103 - accuracy: 0.9990 - val_loss: 0.0133 - val_accuracy: 0.9974
Epoch 20/20
141/141 [==============================] - 35s 249ms/step - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.0337 - val_accuracy: 0.9896
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-5-attention">
<h2>Model 5 (Attention)<a class="headerlink" href="#model-5-attention" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In Model 5, we implement an Attention-based Decoder.</p></li>
<li><p>This Attention mechanism allows the Decoder to use all the information (all hidden states of all timesteps) of the Encoder.</p></li>
</ul>
<p><img alt="" src="../_images/seq2seq-attention-weights.jpeg" /></p>
<div class="section" id="id8">
<h3>Define Model<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define an input sequence and process it.</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_maxlen</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
                       <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>
<span class="c1"># Encoder GRU</span>
<span class="n">encoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_gru&#39;</span><span class="p">)</span>
<span class="n">encoder_out</span><span class="p">,</span> <span class="n">encoder_state</span> <span class="o">=</span> <span class="n">encoder_gru</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="c1"># Set up the decoder GRU, using `encoder_states` as initial state.</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_gru&#39;</span><span class="p">)</span>
<span class="n">decoder_out</span><span class="p">,</span> <span class="n">decoder_state</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span>
                                         <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_state</span><span class="p">)</span>

<span class="c1"># Attention layer</span>

<span class="n">attn_layer</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;attention_layer&quot;</span><span class="p">)</span>

<span class="c1">## The input for Attention: </span>
    <span class="c1">##  `query`: the `decoder_out`</span>
    <span class="c1">##  `value` &amp; `key`: the `encoder_out`</span>
    <span class="c1">## It returns a tensor of shape as `query`, i.e., context tensor</span>

<span class="n">attn_out</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_layer</span><span class="p">([</span><span class="n">decoder_out</span><span class="p">,</span> <span class="n">encoder_out</span><span class="p">],</span>
                                   <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Concat context tensor + decoder_out</span>
<span class="n">decoder_concat_input</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span>
    <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat_layer&#39;</span><span class="p">)([</span><span class="n">decoder_out</span><span class="p">,</span> <span class="n">attn_out</span><span class="p">])</span>

<span class="c1"># Dense layer</span>
<span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">target_vsize</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax_layer&#39;</span><span class="p">)</span>
<span class="n">dense_time</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;time_distributed_layer&#39;</span><span class="p">)</span>
<span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">dense_time</span><span class="p">(</span><span class="n">decoder_concat_input</span><span class="p">)</span>

<span class="c1"># Full model</span>
<span class="n">full_model5</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_pred</span><span class="p">)</span>
<span class="n">full_model5</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_4&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
encoder_inputs (InputLayer)     [(None, 7, 14)]      0                                            
__________________________________________________________________________________________________
decoder_inputs (InputLayer)     [(None, 5, 13)]      0                                            
__________________________________________________________________________________________________
encoder_gru (GRU)               [(None, 7, 256), (No 208896      encoder_inputs[0][0]             
__________________________________________________________________________________________________
decoder_gru (GRU)               [(None, 5, 256), (No 208128      decoder_inputs[0][0]             
                                                                 encoder_gru[0][1]                
__________________________________________________________________________________________________
attention_layer (Attention)     ((None, 5, 256), (No 0           decoder_gru[0][0]                
                                                                 encoder_gru[0][0]                
__________________________________________________________________________________________________
concat_layer (Concatenate)      (None, 5, 512)       0           decoder_gru[0][0]                
                                                                 attention_layer[0][0]            
__________________________________________________________________________________________________
time_distributed_layer (TimeDis (None, 5, 13)        6669        concat_layer[0][0]               
==================================================================================================
Total params: 423,693
Trainable params: 423,693
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model5</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_82_0.png" src="../_images/dl-seq-to-seq-attention-addition_82_0.png" />
</div>
</div>
</div>
<div class="section" id="id9">
<h3>Training<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">full_model5</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history5</span> <span class="o">=</span> <span class="n">full_model5</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_onehot</span><span class="p">,</span> <span class="n">decoder_input_onehot</span><span class="p">],</span>
                           <span class="n">decoder_output_onehot</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
141/141 [==============================] - 15s 87ms/step - loss: 1.8656 - accuracy: 0.3613 - val_loss: 1.4080 - val_accuracy: 0.4777
Epoch 2/20
141/141 [==============================] - 11s 76ms/step - loss: 1.4006 - accuracy: 0.4763 - val_loss: 1.3726 - val_accuracy: 0.4864
Epoch 3/20
141/141 [==============================] - 11s 77ms/step - loss: 1.3686 - accuracy: 0.4887 - val_loss: 1.3227 - val_accuracy: 0.5054
Epoch 4/20
141/141 [==============================] - 12s 85ms/step - loss: 1.2916 - accuracy: 0.5190 - val_loss: 1.1867 - val_accuracy: 0.5547
Epoch 5/20
141/141 [==============================] - 12s 85ms/step - loss: 1.1566 - accuracy: 0.5666 - val_loss: 1.0724 - val_accuracy: 0.5926
Epoch 6/20
141/141 [==============================] - 12s 84ms/step - loss: 1.0492 - accuracy: 0.6016 - val_loss: 0.9891 - val_accuracy: 0.6226
Epoch 7/20
141/141 [==============================] - 12s 82ms/step - loss: 0.9637 - accuracy: 0.6330 - val_loss: 0.9010 - val_accuracy: 0.6530
Epoch 8/20
141/141 [==============================] - 11s 79ms/step - loss: 0.8844 - accuracy: 0.6614 - val_loss: 0.8445 - val_accuracy: 0.6760
Epoch 9/20
141/141 [==============================] - 11s 76ms/step - loss: 0.8293 - accuracy: 0.6797 - val_loss: 0.7911 - val_accuracy: 0.6975
Epoch 10/20
141/141 [==============================] - 12s 83ms/step - loss: 0.7787 - accuracy: 0.7030 - val_loss: 0.7658 - val_accuracy: 0.6997
Epoch 11/20
141/141 [==============================] - 12s 82ms/step - loss: 0.7400 - accuracy: 0.7181 - val_loss: 0.7195 - val_accuracy: 0.7208
Epoch 12/20
141/141 [==============================] - 11s 79ms/step - loss: 0.7058 - accuracy: 0.7312 - val_loss: 0.6870 - val_accuracy: 0.7338
Epoch 13/20
141/141 [==============================] - 12s 82ms/step - loss: 0.6794 - accuracy: 0.7396 - val_loss: 0.6785 - val_accuracy: 0.7364
Epoch 14/20
141/141 [==============================] - 11s 78ms/step - loss: 0.6555 - accuracy: 0.7492 - val_loss: 0.6554 - val_accuracy: 0.7434
Epoch 15/20
141/141 [==============================] - 11s 80ms/step - loss: 0.6323 - accuracy: 0.7572 - val_loss: 0.6357 - val_accuracy: 0.7468
Epoch 16/20
141/141 [==============================] - 11s 79ms/step - loss: 0.6070 - accuracy: 0.7660 - val_loss: 0.6291 - val_accuracy: 0.7505
Epoch 17/20
141/141 [==============================] - 11s 77ms/step - loss: 0.5935 - accuracy: 0.7706 - val_loss: 0.5869 - val_accuracy: 0.7654
Epoch 18/20
141/141 [==============================] - 11s 80ms/step - loss: 0.5603 - accuracy: 0.7816 - val_loss: 0.5662 - val_accuracy: 0.7734
Epoch 19/20
141/141 [==============================] - 11s 79ms/step - loss: 0.5451 - accuracy: 0.7869 - val_loss: 0.5375 - val_accuracy: 0.7838
Epoch 20/20
141/141 [==============================] - 11s 78ms/step - loss: 0.5158 - accuracy: 0.7989 - val_loss: 0.5223 - val_accuracy: 0.7893
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">full_model5</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_85_0.png" src="../_images/dl-seq-to-seq-attention-addition_85_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="save-models">
<h2>Save Models<a class="headerlink" href="#save-models" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Save model</span>
<span class="n">full_model5</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;keras_models/s2s-addition-attention.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interim-comparison">
<h2>Interim Comparison<a class="headerlink" href="#interim-comparison" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">history1</span><span class="p">,</span> <span class="n">history2</span><span class="p">,</span> <span class="n">history3</span><span class="p">,</span> <span class="n">history4</span><span class="p">,</span> <span class="n">history5</span><span class="p">]</span>
<span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">history</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VanilaRNN&#39;</span><span class="p">,</span> <span class="s1">&#39;GRU&#39;</span><span class="p">,</span> <span class="s1">&#39;Birdirectional&#39;</span><span class="p">,</span> <span class="s1">&#39;Peeky&#39;</span><span class="p">,</span> <span class="s1">&#39;Attention&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;keras_models/s2s-attention-addition-history&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;keras_models/s2s-attention-addition-history&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">acc</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">,</span>
             <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
             <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparing Different Sequence Models&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_92_0.png" src="../_images/dl-seq-to-seq-attention-addition_92_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span>
             <span class="n">a</span><span class="p">,</span>
             <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
             <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparing Different Sequence Models&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_93_0.png" src="../_images/dl-seq-to-seq-attention-addition_93_0.png" />
</div>
</div>
</div>
<div class="section" id="attention-model-analysis">
<h2>Attention Model Analysis<a class="headerlink" href="#attention-model-analysis" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ## If the model is loaded via external files</span>
<span class="c1"># ## Load the encoder_model, decoder_model this way</span>
<span class="c1"># from keras.models import load_model</span>
<span class="c1"># full_model5.load_weights(&#39;keras_models/s2s-addition-attention.h5&#39;)</span>
<span class="c1"># full_model5.compile(optimizer=&#39;adam&#39;,</span>
<span class="c1">#                     loss=&#39;categorical_crossentropy&#39;,</span>
<span class="c1">#                     metrics=[&#39;accuracy&#39;])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_model</span> <span class="o">=</span> <span class="n">full_model5</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="inference-encoder">
<h3>Inference Encoder<a class="headerlink" href="#inference-encoder" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Inference model &quot;&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot; Encoder (Inference) model &quot;&quot;&quot;</span>
<span class="n">encoder_inf_inputs</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">encoder_inf_out</span><span class="p">,</span> <span class="n">encoder_inf_state</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>
<span class="n">encoder_inf_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">encoder_inf_inputs</span><span class="p">,</span>
                          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inf_out</span><span class="p">,</span> <span class="n">encoder_inf_state</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">encoder_inf_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_100_0.png" src="../_images/dl-seq-to-seq-attention-addition_100_0.png" />
</div>
</div>
</div>
<div class="section" id="inference-decoder">
<h3>Inference Decoder<a class="headerlink" href="#inference-decoder" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The attention weights refer to at each time step, the relevance of the decoder output h and all the encoder output h’s. Therefore, the shape of the attention weights should be <code class="docutils literal notranslate"><span class="pre">(input_maxlen,)</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Decoder (Inference) model &quot;&quot;&quot;</span>
<span class="n">decoder_inf_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inf_inputs&#39;</span><span class="p">)</span>  <span class="c1">## first target charcater</span>
<span class="n">encoder_inf_states</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_maxlen</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inf_states&#39;</span><span class="p">)</span>  <span class="c1">## initial h&#39;s from encoder</span>
<span class="n">decoder_init_state</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">),</span>
                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_init&#39;</span><span class="p">)</span>  <span class="c1">## initial c from encoder</span>

<span class="n">decoder_inf_gru</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">decoder_inf_out</span><span class="p">,</span> <span class="n">decoder_inf_state</span> <span class="o">=</span> <span class="n">decoder_inf_gru</span><span class="p">(</span>
    <span class="n">decoder_inf_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">decoder_init_state</span><span class="p">)</span>
<span class="c1"># attn_inf_out, attn_inf_weights = attn_layer([encoder_inf_states, decoder_inf_out])</span>
<span class="n">decoder_inf_attention</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="n">attn_inf_out</span><span class="p">,</span> <span class="n">attn_inf_weights</span> <span class="o">=</span> <span class="n">decoder_inf_attention</span><span class="p">(</span>
    <span class="p">[</span><span class="n">decoder_inf_out</span><span class="p">,</span> <span class="n">encoder_inf_states</span><span class="p">],</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">decoder_inf_concat</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">(</span>
    <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat&#39;</span><span class="p">)([</span><span class="n">decoder_inf_out</span><span class="p">,</span> <span class="n">attn_inf_out</span><span class="p">])</span>
<span class="n">decoder_inf_timedense</span> <span class="o">=</span> <span class="n">full_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
<span class="n">decoder_inf_pred</span> <span class="o">=</span> <span class="n">decoder_inf_timedense</span><span class="p">(</span><span class="n">decoder_inf_concat</span><span class="p">)</span>
<span class="n">decoder_inf_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">encoder_inf_states</span><span class="p">,</span> <span class="n">decoder_init_state</span><span class="p">,</span> <span class="n">decoder_inf_inputs</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">decoder_inf_pred</span><span class="p">,</span> <span class="n">attn_inf_weights</span><span class="p">,</span> <span class="n">decoder_inf_state</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">decoder_inf_model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_104_0.png" src="../_images/dl-seq-to-seq-attention-addition_104_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decode_sequence</span><span class="p">(</span><span class="n">input_seq</span><span class="p">):</span>

    <span class="c1">## Initialize target output character &quot;_&quot;</span>
    <span class="c1">#     test_dec_onehot_seq = np.zeros((1,1,target_vsize))</span>
    <span class="c1">#     test_dec_onehot_seq[0,0, target_tokenizer.word_index.get(&#39;_&#39;)]=1.0</span>
    <span class="n">initial_text</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_&#39;</span><span class="p">]</span>
    <span class="n">initial_seq</span> <span class="o">=</span> <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">initial_text</span><span class="p">])</span>
    <span class="c1">#initial_seq = pad_sequences(initial_seq, padding=&#39;post&#39;, maxlen = target_maxlen)</span>
    <span class="n">test_dec_onehot_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="n">to_categorical</span><span class="p">(</span><span class="n">initial_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1">## Encoder Inference</span>
    <span class="n">enc_outs</span><span class="p">,</span> <span class="n">enc_last_state</span> <span class="o">=</span> <span class="n">encoder_inf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>

    <span class="c1">## Update decoder initial state</span>
    <span class="n">dec_state</span> <span class="o">=</span> <span class="n">enc_last_state</span>

    <span class="c1">## Holder for attention weights and decoded texts</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dec_text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_maxlen</span><span class="p">):</span>
        <span class="n">dec_out</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">dec_state</span> <span class="o">=</span> <span class="n">decoder_inf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="p">[</span><span class="n">enc_outs</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">test_dec_onehot_seq</span><span class="p">])</span>
        <span class="n">dec_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dec_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">dec_ind</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>


<span class="c1">#         test_dec_onehot_seq = np.zeros((1,1,target_vsize))</span>
<span class="c1">#         test_dec_onehot_seq[0,0, dec_ind]=1.0</span>
        <span class="n">initial_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">dec_index2word</span><span class="p">[</span><span class="n">dec_ind</span><span class="p">]]</span>
        <span class="n">initial_seq</span> <span class="o">=</span> <span class="n">target_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">initial_text</span><span class="p">])</span>
        <span class="c1">#initial_seq = pad_sequences(initial_seq, padding=&#39;post&#39;, maxlen = target_maxlen)</span>
        <span class="n">test_dec_onehot_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">to_categorical</span><span class="p">(</span><span class="n">initial_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">target_vsize</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">dec_ind</span><span class="p">,</span> <span class="n">attention</span><span class="p">))</span>

        <span class="c1">## append the predicted char</span>
        <span class="n">dec_text</span> <span class="o">+=</span> <span class="n">dec_index2word</span><span class="p">[</span><span class="n">dec_ind</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">dec_text</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">decoder_input_onehot</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoder_input_onehot</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10, 7, 14)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">seq_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="c1"># Take one sequence (part of the training set)</span>
    <span class="c1"># for trying out decoding.</span>

    <span class="n">decoded_sentence</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decode_sequence</span><span class="p">(</span>
        <span class="n">encoder_input_onehot</span><span class="p">[</span><span class="n">seq_index</span><span class="p">:</span><span class="n">seq_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Input sentence:&#39;</span><span class="p">,</span> <span class="n">tr_input_texts</span><span class="p">[</span><span class="n">seq_index</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Decoded sentence:&#39;</span><span class="p">,</span> <span class="n">decoded_sentence</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-
Input sentence: 27+673 
Decoded sentence: 700_
-
Input sentence: 153+27 
Decoded sentence: 181_
-
Input sentence: 93+901 
Decoded sentence: 999_
-
Input sentence: 243+678
Decoded sentence: 919_
-
Input sentence: 269+46 
Decoded sentence: 313_
-
Input sentence: 235+891
Decoded sentence: 1129_
-
Input sentence: 46+290 
Decoded sentence: 339_
-
Input sentence: 324+947
Decoded sentence: 1290_
-
Input sentence: 721+49 
Decoded sentence: 762_
-
Input sentence: 535+7  
Decoded sentence: 540_
-
Input sentence: 45+117 
Decoded sentence: 161_
-
Input sentence: 669+174
Decoded sentence: 830_
-
Input sentence: 904+7  
Decoded sentence: 909_
-
Input sentence: 22+731 
Decoded sentence: 746_
-
Input sentence: 83+742 
Decoded sentence: 825_
-
Input sentence: 808+769
Decoded sentence: 1561_
-
Input sentence: 240+42 
Decoded sentence: 277_
-
Input sentence: 18+44  
Decoded sentence: 62_
-
Input sentence: 4+166  
Decoded sentence: 170_
-
Input sentence: 731+13 
Decoded sentence: 744_
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-attention">
<h2>Plotting Attention<a class="headerlink" href="#plotting-attention" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ind</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">doc_inputs</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">decode_sequence</span><span class="p">(</span><span class="n">encoder_input_onehot</span><span class="p">[</span><span class="n">ind</span><span class="p">:</span><span class="n">ind</span> <span class="o">+</span>
                                                                     <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
<span class="n">mats</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dec_inputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dec_ind</span><span class="p">,</span> <span class="n">attn</span> <span class="ow">in</span> <span class="n">attention_weights</span><span class="p">:</span>
    <span class="n">mats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">dec_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dec_ind</span><span class="p">)</span>
<span class="n">attention_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mats</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_mat</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">attention_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">attention_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span>
    <span class="p">[</span><span class="n">dec_index2word</span><span class="p">[</span><span class="n">inp</span><span class="p">]</span> <span class="k">if</span> <span class="n">inp</span> <span class="o">!=</span> <span class="mi">2</span> <span class="k">else</span> <span class="s2">&quot;&lt;Res&gt;&quot;</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">dec_inputs</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span>
    <span class="n">enc_index2word</span><span class="p">[</span><span class="n">inp</span><span class="p">]</span> <span class="k">if</span> <span class="n">inp</span> <span class="o">!=</span> <span class="mi">2</span> <span class="k">else</span> <span class="s2">&quot;&lt;Res&gt;&quot;</span>
    <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">encoder_input_sequences</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-seq-to-seq-attention-addition_110_0.png" src="../_images/dl-seq-to-seq-attention-addition_110_0.png" />
</div>
</div>
</div>
<div class="section" id="evaluation-on-testing-data">
<h2>Evaluation on Testing Data<a class="headerlink" href="#evaluation-on-testing-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Wrap data preprocessing in a function</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_data</span><span class="p">(</span><span class="n">enc_tokenizer</span><span class="p">,</span> <span class="n">dec_tokenizer</span><span class="p">,</span> <span class="n">enc_text</span><span class="p">,</span> <span class="n">dec_text</span><span class="p">,</span>
                    <span class="n">enc_maxlen</span><span class="p">,</span> <span class="n">dec_maxlen</span><span class="p">,</span> <span class="n">enc_vsize</span><span class="p">,</span> <span class="n">dec_vsize</span><span class="p">):</span>
    <span class="n">enc_seq</span> <span class="o">=</span> <span class="n">enc_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">enc_text</span><span class="p">)</span>
    <span class="n">enc_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">enc_seq</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">enc_maxlen</span><span class="p">)</span>
    <span class="n">enc_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">enc_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">enc_vsize</span><span class="p">)</span>

    <span class="n">dec_seq</span> <span class="o">=</span> <span class="n">dec_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">dec_text</span><span class="p">)</span>
    <span class="n">dec_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">dec_seq</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">dec_maxlen</span><span class="p">)</span>
    <span class="n">dec_onehot</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">dec_seq</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">dec_vsize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">enc_onehot</span><span class="p">,</span> <span class="n">dec_onehot</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_encoder_input_onehot</span><span class="p">,</span> <span class="n">ts_decoder_target_onehot</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span>
    <span class="n">input_tokenizer</span><span class="p">,</span> <span class="n">target_tokenizer</span><span class="p">,</span> <span class="n">ts_input_texts</span><span class="p">,</span> <span class="n">ts_target_texts</span><span class="p">,</span>
    <span class="n">input_maxlen</span><span class="p">,</span> <span class="n">target_maxlen</span><span class="p">,</span> <span class="n">input_vsize</span><span class="p">,</span> <span class="n">target_vsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">ts_encoder_input_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ts_decoder_target_onehot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 7, 14)
(5000, 6, 13)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">ts_encoder_input_onehot</span><span class="p">,</span> <span class="n">ts_decoder_target_onehot</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]],</span>
    <span class="n">ts_decoder_target_onehot</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>20/20 - 1s - loss: 0.5209 - accuracy: 0.7919
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.5208640694618225, 0.7918800115585327]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Attention? Attention!</a></p></li>
<li><p>Bahdanau Attention Layber developed in <a class="reference external" href="https://github.com/thushv89/attention_keras">Thushan</a></p></li>
<li><p>Thushan Ganegedara’s <a class="reference external" href="https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39">Attention in Deep Networks with Keras</a></p></li>
<li><p>This is based on Chapter 7 of <a class="reference external" href="https://www.tenlong.com.tw/products/9789865020675">Deep Learning 2用 Python 進行自然語言處理的基礎理論實作</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./temp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="dl-seq-to-seq-types.html" title="previous page">Attention: Intuition</a>
    <a class='right-next' id="next-link" href="dl-transformers-intuition.html" title="next page">Transformers: Intuition</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>