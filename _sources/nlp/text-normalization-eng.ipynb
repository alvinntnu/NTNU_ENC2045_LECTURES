{"cells":[{"cell_type":"markdown","metadata":{"id":"ZmFNFOtIT33b"},"source":["# Text Normalization\n"]},{"cell_type":"markdown","metadata":{"id":"8BXA6KifT33c"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"id":"VL6wNXNXT33c"},"source":["The objective of text normalization is to clean up the text by removing unnecessary and irrelevant components. What to include or exclude for the later analysis is highly dependent on the research questions. Before any data preprocessing, you always need to ask yourself whether it would have great impact on the data distribution if you remove the target structures."]},{"cell_type":"markdown","metadata":{"id":"zv446X0oT33d"},"source":[":::{contents}\n",":::"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":13505,"status":"ok","timestamp":1709151139855,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"clEnnO7GT33d"},"outputs":[],"source":["import spacy\n","import unicodedata\n","import re\n","import collections\n","import requests\n","import nltk\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from nltk.corpus import wordnet\n","from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer\n","from bs4 import BeautifulSoup\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1709151140283,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"53kpr9yVT33d","outputId":"5c81cf42-b7ac-4311-ab83-f6755c0983d5"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["## NLTK downloads\n","nltk.download(['wordnet', 'stopwords'])"]},{"cell_type":"markdown","metadata":{"id":"10RCJXO3T33e"},"source":["## HTML Tags"]},{"cell_type":"markdown","metadata":{"id":"yBoW1u8iT33e"},"source":["- One of the most frequent data source is the Internet. Assuming that we web-crawl text data from a wikipedia page, we need to clean up the HTML codes quite a bit.\n","- Important steps:\n","    - Download and parse the HTML codes of the webpage\n","    - Identify the elements from the page that we are interested in\n","    - Extract the textual contents of the HTML elements\n","    - Remove unnecessary HTML tags\n","    - Remove extra spacing/spaces"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1709151140558,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"CKvtc7eOT33e","outputId":"ad827d55-f0a2-4b02-f98b-8309b2f0baa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-p'\n"]}],"source":["## Get html data\n","data = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)')\n","content = data.content\n","print(content[:500])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1354,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"lzQJ1dR0T33e","outputId":"1ee37c02-d340-472c-9d9f-95d95f4bb1c6","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["General-purpose programming language\n","PythonParadigmMulti-paradigm: Object-oriented,[1] Procedural (Imperative), Functional, Structured, ReflectiveDesigned byGuido van RossumDeveloperPython Software FoundationFirst appeared20 February 1991; 33 years ago (1991-02-20)[2]Stable release3.12.2 \n","   / 7 February 2024; 21 days ago (7 February 2024)\n","Typing disciplineDuck, Dynamic, Strong typing;[3] Optional type annotations (since 3.5, but those hints are ignored, except with unofficial tools)[4]OSWindows, macOS, Linux/UNIX, Android, Unix-like systems, BSD variants[5][6] and a few other platforms[7]LicensePython Software Foundation LicenseFilename extensions.py, .pyw, .pyz, [8]\n",".pyi, .pyc, .pydWebsitepython.orgMajor implementationsCPython, PyPy, Stackless Python, MicroPython, CircuitPython, IronPython, JythonDialectsCython, RPython, Starlark[9]Influenced byABC,[10] Ada,[11] ALGOL 68,[12] APL,[13] C,[14] C++,[15] CLU,[16] Dylan,[17] Haskell,[18][13] Icon,[19] Lisp,[20] Modula-3,[12][15] Perl,[21]\n"]}],"source":["## Function: remove html tags\n","\n","def strip_html_tags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    ## Can include more HTML preprocessing here...\n","    stripped_html_elements = soup.findAll(name='div',attrs={'id':'mw-content-text'})\n","    stripped_text = ' '.join([h.get_text() for h in stripped_html_elements])\n","    stripped_text = re.sub(r'[\\r|\\n|\\r]+', '\\n', stripped_text)\n","    return stripped_text\n","\n","clean_content = strip_html_tags(content)\n","print(clean_content[:1000])"]},{"cell_type":"markdown","metadata":{"id":"N7htlJy_T33f"},"source":[":::{tip}\n","The above preprocessing of the wiki page content is less ideal. One can further improve the text processing by targeting at the real text content of the entry.\n",":::"]},{"cell_type":"markdown","metadata":{"id":"uG7kEzDYT33f"},"source":["## Stemming"]},{"cell_type":"markdown","metadata":{"id":"9QS1b38KT33f"},"source":["- Stemming is the process where we standardize word forms into their base stem irrespective of their inflections (or other derivational variations).\n","- The `nltk` provides several popular stemmers for English:\n","    - `nltk.stem.PorterStemmer`\n","    - `nltk.stem.LancasterStemmer`\n","    - `nltk.stem.RegexpStemmer`\n","    - `nltk.stem.SnowballStemmer`"]},{"cell_type":"markdown","metadata":{"id":"3ladBumVT33f"},"source":["- We can compare the results of different stemmers."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"lCyl_xS_T33f"},"outputs":[],"source":["\n","words = ['jumping', 'jumps', 'jumped', 'jumpy']\n","ps = PorterStemmer()\n","ls = LancasterStemmer()\n","ss = SnowballStemmer('english')\n","\n","rs = RegexpStemmer('ing$|s$|ed$|y$', min=4) # set the minimum of the string to stem\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"7EmptpphT33f","outputId":"9a71c5d7-4102-499c-f8d2-5b88fd5a8542"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jumpi']"]},"metadata":{},"execution_count":6}],"source":["## Porter Stemmer\n","[ps.stem(w) for w in words]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"Yek_orgbT33f","outputId":"7b4aae20-8509-4cd3-f2a8-5f4a8ed95e38"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jumpy']"]},"metadata":{},"execution_count":7}],"source":["## Lancaster Stemmer\n","[ls.stem(w) for w in words]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"hWa7sgWaT33f","outputId":"997c8f06-5636-48e7-a190-d443f43c875b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jumpi']"]},"metadata":{},"execution_count":8}],"source":["## Snowball Stemmer\n","[ss.stem(w) for w in words]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"kzCaRz0tT33f","outputId":"357cc79c-2793-4b21-8e83-f17abdc53ad1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jump']"]},"metadata":{},"execution_count":9}],"source":["## Regular Expression Stemmer\n","[rs.stem(w) for w in words]"]},{"cell_type":"markdown","metadata":{"id":"C_SQVfj-T33f"},"source":["## Lemmatization\n"]},{"cell_type":"markdown","metadata":{"id":"VKDFsFWzT33f"},"source":["- Lemmatization is similar to stemmatization.\n","- It is a process where we remove word affixes to get the **root word** but not the **root stem**.\n","- These root words, i.e., lemmas, are lexicographically correct words and always present in the dictionary."]},{"cell_type":"markdown","metadata":{"id":"5PcL-WWhT33f"},"source":["```{admonition} Question\n",":class: attention\n","In terms of Lemmatization and Stemmatization, which one requires more computational cost? That is, which processing might be slower?\n","```"]},{"cell_type":"markdown","metadata":{"id":"Epn3MFnHT33f"},"source":["- Two frequently-used lemmatizers\n","    - `nltk.stem.WordNetLemmatizer`\n","    - `spacy`"]},{"cell_type":"markdown","metadata":{"id":"dP6TYBvgT33f"},"source":["### WordNet Lemmatizer"]},{"cell_type":"markdown","metadata":{"id":"rKgHWYkUT33g"},"source":["- WordNetLemmatizer utilizes the dictionary of WordNet.\n","- It requires the **parts of speech** of the word for lemmatization.\n","- I think right now only nouns, verbs and adjectives are important in `WordNetLemmatizer`."]},{"cell_type":"markdown","metadata":{"id":"ZYJseHOUT33g"},"source":[":::{important}\n","For Wordnet-based Lemmatizer, it is important to specify the part of speech of the word form.\n","\n","Valid options are `\"n\"` for nouns, `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"` for satellite adjectives.\n",":::"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1709151141910,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"hhkuCMHbT33g"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","wnl = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3839,"status":"ok","timestamp":1709151145748,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"jkxOU4YhT33g","outputId":"378631c5-fdfb-4980-fdff-590506486848"},"outputs":[{"output_type":"stream","name":"stdout","text":["['car', 'men', 'foot']\n"]}],"source":["# nouns\n","nouns = ['cars','men','feet']\n","nouns_lemma = [wnl.lemmatize(n, 'n') for n in nouns]\n","print(nouns_lemma)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709151145748,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"IMcnOD1cT33g","outputId":"559c78d0-be55-4efd-fb21-d7b00f9736b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["['run', 'eat', 'grow', 'grow']\n"]}],"source":["# verbs\n","verbs = ['running', 'ate', 'grew', 'grown']\n","verbs_lemma = [wnl.lemmatize(v, 'v') for v in verbs]\n","print(verbs_lemma)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709151145748,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"szkH9prGT33g","outputId":"cfbfecaf-371d-4d4a-814c-30bf8972113a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['sad', 'fancy', 'jumpy', 'good', 'least', 'less', 'bad', 'bad', 'best', 'good']\n"]}],"source":["# adj\n","adjs = ['saddest', 'fancier', 'jumpy', 'better', 'least', 'less', 'worse', 'worst','best', 'better']\n","adjs_lemma = [wnl.lemmatize(a, 'a') for a in adjs]\n","print(adjs_lemma)"]},{"cell_type":"markdown","metadata":{"id":"gFMMdyNNT33g"},"source":["### Spacy"]},{"cell_type":"markdown","metadata":{"id":"OJZh688zT33g"},"source":["```{warning}\n","To use `spacy` properly, you need to download/install the language models of the English language first before you load the parameter files. Please see [spacy documentation](https://spacy.io/usage/models/) for installation steps.\n","\n","Also, please remember to install the language models in the right conda environment.\n","```"]},{"cell_type":"markdown","metadata":{"id":"O6MGSXBlT33g"},"source":["- For example, in my Mac:\n","\n","```\n","$ conda activate python-notes\n","$ pip install spacy\n","$ python -m spacy download en_core_web_sm\n","```"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1934,"status":"ok","timestamp":1709151147680,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"Q3EwRapsT33g","scrolled":true},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm', disable=['parse','entity'])"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709151147682,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"H77fA4OJT33g"},"outputs":[],"source":["text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n","text_tagged = nlp(text)"]},{"cell_type":"markdown","metadata":{"id":"CTtC9wV_T33g"},"source":["- `spacy` processes the text by tokenizing it into tokens and enriching the tokens with many annotations."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709151147682,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"7JMLD2_IT33g","outputId":"f8616b6b-00fb-4c52-e0ec-c7eef2b6e36e"},"outputs":[{"output_type":"stream","name":"stdout","text":["My/my/PRON\n","system/system/NOUN\n","keeps/keep/VERB\n","crashing/crash/VERB\n","his/his/PRON\n","crashed/crashed/NOUN\n","yesterday/yesterday/NOUN\n",",/,/PUNCT\n","ours/ours/PRON\n","crashes/crash/VERB\n","daily/daily/ADV\n"]}],"source":["for t in text_tagged:\n","    print(t.text+'/'+t.lemma_ + '/'+ t.pos_)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1709151147682,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"aNWo5YCjT33g","outputId":"27771e06-d74a-40f5-cc8e-aa281c557da8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'my system keep crash ! his crashed yesterday , ours crash daily'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["def lemmatize_text(text):\n","    text = nlp(text)\n","    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n","    return text\n","\n","lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")\n"]},{"cell_type":"markdown","metadata":{"id":"CcvLezZuT33g"},"source":["## Contractions"]},{"cell_type":"markdown","metadata":{"id":"wQ3KRrgfT33p"},"source":["- For the English data, contractions are problematic sometimes.\n","- These may get even more complicated when different tokenizers deal with contractions differently.\n","- A good way is to expand all contractions into their original independent word forms."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25726,"status":"ok","timestamp":1709151173401,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"FcEcysLTUFev","outputId":"fe1fa554-ef68-4783-b59e-1d5eab46eca1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## -------------------- ##\n","## Colab Only           ##\n","## -------------------- ##\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"lQ8Vp3HhT33p"},"source":["```{note}\n","If you are using your local Python environment, please download the `TAWP` directory from the `ENC2045_demo_data`. This directory contains code snippets provided in Sarkar's (2020) book.\n","\n","We assume that you have saved the `TAWP` directory under `GoogleDrive/ENC2045_demo_data`.\n","```"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709151173401,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"LRmzS-6FTR7o","outputId":"ba873b9d-dc54-430d-94b3-b7a4d82e09e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["## -------------------- ##\n","## Colab Only           ##\n","## -------------------- ##\n","## working dir\n","!pwd"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1709151173867,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"AJ4n7M2oTvyR","outputId":"05bdf23c-7da3-43f0-a34d-f475e8013074"},"outputs":[{"output_type":"stream","name":"stdout","text":["05-3_clauseorders.csv\t      dcard-top100.csv\t PDF\t\t     TAWP\n","addition-student-version.csv  glove\t\t pdf-firth-text.png  TAWP.zip\n","date-student-version.csv      movie_reviews.csv  stopwords\n"]}],"source":["## -------------------- ##\n","## Colab Only           ##\n","## -------------------- ##\n","## check ENC2045_demo_data\n","!ls /content/drive/MyDrive/ENC2045_demo_data/"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":1340,"status":"ok","timestamp":1709151175205,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"23kx08wzUl30"},"outputs":[],"source":["## -------------------- ##\n","## Colab Only           ##\n","## -------------------- ##\n","!cp -r /content/drive/MyDrive/ENC2045_demo_data/TAWP /content/"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709151175205,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"},"user_tz":-480},"id":"0dD23JmET33p"},"outputs":[],"source":["import TAWP  ## You have to put TAWA directory under your WORKING DIRECTORY\n","from TAWP.contractions import CONTRACTION_MAP\n","\n","def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n","    ## create a regex pattern of all contracted forms\n","    contractions_pattern = re.compile('({})'.format('|'.join(\n","        contraction_mapping.keys())),flags=re.IGNORECASE | re.DOTALL)\n","\n","    def expand_match(contraction):\n","        match = contraction.group(0)  # the whole matched contraction\n","\n","        # if the matched contraction (=keys) exists in the dict,\n","        # get its corresponding uncontracted form (=values)\n","        expanded_contraction = contraction_mapping.get(match)\\\n","                                if contraction_mapping.get(match)\\\n","                                else contraction_mapping.get(match.lower())\n","\n","        return expanded_contraction\n","\n","    # find each contraction in the pattern,\n","    # find it from text,\n","    # and replace it using the output of\n","    # expand_match\n","    expanded_text = contractions_pattern.sub(expand_match, text)\n","    expanded_text = re.sub(\"'\", \"\", expanded_text)\n","    return expanded_text"]},{"cell_type":"markdown","metadata":{"id":"pTAIjPJVT33p"},"source":[":::{note}\n","In `re.sub(repl, str)`, when `repl` is a function like above, the function is called for every non-overlapping occurrence of pattern `contractions_pattern`. The function `expand_match` takes a single matched contraction, and returns the replacement string, i.e., its uncontracted form in the dictionary.\n","\n","In other words, `contractions_pattern.sub(expand_match, text)` above says: to substitute every non-overlapping occurrence of pattern `contractions_pattern`  in `text` with the output of `expand_match()`\n",":::"]},{"cell_type":"markdown","metadata":{"id":"M3y9vjSJWEsQ"},"source":[]},{"cell_type":"markdown","metadata":{"id":"dCOVoLURWEsQ"},"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"id":"cRU5qPcYT33p","outputId":"0eff89e7-bc58-4481-93fd-97acca1a9e69","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175205,"user_tz":-480,"elapsed":16,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["you all cannot expand contractions I would think\n","I am very glad he is here! And it is not here!\n"]}],"source":["print(expand_contractions(\"Y'all can't expand contractions I'd think\"))\n","print(expand_contractions(\"I'm very glad he's here! And it ain't here!\"))"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"GOy9LkT1T33p","outputId":"07bb9fb3-09a6-4712-8e19-b9c7d9ccb5a1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175206,"user_tz":-480,"elapsed":16,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{},"execution_count":24}],"source":["type(CONTRACTION_MAP)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"3YKHtLoqT33p","outputId":"a8568082-32ce-4cad-92b0-47d892eb1924","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":16,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(\"ain't\", 'is not'),\n"," (\"aren't\", 'are not'),\n"," (\"can't\", 'cannot'),\n"," (\"can't've\", 'cannot have'),\n"," (\"'cause\", 'because')]"]},"metadata":{},"execution_count":25}],"source":["list(CONTRACTION_MAP.items())[:5] # check the first five items"]},{"cell_type":"markdown","metadata":{"id":"U5E5sZa4T33p"},"source":["## Accented Characters (Non-ASCII)\n","\n","- The `unicodedata` module handles unicode characters very efficiently. Please check [unicodedata dcoumentation](https://docs.python.org/3/library/unicodedata.html) for more details.\n","- When dealing with the English data, we may often encounter foreign characters in texts that are not part of the ASCII character set."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"xRJm7N4kT33p","outputId":"8da8d37e-36a7-46a4-d8f9-0f8ca11a6d78","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":15,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Some Accented text'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}],"source":["## Function: remove accented chars\n","\n","def remove_accented_chars(text):\n","#     ```\n","#     (NFKD) will apply the compatibility decomposition, i.e.\n","#     replace all compatibility characters with their equivalents.\n","#     ```\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text\n","\n","\n","remove_accented_chars('Sómě Áccěntěd těxt')\n","\n","# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt'))\n","# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt').encode('ascii','ignore'))\n","# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt').encode('ascii','ignore').decode('utf-8', 'ignore'))"]},{"cell_type":"markdown","metadata":{"id":"wkJSsjOST33q"},"source":[":::{note}\n","- `str.encode()` returns an encoded version of the string as a bytes object using the specified encoding.\n","- `byes.decode()` returns a string decoded from the given bytes using the specified encoding.\n",":::"]},{"cell_type":"markdown","metadata":{"id":"Y6fkmIy7T33q"},"source":["- Another common scenario is the case where texts include both English and Chinese characters. What's worse, the English characters are in full-width."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"oPzC9jNyT33q","outputId":"9de73c97-c44f-4fc6-b8fb-35a14a7226a5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":14,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["中英文abc,,。..ABC123\n","中英文abc,,。..ABC123\n","中英文abc,，。.．ＡＢＣ１２３\n","中英文abc,，。.．ＡＢＣ１２３\n"]}],"source":["## Chinese characters with full-width English letters and punctuations\n","text = '中英文abc,，。.．ＡＢＣ１２３'\n","print(unicodedata.normalize('NFKD', text))\n","print(unicodedata.normalize('NFKC', text))  # recommended method\n","print(unicodedata.normalize('NFC', text))\n","print(unicodedata.normalize('NFD', text))"]},{"cell_type":"markdown","metadata":{"id":"b4IvyJ4WT33q"},"source":["- Sometimes, we may even want to keep characters of one language only."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"OFhbkhfqT33q","outputId":"4ec0c94e-13c4-4449-bf4c-ac20023f89bc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":13,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["中文ＣＨＩＮＥＳＥ o 2020520 alvingmalcob\n","中文ＣＨＩＮＥＳＥoalvingmalcob\n","中文。！＝=.= ＾＾ 2020/5/20 @.@%&*\n","中文\n"]}],"source":["text = \"中文ＣＨＩＮＥＳＥ。！＝=.= ＾o＾ 2020/5/20 alvin@gmal.cob@%&*\"\n","\n","# remove puncs/symbols\n","print(''.join(\n","    [c for c in text if unicodedata.category(c)[0] not in [\"P\", \"S\"]]))\n","\n","# select letters\n","print(''.join([c for c in text if unicodedata.category(c)[0] in [\"L\"]]))\n","\n","# remove alphabets\n","print(''.join(\n","    [c for c in text if unicodedata.category(c)[:2] not in [\"Lu\", 'Ll']]))\n","\n","# select Chinese chars?\n","print(''.join([c for c in text if unicodedata.category(c)[:2] in [\"Lo\"]]))"]},{"cell_type":"markdown","metadata":{"id":"I95fvwdvT33q"},"source":["```{note}\n","Please check [this page](https://www.fileformat.info/info/unicode/category/index.htm) for unicode category names.\n","\n","It seems that the unicode catetory `Lo` is good to identify Chinese characters?\n","\n","We can also make use of the category names to identify punctuations.\n","```"]},{"cell_type":"markdown","metadata":{"id":"dB_d8E-6T33q"},"source":["```{note}\n","[This page](https://www.compart.com/en/unicode/) shows how unicode deals with combining or decomposing character classes.\n","```"]},{"cell_type":"markdown","metadata":{"id":"zRGGnIzQT33q"},"source":["## Special Characters"]},{"cell_type":"markdown","metadata":{"id":"Td50WornT33q"},"source":["- Depending on the research questions and the defined tasks, we often need to decide whether to remove irrelevant characters.\n","- Common irrelevant (aka. non-informative) characters may include:\n","    - Punctuation marks and symbols\n","    - Digits\n","    - Any other non-alphanumeric characters"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"2j8LR3w5T33q","outputId":"60446f28-ddf0-4e48-ad77-6ba535d00007","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":12,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["This is a simple case Removing 1 or 2 symbols is probably ok\n","This is a simple case Removing  or  symbols is probably ok\n"]}],"source":["def remove_special_characters(text, remove_digits=False):\n","    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text\n","\n","s = \"This is a simple case! Removing 1 or 2 symbols is probably ok...:)\"\n","print(remove_special_characters(s))\n","print(remove_special_characters(s, True))"]},{"cell_type":"markdown","metadata":{"id":"xiCj6vBwT33q"},"source":[":::{warning}\n","In the following example, if we use the same `remove_special_characters()` to pre-process the text, what additional problems will we encounter?\n","\n","Any suggestions or better alternative methods?\n",":::"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"vEHL6CIyT33q","outputId":"e38d55e1-da3e-450d-c7e8-fac46bab9809","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":11,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Its a complex sentences and Im not sure if its ok to replace all symbols then  What now\n"]}],"source":["s = \"It's a complex sentences, and I'm not sure if it's ok to replace all symbols then :( What now!!??)\"\n","print(remove_special_characters(s))"]},{"cell_type":"markdown","metadata":{"id":"O89UX81HT33q"},"source":["## Stopwords"]},{"cell_type":"markdown","metadata":{"id":"aS9TiedlT33q"},"source":["- At the word-token level, there are words that have little semantic information and are usually removed from text in text preprocessing. These words are often referred to as **stopwords**.\n","- However, there is no universal stopword list. Whether a word is informative or not depends on your research/project objective. It is a linguistic decision.\n","- The `nltk.corpus.stopwords.words()` provides a standard English language stopwords list."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"WNkAjKaET33q","outputId":"4eae1ad7-9bc2-4ebc-aea1-89a3dcac1c7a","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":10,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["', , stopwords , computer'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["tokenizer = ToktokTokenizer()\n","    # assuming per line per sentence\n","    # for other Tokenizer, maybe sent.tokenize should go first\n","stopword_list = nltk.corpus.stopwords.words('english')\n","\n","## Function: remove stopwords (English)\n","\n","def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopwords]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n","    filtered_text = ' '.join(filtered_tokens)\n","    return filtered_text\n","\n","remove_stopwords(\"The, and, if are stopwords, computer is not\")"]},{"cell_type":"markdown","metadata":{"id":"K1XcxwCtT33q"},"source":["- We can check the languages of the stopwords lists provided by `nltk`."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"vODloWMDT33q","outputId":"4545851c-8d65-4455-bf63-72015711b49b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":10,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['arabic',\n"," 'azerbaijani',\n"," 'basque',\n"," 'bengali',\n"," 'catalan',\n"," 'chinese',\n"," 'danish',\n"," 'dutch',\n"," 'english',\n"," 'finnish',\n"," 'french',\n"," 'german',\n"," 'greek',\n"," 'hebrew',\n"," 'hinglish',\n"," 'hungarian',\n"," 'indonesian',\n"," 'italian',\n"," 'kazakh',\n"," 'nepali',\n"," 'norwegian',\n"," 'portuguese',\n"," 'romanian',\n"," 'russian',\n"," 'slovene',\n"," 'spanish',\n"," 'swedish',\n"," 'tajik',\n"," 'turkish']"]},"metadata":{},"execution_count":32}],"source":["nltk.corpus.stopwords.fileids()"]},{"cell_type":"markdown","metadata":{"id":"YarfSyLzT33r"},"source":["## Redundant Whitespaces"]},{"cell_type":"markdown","metadata":{"id":"NUMVBfAVT33r"},"source":["- Very often we would see redundant duplicate whitespaces in texts.\n","- Sometimes, when we remove special characters (punctuations, digits etc.), we may replace those characters with whitespaces (not empty string), which may lead to duplicate whitespaces in texts."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ryuoolmvT33r","executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":9,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[],"source":["## function: remove redundant spaces\n","def remove_redundant_whitespaces(text):\n","    text = re.sub(r'\\s+',\" \", text)\n","    return text.strip()"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"WZR-t8MbT33r","outputId":"ec408d5d-c909-44a5-dfe1-01a2c5c2d806","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1709151175207,"user_tz":-480,"elapsed":9,"user":{"displayName":"Alvin Chen","userId":"06244732172561186175"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'We are humans and we often have typos.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}],"source":["s = \"We are humans  and we   often have typos.  \"\n","remove_redundant_whitespaces(s)"]},{"cell_type":"markdown","metadata":{"id":"s_CryqEbT33r"},"source":["## Packing things together"]},{"cell_type":"markdown","metadata":{"id":"A7tZ4dh5T33r"},"source":["- Ideally, we can wrap all the relevant steps of text preprocessing in one coherent procedure.\n","\n","- Please study Sarkar's `text_normalizer.py`\n","\n","```\n","def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n","                     accented_char_removal=True, text_lower_case=True,\n","                     text_stemming=False, text_lemmatization=True,\n","                     special_char_removal=True, remove_digits=True,\n","                     stopword_removal=True, stopwords=stopword_list):\n","                     \n","                     ## Your codes here\n","    return corpus_normalized\n","```\n","    "]},{"cell_type":"markdown","metadata":{"id":"gLsImgxvT33r"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"Xf_J6LIsT33r"},"source":["- Sarkar (2020), Chapter 3."]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"python-notes","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}