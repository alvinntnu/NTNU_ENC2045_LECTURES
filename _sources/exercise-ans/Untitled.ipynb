{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)\n",
    "def letter_extractor(x):\n",
    "        feats = []\n",
    "        y = x[0] + \" \" + x[0:2] + \" \" + x[0:3] + \" \" + x[-3:] + \" \" + x[-2:] + \" \" + x[-1]\n",
    "        nmlen = str(len(x))\n",
    "        feats.append(y + ' ' + nmlen)\n",
    "        if re.search(r\"[aeiou][^aeiou]{2}[aeiou]\\b\", x):\n",
    "            feats.append(\"VccV\")\n",
    "        if re.search(r\"[aeiou]{2}\\b\", x):\n",
    "            feats.append(\"VV\")\n",
    "        if x[-1] in ['a','e','i']:\n",
    "            feats.append(\"_fem_\")\n",
    "        if x[-1] == 'o':\n",
    "            feats.append(\"_masc_\")\n",
    "        #return feats\n",
    "        concat_feats = \" \".join(i for i in feats)\n",
    "        return concat_feats\n",
    "    \n",
    "def text_vec(data):\n",
    "    ready = []\n",
    "    if type(data) == str:\n",
    "        b = letter_extractor(data)\n",
    "        ready.append(b)\n",
    "    elif type(data[0]) == str:\n",
    "        for i in data:\n",
    "            y = letter_extractor(i)\n",
    "            ready.append(y)\n",
    "    return ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)\n",
    "def letter_extractor(x):\n",
    "        feats = []\n",
    "        y = x[0] + \" \" + x[0:2] + \" \" + x[0:3] + \" \" + x[-3:] + \" \" + x[-2:] + \" \" + x[-1]\n",
    "        nmlen = str(len(x))\n",
    "        feats.append(y + ' ' + nmlen)\n",
    "        if re.search(r\"[aeiou][^aeiou]{2}[aeiou]\\b\", x):\n",
    "            feats.append(\"VccV\")\n",
    "        if re.search(r\"[aeiou]{2}\\b\", x):\n",
    "            feats.append(\"VV\")\n",
    "        if x[-1] in ['a','e','i']:\n",
    "            feats.append(\"_fem_\")\n",
    "        if x[-1] == 'o':\n",
    "            feats.append(\"_masc_\")\n",
    "        concat_feats = \" \".join(i for i in feats)\n",
    "        return concat_feats\n",
    "\n",
    "    \n",
    "def text_vec(data):\n",
    "    ready = []\n",
    "    if type(data) == str:\n",
    "        b = letter_extractor(data)\n",
    "        ready.append(b)\n",
    "    elif type(data[0]) == str:\n",
    "        for i in data:\n",
    "            y = letter_extractor(i)\n",
    "            ready.append(y)\n",
    "    return ready\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "train, test = train_test_split(labeled_names, test_size = 0.33, random_state=42)\n",
    "\n",
    "X_train = [i[0] for i in train]\n",
    "X_test = [i[0] for i in test]\n",
    "y_train = [i[1] for i in train]\n",
    "y_test = [i[1] for i in test]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "feat_ext = text_vec(X_train) #applying feature extraction to list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Al Alb sis is s 7'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_extractor('Albasis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_extractor(x):\n",
    "        feats = ''\n",
    "        y = x[0] + \" \" + x[0:2] + \" \" + x[0:3] + \" \" + x[-3:] + \" \" + x[-2:] + \" \" + x[-1]\n",
    "#         nmlen = str(len(x))\n",
    "#         feats.append(y + ' ' + nmlen)\n",
    "        feats += y\n",
    "#         if re.search(r\"[aeiou][^aeiou]{2}[aeiou]\\b\", x):\n",
    "#             feats+=\" VccV\"\n",
    "#         if re.search(r\"[aeiou]{2}\\b\", x):\n",
    "#             feats+=\" VV\"\n",
    "#         if x[-1] in ['a','e','i']:\n",
    "#             feats+=\" _fem_\"\n",
    "#         if x[-1] == 'o':\n",
    "#             feats+=\" _masc_\"\n",
    "        feats_tok = feats.strip().split(' ')\n",
    "        return feats_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 'Ma', 'Mar', 'rco', 'co', 'o', 'VccV', '_masc_']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_extractor(X_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(n):\n",
    "    return [n[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vee']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer('Alvee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(tokenizer=letter_extractor)#, min_df = 10, token_pattern = r'[a-zA-Z0-9_]+', lowercase = False)\n",
    "tfidf_feats_train = tfidf_vec.fit_transform(X_train) #create tfidf of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aac',\n",
       " 'aak',\n",
       " 'aam',\n",
       " 'aar',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'abb',\n",
       " 'abd',\n",
       " 'abe',\n",
       " 'abi',\n",
       " 'abn',\n",
       " 'abr',\n",
       " 'abs',\n",
       " 'aby',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'ace',\n",
       " 'ach',\n",
       " 'aci',\n",
       " 'ack',\n",
       " 'aco',\n",
       " 'acy',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'add',\n",
       " 'ade',\n",
       " 'adi',\n",
       " 'adl',\n",
       " 'ado',\n",
       " 'adr',\n",
       " 'ady',\n",
       " 'ae',\n",
       " 'ael',\n",
       " 'aer',\n",
       " 'af',\n",
       " 'afa',\n",
       " 'ag',\n",
       " 'aga',\n",
       " 'age',\n",
       " 'agg',\n",
       " 'agh',\n",
       " 'agn',\n",
       " 'ago',\n",
       " 'agr',\n",
       " 'agu',\n",
       " 'ah',\n",
       " 'ahm',\n",
       " 'ai',\n",
       " 'aia',\n",
       " 'aid',\n",
       " 'aig',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'ais',\n",
       " 'aj',\n",
       " 'aja',\n",
       " 'ak',\n",
       " 'ake',\n",
       " 'ako',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alb',\n",
       " 'ald',\n",
       " 'ale',\n",
       " 'alf',\n",
       " 'alg',\n",
       " 'ali',\n",
       " 'all',\n",
       " 'alm',\n",
       " 'alo',\n",
       " 'alp',\n",
       " 'als',\n",
       " 'alt',\n",
       " 'alv',\n",
       " 'alw',\n",
       " 'aly',\n",
       " 'am',\n",
       " 'ama',\n",
       " 'amb',\n",
       " 'ame',\n",
       " 'ami',\n",
       " 'amm',\n",
       " 'ams',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'and',\n",
       " 'ane',\n",
       " 'ang',\n",
       " 'ani',\n",
       " 'anj',\n",
       " 'ank',\n",
       " 'ann',\n",
       " 'ano',\n",
       " 'ans',\n",
       " 'ant',\n",
       " 'anu',\n",
       " 'any',\n",
       " 'anz',\n",
       " 'ao',\n",
       " 'ap',\n",
       " 'apo',\n",
       " 'app',\n",
       " 'apr',\n",
       " 'ar',\n",
       " 'ara',\n",
       " 'arb',\n",
       " 'arc',\n",
       " 'ard',\n",
       " 'are',\n",
       " 'ari',\n",
       " 'arj',\n",
       " 'ark',\n",
       " 'arl',\n",
       " 'arm',\n",
       " 'arn',\n",
       " 'aro',\n",
       " 'arp',\n",
       " 'arr',\n",
       " 'ars',\n",
       " 'art',\n",
       " 'arv',\n",
       " 'ary',\n",
       " 'as',\n",
       " 'asa',\n",
       " 'ase',\n",
       " 'ash',\n",
       " 'asi',\n",
       " 'ass',\n",
       " 'ast',\n",
       " 'at',\n",
       " 'ata',\n",
       " 'ate',\n",
       " 'ath',\n",
       " 'ati',\n",
       " 'ato',\n",
       " 'att',\n",
       " 'aty',\n",
       " 'au',\n",
       " 'aub',\n",
       " 'aud',\n",
       " 'aug',\n",
       " 'aul',\n",
       " 'aun',\n",
       " 'aur',\n",
       " 'aus',\n",
       " 'aux',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'ave',\n",
       " 'avi',\n",
       " 'avo',\n",
       " 'avr',\n",
       " 'avy',\n",
       " 'aw',\n",
       " 'awn',\n",
       " 'ax',\n",
       " 'axe',\n",
       " 'axi',\n",
       " 'ay',\n",
       " 'aya',\n",
       " 'aye',\n",
       " 'ayl',\n",
       " 'ayn',\n",
       " 'az',\n",
       " 'azi',\n",
       " 'b',\n",
       " 'ba',\n",
       " 'bab',\n",
       " 'bah',\n",
       " 'bai',\n",
       " 'bal',\n",
       " 'bam',\n",
       " 'ban',\n",
       " 'bar',\n",
       " 'bas',\n",
       " 'bat',\n",
       " 'bax',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bba',\n",
       " 'bbe',\n",
       " 'bbi',\n",
       " 'bby',\n",
       " 'be',\n",
       " 'bea',\n",
       " 'bec',\n",
       " 'bee',\n",
       " 'bei',\n",
       " 'bek',\n",
       " 'bel',\n",
       " 'ben',\n",
       " 'beo',\n",
       " 'ber',\n",
       " 'bes',\n",
       " 'bet',\n",
       " 'bev',\n",
       " 'bey',\n",
       " 'bh',\n",
       " 'bha',\n",
       " 'bhu',\n",
       " 'bi',\n",
       " 'bia',\n",
       " 'bie',\n",
       " 'bif',\n",
       " 'bil',\n",
       " 'bin',\n",
       " 'bir',\n",
       " 'bit',\n",
       " 'bj',\n",
       " 'bjo',\n",
       " 'bl',\n",
       " 'bla',\n",
       " 'ble',\n",
       " 'bli',\n",
       " 'blo',\n",
       " 'bly',\n",
       " 'bo',\n",
       " 'bob',\n",
       " 'bod',\n",
       " 'bog',\n",
       " 'bon',\n",
       " 'bor',\n",
       " 'bot',\n",
       " 'boy',\n",
       " 'br',\n",
       " 'bra',\n",
       " 'bre',\n",
       " 'bri',\n",
       " 'bro',\n",
       " 'bru',\n",
       " 'bry',\n",
       " 'bs',\n",
       " 'bu',\n",
       " 'bub',\n",
       " 'buc',\n",
       " 'bud',\n",
       " 'buf',\n",
       " 'bun',\n",
       " 'bur',\n",
       " 'bus',\n",
       " 'but',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'byl',\n",
       " 'byr',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cac',\n",
       " 'cae',\n",
       " 'cah',\n",
       " 'cai',\n",
       " 'cal',\n",
       " 'cam',\n",
       " 'can',\n",
       " 'cap',\n",
       " 'car',\n",
       " 'cas',\n",
       " 'cat',\n",
       " 'cay',\n",
       " 'cca',\n",
       " 'ce',\n",
       " 'cea',\n",
       " 'cec',\n",
       " 'cee',\n",
       " 'cei',\n",
       " 'cel',\n",
       " 'cer',\n",
       " 'ces',\n",
       " 'cey',\n",
       " 'ch',\n",
       " 'cha',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'chl',\n",
       " 'cho',\n",
       " 'chr',\n",
       " 'cht',\n",
       " 'chy',\n",
       " 'ci',\n",
       " 'cia',\n",
       " 'cic',\n",
       " 'cie',\n",
       " 'cil',\n",
       " 'cin',\n",
       " 'cio',\n",
       " 'cis',\n",
       " 'ck',\n",
       " 'cka',\n",
       " 'cke',\n",
       " 'cki',\n",
       " 'cky',\n",
       " 'cl',\n",
       " 'cla',\n",
       " 'cle',\n",
       " 'cli',\n",
       " 'clo',\n",
       " 'co',\n",
       " 'cob',\n",
       " 'cod',\n",
       " 'coe',\n",
       " 'col',\n",
       " 'con',\n",
       " 'coo',\n",
       " 'cor',\n",
       " 'cos',\n",
       " 'cot',\n",
       " 'cou',\n",
       " 'cov',\n",
       " 'cr',\n",
       " 'cra',\n",
       " 'cre',\n",
       " 'cri',\n",
       " 'cry',\n",
       " 'ct',\n",
       " 'cu',\n",
       " 'cur',\n",
       " 'cus',\n",
       " 'cy',\n",
       " 'cyb',\n",
       " 'cyn',\n",
       " 'cyr',\n",
       " 'd',\n",
       " 'da',\n",
       " 'dac',\n",
       " 'dad',\n",
       " 'daf',\n",
       " 'dag',\n",
       " 'dah',\n",
       " 'dai',\n",
       " 'dal',\n",
       " 'dam',\n",
       " 'dan',\n",
       " 'dap',\n",
       " 'dar',\n",
       " 'das',\n",
       " 'dat',\n",
       " 'dav',\n",
       " 'daw',\n",
       " 'dd',\n",
       " 'dda',\n",
       " 'dde',\n",
       " 'ddi',\n",
       " 'ddy',\n",
       " 'de',\n",
       " 'dea',\n",
       " 'deb',\n",
       " 'ded',\n",
       " 'dee',\n",
       " 'deh',\n",
       " 'dei',\n",
       " 'del',\n",
       " 'dem',\n",
       " 'den',\n",
       " 'deo',\n",
       " 'der',\n",
       " 'des',\n",
       " 'dev',\n",
       " 'dew',\n",
       " 'dex',\n",
       " 'dey',\n",
       " 'dge',\n",
       " 'di',\n",
       " 'dia',\n",
       " 'dic',\n",
       " 'did',\n",
       " 'die',\n",
       " 'dil',\n",
       " 'dim',\n",
       " 'din',\n",
       " 'dio',\n",
       " 'dir',\n",
       " 'dis',\n",
       " 'dit',\n",
       " 'dix',\n",
       " 'dly',\n",
       " 'dm',\n",
       " 'dmi',\n",
       " 'dna',\n",
       " 'dne',\n",
       " 'do',\n",
       " 'dod',\n",
       " 'dol',\n",
       " 'dom',\n",
       " 'don',\n",
       " 'dor',\n",
       " 'dos',\n",
       " 'dot',\n",
       " 'dou',\n",
       " 'dov',\n",
       " 'dow',\n",
       " 'dr',\n",
       " 'dra',\n",
       " 'dre',\n",
       " 'dro',\n",
       " 'drs',\n",
       " 'dru',\n",
       " 'dry',\n",
       " 'ds',\n",
       " 'dt',\n",
       " 'du',\n",
       " 'duk',\n",
       " 'dul',\n",
       " 'dun',\n",
       " 'dur',\n",
       " 'dus',\n",
       " 'dw',\n",
       " 'dwa',\n",
       " 'dwi',\n",
       " 'dy',\n",
       " 'dya',\n",
       " 'dye',\n",
       " 'dyl',\n",
       " 'dyn',\n",
       " 'dys',\n",
       " 'e',\n",
       " \"e'\",\n",
       " \"e'l\",\n",
       " 'ea',\n",
       " 'ead',\n",
       " 'eah',\n",
       " 'eal',\n",
       " 'ean',\n",
       " 'ear',\n",
       " 'eas',\n",
       " 'eau',\n",
       " 'eb',\n",
       " 'eba',\n",
       " 'ebb',\n",
       " 'ebe',\n",
       " 'ebi',\n",
       " 'ebo',\n",
       " 'eca',\n",
       " 'ece',\n",
       " 'ech',\n",
       " 'ed',\n",
       " 'eda',\n",
       " 'edd',\n",
       " 'ede',\n",
       " 'edg',\n",
       " 'edi',\n",
       " 'edn',\n",
       " 'edo',\n",
       " 'edu',\n",
       " 'edw',\n",
       " 'edy',\n",
       " 'ee',\n",
       " 'eed',\n",
       " 'eel',\n",
       " 'eem',\n",
       " 'een',\n",
       " 'ees',\n",
       " 'eet',\n",
       " 'eev',\n",
       " 'ef',\n",
       " 'efa',\n",
       " 'efe',\n",
       " 'eg',\n",
       " 'egb',\n",
       " 'eh',\n",
       " 'ehu',\n",
       " 'ei',\n",
       " 'eif',\n",
       " 'eil',\n",
       " 'eim',\n",
       " 'ein',\n",
       " 'eir',\n",
       " 'ej',\n",
       " 'ek',\n",
       " 'eka',\n",
       " 'eko',\n",
       " 'eks',\n",
       " 'el',\n",
       " 'ela',\n",
       " 'elb',\n",
       " 'eld',\n",
       " 'ele',\n",
       " 'elf',\n",
       " 'elg',\n",
       " 'eli',\n",
       " 'elk',\n",
       " 'ell',\n",
       " 'elm',\n",
       " 'eln',\n",
       " 'elo',\n",
       " 'elr',\n",
       " 'els',\n",
       " 'elt',\n",
       " 'elv',\n",
       " 'elw',\n",
       " 'ely',\n",
       " 'em',\n",
       " 'ema',\n",
       " 'eme',\n",
       " 'emi',\n",
       " 'eml',\n",
       " 'emm',\n",
       " 'emo',\n",
       " 'emp',\n",
       " 'emy',\n",
       " 'en',\n",
       " 'ena',\n",
       " 'end',\n",
       " 'ene',\n",
       " 'eng',\n",
       " 'eni',\n",
       " 'enn',\n",
       " 'enr',\n",
       " 'ens',\n",
       " 'ent',\n",
       " 'eny',\n",
       " 'eo',\n",
       " 'eol',\n",
       " 'eon',\n",
       " 'ep',\n",
       " 'epe',\n",
       " 'eph',\n",
       " 'epi',\n",
       " 'er',\n",
       " 'era',\n",
       " 'erd',\n",
       " 'ere',\n",
       " 'erg',\n",
       " 'erh',\n",
       " 'eri',\n",
       " 'erk',\n",
       " 'erl',\n",
       " 'erm',\n",
       " 'ern',\n",
       " 'ero',\n",
       " 'ers',\n",
       " 'ert',\n",
       " 'ery',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'ese',\n",
       " 'esh',\n",
       " 'esm',\n",
       " 'ess',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eta',\n",
       " 'ete',\n",
       " 'etf',\n",
       " 'eth',\n",
       " 'etr',\n",
       " 'ett',\n",
       " 'eu',\n",
       " 'eud',\n",
       " 'eug',\n",
       " 'eul',\n",
       " 'eun',\n",
       " 'eup',\n",
       " 'eus',\n",
       " 'ev',\n",
       " 'eva',\n",
       " 'eve',\n",
       " 'evi',\n",
       " 'evv',\n",
       " 'evy',\n",
       " 'ew',\n",
       " 'ewa',\n",
       " 'ewe',\n",
       " 'ex',\n",
       " 'exa',\n",
       " 'exi',\n",
       " 'ey',\n",
       " 'eya',\n",
       " 'eyd',\n",
       " 'ez',\n",
       " 'eza',\n",
       " 'eze',\n",
       " 'f',\n",
       " 'fa',\n",
       " 'fae',\n",
       " 'fai',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'fat',\n",
       " 'fau',\n",
       " 'faw',\n",
       " 'fax',\n",
       " 'fay',\n",
       " 'faz',\n",
       " 'fe',\n",
       " 'fed',\n",
       " 'fel',\n",
       " 'fen',\n",
       " 'feo',\n",
       " 'fer',\n",
       " 'fey',\n",
       " 'ff',\n",
       " 'ffi',\n",
       " 'ffy',\n",
       " 'fi',\n",
       " 'fia',\n",
       " 'fid',\n",
       " 'fie',\n",
       " 'fil',\n",
       " 'fin',\n",
       " 'fio',\n",
       " 'fit',\n",
       " 'fl',\n",
       " 'fle',\n",
       " 'fli',\n",
       " 'flo',\n",
       " 'fo',\n",
       " 'fon',\n",
       " 'for',\n",
       " 'fow',\n",
       " 'fr',\n",
       " 'fra',\n",
       " 'fre',\n",
       " 'fri',\n",
       " 'fry',\n",
       " 'fty',\n",
       " 'fu',\n",
       " 'ful',\n",
       " 'fus',\n",
       " 'fy',\n",
       " 'fyo',\n",
       " 'g',\n",
       " 'ga',\n",
       " 'gab',\n",
       " 'gae',\n",
       " 'gai',\n",
       " 'gal',\n",
       " 'gam',\n",
       " 'gan',\n",
       " 'gar',\n",
       " 'gas',\n",
       " 'gat',\n",
       " 'gav',\n",
       " 'gay',\n",
       " 'ge',\n",
       " 'gea',\n",
       " 'gel',\n",
       " 'gen',\n",
       " 'geo',\n",
       " 'ger',\n",
       " 'get',\n",
       " 'ggi',\n",
       " 'ggy',\n",
       " 'gh',\n",
       " 'gha',\n",
       " 'ghn',\n",
       " 'ght',\n",
       " 'gi',\n",
       " 'gia',\n",
       " 'gib',\n",
       " 'gid',\n",
       " 'gie',\n",
       " 'gif',\n",
       " 'gil',\n",
       " 'gin',\n",
       " 'gio',\n",
       " 'gip',\n",
       " 'gis',\n",
       " 'git',\n",
       " 'giu',\n",
       " 'gl',\n",
       " 'gla',\n",
       " 'gle',\n",
       " 'glo',\n",
       " 'gly',\n",
       " 'gna',\n",
       " 'go',\n",
       " 'god',\n",
       " 'gol',\n",
       " 'gon',\n",
       " 'gor',\n",
       " 'got',\n",
       " 'gr',\n",
       " 'gra',\n",
       " 'gre',\n",
       " 'gri',\n",
       " 'gro',\n",
       " 'gs',\n",
       " 'gsy',\n",
       " 'gt',\n",
       " 'gu',\n",
       " 'gue',\n",
       " 'gug',\n",
       " 'gui',\n",
       " 'gun',\n",
       " 'gus',\n",
       " 'gut',\n",
       " 'gw',\n",
       " 'gwe',\n",
       " 'gwy',\n",
       " 'gy',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'hag',\n",
       " 'hah',\n",
       " 'hai',\n",
       " 'hal',\n",
       " 'ham',\n",
       " 'han',\n",
       " 'hap',\n",
       " 'har',\n",
       " 'has',\n",
       " 'hat',\n",
       " 'hav',\n",
       " 'hay',\n",
       " 'haz',\n",
       " 'he',\n",
       " 'hea',\n",
       " 'hed',\n",
       " 'hee',\n",
       " 'hei',\n",
       " 'hel',\n",
       " 'hem',\n",
       " 'hen',\n",
       " 'heo',\n",
       " 'hep',\n",
       " 'her',\n",
       " 'hes',\n",
       " 'het',\n",
       " 'hev',\n",
       " 'hew',\n",
       " 'hey',\n",
       " 'hez',\n",
       " 'hi',\n",
       " 'hia',\n",
       " 'hie',\n",
       " 'hig',\n",
       " 'hil',\n",
       " 'hin',\n",
       " 'hir',\n",
       " 'hn',\n",
       " 'hna',\n",
       " 'hne',\n",
       " 'hny',\n",
       " 'ho',\n",
       " 'hol',\n",
       " 'hom',\n",
       " 'hon',\n",
       " 'hor',\n",
       " 'how',\n",
       " 'hoy',\n",
       " 'hry',\n",
       " 'ht',\n",
       " 'hu',\n",
       " 'hua',\n",
       " 'hud',\n",
       " 'hue',\n",
       " 'hug',\n",
       " 'hul',\n",
       " 'hum',\n",
       " 'hun',\n",
       " 'hur',\n",
       " 'hus',\n",
       " 'hy',\n",
       " 'hya',\n",
       " 'hye',\n",
       " 'hyl',\n",
       " 'hyn',\n",
       " 'i',\n",
       " 'ia',\n",
       " 'iag',\n",
       " 'iah',\n",
       " 'iai',\n",
       " 'ial',\n",
       " 'iam',\n",
       " 'ian',\n",
       " 'ias',\n",
       " 'ib',\n",
       " 'ibb',\n",
       " 'ic',\n",
       " 'ica',\n",
       " 'ice',\n",
       " 'ich',\n",
       " 'ici',\n",
       " 'ick',\n",
       " 'ico',\n",
       " 'ict',\n",
       " 'id',\n",
       " 'ida',\n",
       " 'ide',\n",
       " 'idi',\n",
       " 'ie',\n",
       " 'ied',\n",
       " 'iee',\n",
       " 'iel',\n",
       " 'ien',\n",
       " 'ier',\n",
       " 'iet',\n",
       " 'iew',\n",
       " 'if',\n",
       " 'iff',\n",
       " 'ig',\n",
       " 'iga',\n",
       " 'ige',\n",
       " 'igg',\n",
       " 'igh',\n",
       " 'igi',\n",
       " 'ign',\n",
       " 'igo',\n",
       " 'ijo',\n",
       " 'ik',\n",
       " 'ika',\n",
       " 'ike',\n",
       " 'iki',\n",
       " 'iko',\n",
       " 'il',\n",
       " 'ila',\n",
       " 'ild',\n",
       " 'ile',\n",
       " 'ili',\n",
       " 'ill',\n",
       " 'ilo',\n",
       " 'ils',\n",
       " 'ilt',\n",
       " 'ilu',\n",
       " 'ily',\n",
       " 'im',\n",
       " 'ima',\n",
       " 'ime',\n",
       " 'imi',\n",
       " 'imm',\n",
       " 'imo',\n",
       " 'in',\n",
       " 'ina',\n",
       " 'inc',\n",
       " 'ind',\n",
       " 'ine',\n",
       " 'ing',\n",
       " 'ini',\n",
       " 'inn',\n",
       " 'ino',\n",
       " 'ins',\n",
       " 'int',\n",
       " 'io',\n",
       " 'ioa',\n",
       " 'iol',\n",
       " 'ion',\n",
       " 'ior',\n",
       " 'iot',\n",
       " 'ip',\n",
       " 'ipa',\n",
       " 'ipe',\n",
       " 'ipp',\n",
       " 'ir',\n",
       " 'ira',\n",
       " 'ird',\n",
       " 'ire',\n",
       " 'iri',\n",
       " 'irk',\n",
       " 'irl',\n",
       " 'irm',\n",
       " 'iro',\n",
       " 'irv',\n",
       " 'irw',\n",
       " 'is',\n",
       " 'isa',\n",
       " 'ise',\n",
       " 'ish',\n",
       " 'isi',\n",
       " 'ism',\n",
       " 'iso',\n",
       " 'isr',\n",
       " 'iss',\n",
       " 'ist',\n",
       " 'isy',\n",
       " 'it',\n",
       " 'ita',\n",
       " 'ite',\n",
       " 'ith',\n",
       " 'ito',\n",
       " 'its',\n",
       " 'itt',\n",
       " 'ity',\n",
       " 'itz',\n",
       " 'ius',\n",
       " 'iv',\n",
       " 'iva',\n",
       " 'ive',\n",
       " 'ivi',\n",
       " 'ivo',\n",
       " 'ivy',\n",
       " 'ix',\n",
       " 'ixi',\n",
       " 'iya',\n",
       " 'iz',\n",
       " 'iza',\n",
       " 'izz',\n",
       " 'j',\n",
       " 'ja',\n",
       " 'jac',\n",
       " 'jad',\n",
       " 'jae',\n",
       " 'jah',\n",
       " 'jai',\n",
       " 'jak',\n",
       " 'jam',\n",
       " 'jan',\n",
       " 'jaq',\n",
       " 'jar',\n",
       " 'jas',\n",
       " 'jav',\n",
       " 'jay',\n",
       " 'jaz',\n",
       " 'je',\n",
       " 'jea',\n",
       " 'jeb',\n",
       " 'jed',\n",
       " 'jef',\n",
       " 'jeh',\n",
       " 'jel',\n",
       " 'jem',\n",
       " 'jen',\n",
       " 'jep',\n",
       " 'jer',\n",
       " 'jes',\n",
       " 'jet',\n",
       " 'jew',\n",
       " 'ji',\n",
       " 'jie',\n",
       " 'jil',\n",
       " 'jim',\n",
       " 'jin',\n",
       " 'jo',\n",
       " 'jo-',\n",
       " 'joa',\n",
       " 'job',\n",
       " 'joc',\n",
       " 'jod',\n",
       " 'joe',\n",
       " 'joh',\n",
       " 'joi',\n",
       " 'joj',\n",
       " 'jol',\n",
       " 'jon',\n",
       " 'jor',\n",
       " 'jos',\n",
       " 'jot',\n",
       " 'jou',\n",
       " 'joy',\n",
       " 'ju',\n",
       " 'jua',\n",
       " 'jud',\n",
       " 'jue',\n",
       " 'jul',\n",
       " 'jun',\n",
       " 'jus',\n",
       " 'jy',\n",
       " 'jyo',\n",
       " 'k',\n",
       " 'ka',\n",
       " 'kaa',\n",
       " 'kac',\n",
       " 'kah',\n",
       " 'kai',\n",
       " 'kak',\n",
       " 'kal',\n",
       " 'kam',\n",
       " 'kan',\n",
       " 'kar',\n",
       " 'kas',\n",
       " 'kat',\n",
       " 'kay',\n",
       " 'ke',\n",
       " 'kee',\n",
       " 'kei',\n",
       " 'kel',\n",
       " 'ken',\n",
       " 'ker',\n",
       " 'kes',\n",
       " 'ket',\n",
       " 'kev',\n",
       " 'key',\n",
       " 'ki',\n",
       " 'kia',\n",
       " 'kie',\n",
       " 'kil',\n",
       " 'kim',\n",
       " 'kin',\n",
       " 'kip',\n",
       " 'kir',\n",
       " 'kis',\n",
       " 'kit',\n",
       " 'kiz',\n",
       " 'kki',\n",
       " 'kky',\n",
       " 'kl',\n",
       " 'kla',\n",
       " 'kle',\n",
       " 'kn',\n",
       " 'kno',\n",
       " 'ko',\n",
       " 'kob',\n",
       " 'kon',\n",
       " ...]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review ID-63:\n",
      "--------------------------------------------------\n",
      "Review Text:\n",
      " Durant\n",
      "--------------------------------------------------\n",
      "Probability(male) = 0.9513949375131412\n",
      "Probability(female) = 0.048605062486858634\n",
      "Predicted class: ['male']\n",
      "True class: male\n"
     ]
    }
   ],
   "source": [
    "chk_mod = text_vec(X_test)\n",
    "#tfidf_feats_test = tfidf_vec.transform(chk_mod)\n",
    "\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "#text_vec_trfmr = FunctionTransformer(text_vec)\n",
    "pipeline = Pipeline([\n",
    "    #(\"text_vectorizer\", text_vec_trfmr),\n",
    "    (\"feature_vectorizer\", tfidf_vec),\n",
    "    (\"classifier\", svm.SVC(C=4, kernel='linear',probability=True))])\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "import textwrap\n",
    "names_test = X_test\n",
    "gender_test = y_test\n",
    "\n",
    "idx = 63\n",
    "text_sample = X_test[idx]\n",
    "class_names = ['female', 'male']\n",
    "\n",
    "print('Review ID-{}:'.format(idx))\n",
    "print('-'*50)\n",
    "print('Review Text:\\n', textwrap.fill(text_sample,400))\n",
    "print('-'*50)\n",
    "print('Probability(male) =', pipeline.predict_proba([text_sample])[0,1])\n",
    "print('Probability(female) =', pipeline.predict_proba([text_sample])[0,0])\n",
    "print('Predicted class: %s' % pipeline.predict([text_sample]))\n",
    "print('True class: %s' % y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Al Alv vin in n 5']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec('Alvin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-64d0e7d47ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m explanation = explainer.explain_instance(text_sample,       \n\u001b[1;32m      9\u001b[0m                                          \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                          num_features=1)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    413\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[1;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             distance_metric=distance_metric)\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1879\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-7c662f4ba06f>\u001b[0m in \u001b[0;36mletter_extractor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mletter_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#         nmlen = str(len(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         feats.append(y + ' ' + nmlen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300\n",
    "%matplotlib inline\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names, char_level=True)\n",
    "\n",
    "# Error is occurring here,but traces all the way back to the letter_extractor function created earlier\n",
    "explanation = explainer.explain_instance(text_sample,       \n",
    "                                         pipeline.predict_proba,\n",
    "                                         num_features=1)\n",
    "explanation.show_in_notebook(text=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8444248 0.1555752]]\n",
      "['female']\n",
      "[[0.92922597 0.07077403]\n",
      " [0.15516181 0.84483819]\n",
      " [0.9250923  0.0749077 ]\n",
      " [0.18206058 0.81793942]]\n",
      "['female' 'male' 'female' 'male']\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.predict_proba(text_sample))\n",
    "print(pipeline.predict(text_sample))\n",
    "print(pipeline.predict_proba(X_train[0:4]))\n",
    "print(pipeline.predict(X_train[0:4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
