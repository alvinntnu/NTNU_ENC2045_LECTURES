{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In deep learning NLP, sequence models are often the most widely adopted methods.\n",
    "- In this tutorial, we will go over the intuitions of sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural network expects an numeric input, i.e., a numeric representation of the input text.\n",
    "- So the first step in deep learning is the same as traditional ML, which is text representation.\n",
    "- And because a sequence model eats in one word as the input, word representation is the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Representations in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like in traditional machine learning, feature engineering is crucial to the success of the computational modeling.\n",
    "- Of particular importance is the transformation of each text into numeric representation that has a significant portion of **textual semantics**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-hot encoding is the most intuitive way to represent lexical words numerically.\n",
    "- If the language vocabulary size is *V*, each word can be represented as a vector of size *V*, with its correpsonding dimension to be the value of **1** and the rest being **0**'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-1hot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main problem with is one-hot encoding is that the semantic distances in-between words are all the same, i.e., D(mice,rats)=D(mice, horses)= 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now via neural network, we can learn **word embeddings** automatically. (See Word Embeddings notes).\n",
    "- These word embeddings allows us to perform computation of lexical semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.97575491, 0.16937447],\n",
       "       [0.97575491, 1.        , 0.30567806],\n",
       "       [0.16937447, 0.30567806, 1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "mice = np.array([0.2,0.0, 0.8, 0.1, 0.0])\n",
    "rats = np.array([0.2,0.1,0.9, 0.3,0.0])\n",
    "horses = np.array([0.0,0.0,0.1,0.9,0.8])\n",
    "cosine_similarity([mice, rats, horses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each dimension may not have specific semantic fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From One-hot to Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-1hot-to-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN) Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that each word is represented by embeddings, we can represent words as ebmeddings, and build a RNN language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-rnn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The RNN Langage model takes the word at each time step as the input and output the predicted next word.\n",
    "- And the output at time step *i* becomes the input of the RNN at time step *i+1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-rnn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss function of the RNN Language Model is the distance between the predicted word, *y*, and the correct next-word (in its one-hot form).\n",
    "- The training of the RNN language model is thus to minimize the sum of the **cross-entroy** at all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-rnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For example, if the target next work is *cat*, it's one-hot representation is `[0, 0, 0, 1 ,0]`, and the RNN LM predicted *y* is `[0.2, 0.1, 0.1, 0.4, 0.2]`, we can compute the **cross-entropy** at this time step as follows.\n",
    "\n",
    "$$\n",
    "E = - \\sum_{k}t_k \\log{y_k}\n",
    "$$\n",
    "\n",
    "- k: refers to the dimensions of the one-hot vectors\n",
    "- t: refers to the target next word\n",
    "- y: refers to the predicted y from the RNN LM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9162904818741863"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "\n",
    "t = [0.0, 0.0, 0.0, 1.0, 0.0]\n",
    "y = [0.2, 0.1, 0.1, 0.4, 0.2]\n",
    "\n",
    "cross_entropy(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And we compute the average cross-entropy errors across all time steps.\n",
    "\n",
    "$$\n",
    "E = - \\frac{1}{N}\\sum_n\\sum_{k}t_{nk} \\log{y_{nk}}\n",
    "$$\n",
    "\n",
    "- N: the number of words in the input text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the defined **loss function**, we can learn how good our current model is in the training process (i.e., the distance between the true target and the predicted label).\n",
    "- In deep learning, we can use **back propogation** to find out:\n",
    "    - how each parameter of the RNN LM is connected to the loss function\n",
    "    - or, which parameter of the RNN LM contribute to the change of the loss function more\n",
    "    - And therefore, we can **adjust** the parameters of the RNN LM accordingly.\n",
    "    - The algorthm often used is called **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/seq-rnn4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many-to-Many \n",
    "\n",
    "![](../images/seq-m2m.png)\n",
    "![](../images/seq-m2m-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many-to-One\n",
    "\n",
    "![](../images/seq-m21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-to-Many\n",
    "\n",
    "![](../images/seq-12m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- These lecture notes are based on a talk presented by Ananth Sankar: [Sequence to Sequence Learning with Encoder-Decoder Neural Network Models](https://confengine.com/conferences/odsc-india-2019/proposal/10176/sequence-to-sequence-learning-with-encoder-decoder-neural-network-models). Graphs used here are taken from Dr. Sankar's slides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
