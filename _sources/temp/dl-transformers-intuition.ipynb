{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsED9dYaaqdv"
   },
   "source": [
    "# Transformers: Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTQbYzuVaqd5"
   },
   "source": [
    "![](../images/transformers-ex.png)\n",
    "(Source: http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqRZh9kdaqd6"
   },
   "source": [
    "- Transformers are a very exciting development in deep learning NLP.\n",
    "- It can be seen as an important architecture in deep learning that allows the model to learn things from the co-occurring contexts of words.\n",
    "- Most importantly, this mechanism enables the model to effectively model the long distance dependency relations in languages, which have long been a difficult task in traditional statistical NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Attention is All We Need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the encoder-decoder sequence model, as the input sequence length grows, more and more of the inputs are compressed into one vector (e.g., the last time step hidden state of the LSTM).\n",
    "- The intermediate hidden states of the inputs out of the encoder are NOT easily available to the decoder.\n",
    "- That is, the encoder states are mostly hidden from the decoder.\n",
    "- The goal of **Attention layer** is to make available the hidden states of the encoder at all time steps to the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Attention\n",
    "\n",
    "- The **General Attention** mechanism allows the decoder to focus its attention on a subset of the encoded input vectors while decoding.\n",
    "- The **Self Attention** enables the connections between different encodings of input tokens in different positions.\n",
    "    - Each attention mechanism may need a way to determine for a given token, how much it is dependent on the other tokens of the input. This is referred to as the attention weights.\n",
    "    - There are several ways for attention weights computation: **Bahdanau Attention**, **Luong's** Attention.\n",
    "    - The key is that the output of the Attention Layer is a weighted version of the input vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDqDtEjAaqd6"
   },
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-Ib_DDkaqd7"
   },
   "source": [
    "- The fundamental component of a transformer is the **self-attention** mechanism.\n",
    "- Self-attention is like a sequence-to-sequence model, where an input sequence goes in and an output sequence comes out.\n",
    "- The main characteristics of self-attention is when determining every token of the output sequence, it considers not only one particular token of the input sequence, but all the other input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT8bq44Uaqd7"
   },
   "source": [
    "- In other words, each output token, $y_i$, is a weighted average over all the input tokens .\n",
    "\n",
    "$$\n",
    "y_i = \\sum_jw_{ij}x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2w5fcJpaqd7"
   },
   "source": [
    "![](../images/transformers-self-attention.svg)\n",
    "(Source: http://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB-xTdTXaqd8"
   },
   "source": [
    "## From Self-Attention to Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhTp8vP-aqd9"
   },
   "source": [
    "- A **transformer** is an architecture that builds upon self-attention layers.\n",
    "- Peter Bloem's definition of transformers:\n",
    "\n",
    "> Any architecture designed to process a connected set of units--such as the tokens in a sequence or the pixels in an image--where the only interaction between units is through self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S4iRy3Haqd9"
   },
   "source": [
    "![](../images/transformer-block.svg)\n",
    "(Source: http://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oI-1xd4aqd-"
   },
   "source": [
    "- A transformer block combines the self-attention layer with a local feedforward network and add normalization and residual connections.\n",
    "- Normalization and residual connections are standard tricks used to help neural network train faster and more accurately.\n",
    "- A transformer block can also have **multiheaded attention layers**, which multiple self-attention layers to keep track of different types of long-distance relationships between input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW9F6Igsaqd-"
   },
   "source": [
    "## From Transformers to Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqvQ0KG9aqd-"
   },
   "source": [
    "- With the transformer blocks, the most common way to build a classifier is to have a architecture consisting of a large chain of transformer blocks.\n",
    "- All we need to do is work out how to feed the input sequences into the architecture and how to transform the final output sequence into a single classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xriIskCYaqd_"
   },
   "source": [
    "![](../images/transformers-classifier.svg)\n",
    "(Source: http://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8a88YWKaqd_"
   },
   "source": [
    "- The trick in the classifier is to apply global average pooling to the final output sequence, and map the result to a softmaxed class vector.\n",
    "    - The output sequence is averaged to produce a single vector.\n",
    "    - This vector is then projected down to a vector with one element per class and softmaxed into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geHE-2Vbaqd_"
   },
   "source": [
    "## Token Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CN3CtxQaqeA"
   },
   "source": [
    "- The above operation of transformers does not take into account the relative positions of tokens in each sequence. \n",
    "- The output sequence may therefore be the same no matter how the tokens of the input sequence vary in order. (The model is **permutation invariant**).\n",
    "- To fix this, most transformers models create **position embeddings** or **position encodings** for each token of the sequence to:\n",
    "    - represent the position of the word/token in the current sequence\n",
    "    - add this to word/token embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghMJKAXraqeA"
   },
   "source": [
    "## Famous Transformers-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjJtfC-YaqeA"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxd8psiaqeA"
   },
   "source": [
    "- Prior embeddings models are mostly context-free; BERT, however, is claimed to be considering contexts in its language model. BERT was developed by Google Research in May 2019.\n",
    "- The paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n",
    "- BERT refers to Bi-directional Encoder Representations from Transformers.\n",
    "- BERT consists of a simple stacks of transformer blocks.\n",
    "- It is pre-trained on a large general-domain corpus consisting of 800M words from English books and 2.5B words of Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u30F3IhcaqeB"
   },
   "source": [
    "- BERT pretraining features two language tasks:\n",
    "    - **Masking**: A certain number of words in the input sequences are randomly masked out and the model is to learn to predict which words have been modified and what the original words are for each input sequence.\n",
    "    - **Next Sequence Classification**: Two sequences (around 256 words) are sampled from the corpus which may follow each other directly in the corpus, or are taken from random places. The model needs to learn which case it would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XN81KH4kaqeB"
   },
   "source": [
    "- BERT utilizes **WordPiece** tokenization. Each token is somewhere in between word-level and character level sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viqjQMrBaqeB"
   },
   "source": [
    "- With this pretrained BERT, we can add signle task-specific layer after the stach of transformer blocks, which maps the general purpose representation to a task specific output (e.g., binary classification).\n",
    "- The model then will be fine-tuned for that particular task at hand. (**transfer learning**!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gobvpz67aqeB"
   },
   "source": [
    "- Statistics of the large BERT model:\n",
    "    - Transformer blocks: 24\n",
    "    - Sequence length: 256(?)\n",
    "    - Embedding dimension: 1024\n",
    "    - Attention heads: 16\n",
    "    - Parameter number: 340M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rud6k6nHfHSz"
   },
   "source": [
    "#### English Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2977,
     "status": "ok",
     "timestamp": 1615456032533,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "iZ1hBp4Ta1fD",
    "outputId": "dca33e9d-d4fb-45a5-ff3f-bd10955fba2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "## Google Colab Setting\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5818,
     "status": "ok",
     "timestamp": 1615456035396,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "4_5ACiHnaqeC"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer_dbert = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model_dbert = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5809,
     "status": "ok",
     "timestamp": 1615456035398,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "XDXIn6zsaqeE",
    "outputId": "a6f259cd-bef5-45dd-850e-55ecc72a5291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.2 ms, sys: 4.02 ms, total: 60.2 ms\n",
      "Wall time: 59.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = f\"China and Taiwan are two {tokenizer_dbert.mask_token} countries.\"\n",
    "input = tokenizer_dbert.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "mask_token_index = torch.where(input==tokenizer_dbert.mask_token_id)[1]\n",
    "token_logits = model_dbert(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7165,
     "status": "ok",
     "timestamp": 1615456036767,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "Dz8S6uJ0e3bV",
    "outputId": "e67273ae-fd20-40c8-b537-aabd927b245d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China and Taiwan are two sister countries.\n",
      "China and Taiwan are two partner countries.\n",
      "China and Taiwan are two neighbouring countries.\n",
      "China and Taiwan are two neighboring countries.\n",
      "China and Taiwan are two member countries.\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "...     print(text.replace(tokenizer_dbert.mask_token, tokenizer_dbert.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzGZZfO7hIKy"
   },
   "source": [
    "#### Chinese examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7383,
     "status": "ok",
     "timestamp": 1615456036992,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "LaHMcCROhLgy"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer_zh_albert = AutoTokenizer.from_pretrained(\"ckiplab/albert-tiny-chinese\")\n",
    "model_zh_albert = AutoModelForMaskedLM.from_pretrained(\"ckiplab/albert-tiny-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7375,
     "status": "ok",
     "timestamp": 1615456036993,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "x73savm1hNcB",
    "outputId": "c721b730-8df1-44a1-83da-366d218f7ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 ms, sys: 889 µs, total: 16.2 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = f\"台灣與中國是兩個{tokenizer_zh_albert.mask_token}的國家。\"\n",
    "input = tokenizer_zh_albert.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "mask_token_index = torch.where(input==tokenizer_zh_albert.mask_token_id)[1]\n",
    "token_logits = model_zh_albert(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7362,
     "status": "ok",
     "timestamp": 1615456036994,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "nTaM7VsZhzxn",
    "outputId": "0a25f0d9-7eea-44b0-b55d-b75e785874da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台灣與中國是兩個新的國家。\n",
      "台灣與中國是兩個國的國家。\n",
      "台灣與中國是兩個好的國家。\n",
      "台灣與中國是兩個洲的國家。\n",
      "台灣與中國是兩個大的國家。\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "...     print(text.replace(tokenizer_zh_albert.mask_token, tokenizer_zh_albert.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psquPQAUaqeF"
   },
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejcIY-UkaqeF"
   },
   "source": [
    "- GPT-2 is famous (notorious) in the news media as the \"[malicious writing AI](https://www.bbc.com/news/technology-47249163)\".\n",
    "- Different from BERT, GPT-2 is fundamentally a language **generation** model.\n",
    "- GPT-2 features its the linguistic diversity of their training data (e.g., posts and links via the social media site *Reddit* with a minimum level of social support, i.e., 按讚數).\n",
    "- Statistics of GPT-2:\n",
    "    - Transformer blocks: 48\n",
    "    - Sequence length: 1024\n",
    "    - Ebmedding dimension: 1600\n",
    "    - Attention heads: 36\n",
    "    - Parameter number: 1.5B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ11mr3ZfOBo"
   },
   "source": [
    "#### English Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11975,
     "status": "ok",
     "timestamp": 1615456041616,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "kAp_2GgDaqeG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_en_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "#model = AutoModelWithLMHead.from_pretrained(\"gpt2-xl\")\n",
    "model_en_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15899,
     "status": "ok",
     "timestamp": 1615456045547,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "T7cfi-CwaqeH",
    "outputId": "2c9ad4e0-8d8b-4020-c866-928520648015"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Taiwan and China are two independent\n",
      "countries. The Republic of China has\n",
      "claimed the status of a democratic\n",
      "nation for 100 years, in accordance with\n",
      "China's state laws, but has been engaged\n",
      "in a civil war with the Chinese mainland\n",
      "for nearly a decade.  In the past year,\n",
      "North Korea's rocket launch has been\n",
      "repeatedly monitored by South Korean\n",
      "media outlets. It has not been\n",
      "independently confirmed.<|endoftext|>\n",
      "CPU times: user 3.89 s, sys: 25.6 ms, total: 3.92 s\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Taiwan and China are two independent countries\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input = tokenizer_en_gpt2.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model_en_gpt2.generate(input, max_length=100, do_sample=True,top_p=0.95, top_k=60)\n",
    "\n",
    "# Print the output\n",
    "print(textwrap.fill('\\n'+tokenizer_en_gpt2.decode(output[0]),40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ4xi99ffymi"
   },
   "source": [
    "#### Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 19437,
     "status": "ok",
     "timestamp": 1615456049095,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "PNpYoEnsf320"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_zh_gpt2 = AutoTokenizer.from_pretrained(\"ckiplab/gpt2-base-chinese\")\n",
    "\n",
    "#model = AutoModelWithLMHead.from_pretrained(\"gpt2-xl\")\n",
    "model_zh_gpt2 = AutoModelForCausalLM.from_pretrained(\"ckiplab/gpt2-base-chinese\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38403,
     "status": "ok",
     "timestamp": 1615456068071,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "5lgjulZ_gFQn",
    "outputId": "65eae19a-21cd-46f9-c39a-984dbdb89f58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#text = \"打從一開始，我就不想信人工智慧，因為\"\n",
    "text = \"老太太把眼鏡往下一移，從眼鏡上面朝屋內四處張望了一圈，然後又把眼鏡往上抬著，從眼鏡下面往屋外瞧。她平時很少、甚至從來沒有透過眼鏡去找像一個小男孩這樣小傢伙。對她來說，自己這副做工考究的眼鏡是地位的象徵，它的裝飾價值遠遠超出了實用價值，其實，即使戴上兩片爐蓋也照樣看得一清二楚。\"\n",
    "# Tokenize the input string\n",
    "input = tokenizer_zh_gpt2.encode(text, add_special_tokens=False,return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model_zh_gpt2.generate(input, max_length=500, \n",
    "                                do_sample=True,top_p=0.95, top_k=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38396,
     "status": "ok",
     "timestamp": 1615456068073,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "BBj6lvEaqzBM",
    "outputId": "373a1fbe-122b-47a4-c538-c81110328555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 老 太 太 把 眼 鏡 往 下 一 移 ， 從 眼 鏡 上 面 朝 屋 內 四\n",
      "處 張 望 了 一 圈 ， 然 後 又 把 眼 鏡 往 上 抬 著 ， 從 眼\n",
      "鏡 下 面 往 屋 外 瞧 。 她 平 時 很 少 、 甚 至 從 來 沒 有\n",
      "透 過 眼 鏡 去 找 像 一 個 小 男 孩 這 樣 小 傢 伙 。 對 她\n",
      "來 說 ， 自 己 這 副 做 工 考 究 的 眼 鏡 是 地 位 的 象 徵\n",
      "， 它 的 裝 飾 價 值 遠 遠 超 出 了 實 用 價 值 ， 其 實 ，\n",
      "即 使 戴 上 兩 片 爐 蓋 也 照 樣 看 得 一 清 二 楚 。 她 們\n",
      "一 共 只 見 到 了 六 分 鐘 的 眼 鏡 ， 就 有 兩 片 小 男 孩\n",
      "眼 鏡 。 這 時 ， 一 位 自 稱 是 [UNK] [UNK] 的 眼 鏡\n",
      "， 卻 有 一 塊 不 亮 的 小 男 孩 ， 在 沒 有 任 何 人 都 能\n",
      "看 到 的 情 況 下 ， 眼 鏡 可 能 是 不 存 在 的 ， 眼 鏡 能\n",
      "在 幾 秒 內 變 回 更 多 的 距 離 。 眼 鏡 中 的 鏡 面 亮 度\n",
      "最 高 ， 僅 有 6 米 ， 是 世 界 上 最 高 。 她 的 眼 睛 和\n",
      "地 球 之 間 的 不 同 屬 於 全 球 性 的 ， 這 對 這 種 眼 鏡\n",
      "是 最 好 的 一 種 。 由 於 眼 睛 的 顏 色 以 往 是 棕 色 、\n",
      "深 色 、 光 色 和 黃 色 ， 與 以 往 的 鏡 眼 並 不 相 同 。\n",
      "她 可 以 辨 識 許 多 不 同 顏 色 的 人 ， 例 如 美 國 女 性\n",
      "， 以 及 來 自 英 國 的 女 性 ， 不 會 像 一 個 更 為 聰 明\n",
      "的 男 性 所 感 到 的 奇 怪 、 更 危 險 。 眼 鏡 也 會 像\n",
      "[UNK]. net ， 比 如 將 眼 鏡 塗 抹 在 人 眼 上 的 人 更\n",
      "為 亮 眼 。 她 會 在 人 眼 上 塗 抹 的 是 綠 色 ， 以 防 止\n",
      "眼 睛 內 出 現 不 明 原 因 ， 最 終 它 們 會 被 破 壞 。 她\n",
      "用 眼 睛 塗 抹 的 是 紅 色 或 黑 色 。 她 還 被 作 為 女 人\n",
      "臉 部 塗 料 。 但 是 ， 她 還 能 夠 使 用 自 己 的 眼 球 塗\n",
      "抹 。 她 在 看 起 來 是 自 然 光 或 者 是 [UNK]. com 。\n",
      "另 一 個 是 ， 因 為 看 起\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the output\n",
    "print(textwrap.fill('\\n'+ tokenizer_zh_gpt2.decode(output[0]), 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fny-ISh8aqeH"
   },
   "source": [
    "## More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtKg5fY7aqeI"
   },
   "source": [
    "- [Transformer-XL](https://arxiv.org/abs/1901.02860)\n",
    "- The current performance limit is purely in the hardware.\n",
    "- Transformers are generic, waiting to be exploited in many more fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqK_I67faqeI"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1gBhGfFaqeI"
   },
   "source": [
    "- The paper: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- This lecture is Peter Bloem's blog post: [Transformers from Scratch](http://peterbloem.nl/blog/transformers).\n",
    "- Jay Alammar's blog post: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- Jay Alammar's blog post: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dl-transformers-intuition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
