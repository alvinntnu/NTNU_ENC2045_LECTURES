{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/transformers-ex.png)\n",
    "(Source: http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformers are a very exciting development in deep learning NLP.\n",
    "- It can be seen as an important architecture in deep learning that allows the model to learn things from the co-occurring contexts of words.\n",
    "- Most importantly, this mechanism enables the model to effectively model the long distance dependency relations in languages, which have long been a difficult task in traditional statistical NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The fundamental component of a transformer is the **self-attention** mechanism.\n",
    "- Self-attention is like a sequence-to-sequence model, where an input sequence goes in and an output sequence comes out.\n",
    "- The main characteristics of self-attention is when determining every token of the output sequence, it considers not only one particular token of the input sequence, but all the other input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In other words, each output token, $y_i$, is a weighted average over all the input tokens .\n",
    "\n",
    "$$\n",
    "y_i = \\sum_jw_{ij}x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/transformers-self-attention.svg)\n",
    "(Source: http://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Self-Attention to Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **transformer** is an architecture that builds upon self-attention layers.\n",
    "- Peter Bloem's definition of transformers:\n",
    "\n",
    "> Any architecture designed to process a connected set of units--such as the tokens in a sequence or the pixels in an image--where the only interaction between units is through self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/transformer-block.svg)\n",
    "(Source: http://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A transformer block combines the self-attention layer with a local feedforward network and add normalization and residual connections.\n",
    "- Normalization and residual connections are standard tricks used to help neural network train faster and more accurately.\n",
    "- A transformer block can also have **multiheaded attention layers**, which multiple self-attention layers to keep track of different types of long-distance relationships between input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Transformers to Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the transformer blocks, the most common way to build a classifier is to have a architecture consisting of a large chain of transformer blocks.\n",
    "- All we need to do is work out how to feed the input sequences into the architecture and how to transform the final output sequence into a single classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/transformers-classifier.svg)\n",
    "(Source: http://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The trick in the classifier is to apply global average pooling to the final output sequence, and map the result to a softmaxed class vector.\n",
    "    - The output sequence is averaged to produce a single vector.\n",
    "    - This vector is then projected down to a vector with one element per class and softmaxed into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above operation of transformers does not take into account the relative positions of tokens in each sequence. \n",
    "- The output sequence may therefore be the same no matter how the tokens of the input sequence vary in order. (The model is **permutation invariant**).\n",
    "- To fix this, most transformers models create **position embeddings** or **position encodings** for each token of the sequence to:\n",
    "    - represent the position of the word/token in the current sequence\n",
    "    - add this to word/token embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Famous Transformers-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "- BERT consists of a simple stacks of transformer blocks.\n",
    "- It is pre-trained on a large general-domain corpus consisting of 800M words from English books and 2.5B words of Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT pretraining features two language tasks:\n",
    "    - **Masking**: A certain number of words in the input sequences are randomly masked out and the model is to learn to predict which words have been modified and what the original words are for each input sequence.\n",
    "    - **Next Sequence Classification**: Two sequences (around 256 words) are sampled from the corpus which may follow each other directly in the corpus, or are taken from random places. The model needs to learn which case it would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT utilizes **WordPiece** tokenization. Each token is somewhere in between word-level and character level sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With this pretrained BERT, we can add signle task-specific layer after the stach of transformer blocks, which maps the general purpose representation to a task specific output (e.g., binary classification).\n",
    "- The model then will be fine-tuned for that particular task at hand. (**transfer learning**!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistics of the large BERT model:\n",
    "    - Transformer blocks: 24\n",
    "    - Sequence length: 256(?)\n",
    "    - Embedding dimension: 1024\n",
    "    - Attention heads: 16\n",
    "    - Parameter number: 340M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d5cb635adb4ddb9bfe43f98ae7004c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=701.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74969bb6bee4a53a8ada6d8cd20286c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=109540.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4575b300af7f46019d36849c281037f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793e03b6ad754e8abc93aeb5937cfdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2066e693904ad39d1411ff15a690ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=409251346.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer_zh_bert = AutoTokenizer.from_pretrained(\"ckiplab/bert-base-chinese\")\n",
    "\n",
    "model_zh_bert = AutoModelForMaskedLM.from_pretrained(\"ckiplab/bert-base-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 ms, sys: 1.91 ms, total: 3.3 ms\n",
      "Wall time: 3.83 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '等',\n",
       " '到',\n",
       " '潮',\n",
       " '水',\n",
       " '[MASK]',\n",
       " '了',\n",
       " '，',\n",
       " '就',\n",
       " '知',\n",
       " '道',\n",
       " '誰',\n",
       " '沒',\n",
       " '穿',\n",
       " '褲',\n",
       " '子',\n",
       " '。']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text = \"台灣是很美麗的島嶼\"\n",
    "tokenizer_zh_bert.tokenize(text)\n",
    "\n",
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
    "tokenizer_zh_bert.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [CLS] [CLS] 等 到 潮 水 [MASK] 了 ， 就 知 道 誰 沒 穿 褲 子 。 [SEP] 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input string\n",
    "input = tokenizer_zh_bert.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model_zh_bert.generate(input, max_length=50, do_sample=False)\n",
    "\n",
    "# Print the output\n",
    "print('\\n',tokenizer_zh_bert.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT-2 is famous (notorious) in the news media as the \"[malicious writing AI](https://www.bbc.com/news/technology-47249163)\".\n",
    "- Different from BERT, GPT-2 is fundamentally a language **generation** model.\n",
    "- GPT-2 features its the linguistic diversity of their training data (e.g., posts and links via the social media site *Reddit* with a minimum level of social support, i.e., 按讚數).\n",
    "- Statistics of GPT-2:\n",
    "    - Transformer blocks: 48\n",
    "    - Sequence length: 1024\n",
    "    - Ebmedding dimension: 1600\n",
    "    - Attention heads: 36\n",
    "    - Parameter number: 1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_en_gpt2 = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "#model = AutoModelWithLMHead.from_pretrained(\"gpt2-xl\")\n",
    "model_en_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Taiwan and China are two of the world's most important trading partners, and the two countries have\n",
      "CPU times: user 6.39 s, sys: 126 ms, total: 6.51 s\n",
      "Wall time: 6.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Taiwan and China are two\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input = tokenizer_en_gpt2.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model_en_gpt2.generate(input, max_length=20, do_sample=False)\n",
    "\n",
    "# Print the output\n",
    "print('\\n',tokenizer_en_gpt2.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Transformer-XL](https://arxiv.org/abs/1901.02860)\n",
    "- The current performance limit is purely in the hardware.\n",
    "- Transformers are generic, waiting to be exploited in many more fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- This lecture is Peter Bloem's blog post: [Transformers from Scratch](http://peterbloem.nl/blog/transformers).\n",
    "- Jay Alammar's blog post: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- Jay Alammar's blog post: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
