{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs39Dske1RXR"
   },
   "source": [
    "# Machine Translation with Attention Char-based\n",
    "- Word-based machine translation with attention seq-to-seq model\n",
    "- Bahdanau Attention Layber developed in [Thushan](https://github.com/thushv89/attention_keras)\n",
    "- Thushan Ganegedara's\n",
    "[Attention in Deep Networks with Keras](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)\n",
    "- This notebook implements the example of English-to-Chinese neural machine translation. (It took 14h 26min 15s to train the model on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1604270561160,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "en-5vhhf1bcX",
    "outputId": "0676bc70-e453-4138-c172-20c0f7461415"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1604270561161,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "n0q4MgpC21qT"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/content/drive/My Drive/_MySyncDrive/Repository/python-notes/nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1604270561162,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "bGkO78wv3GGc",
    "outputId": "52773412-1c86-4c06-e0ed-6605da86e12e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Alvin/GoogleDrive/_MySyncDrive/Repository/ENC2045/temp'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras # use tensorflow keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "import numpy as np\n",
    "from random import randint, randrange\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Concatenate\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Dense, Masking\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras import Input\n",
    "from keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Hyerparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### hyperparameters\n",
    "batch_size = 128 # Batch size for training\n",
    "latent_dim = 256 # Latent dimensionality of the encoder and decoder\n",
    "epochs = 100 # Number of epochs to train for\n",
    "num_samples=10000 # Number of samples to train on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7bx-mx71RXe"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A character-based processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 19578\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens (char): 73\n",
      "Number of unique output tokens (char): 2640\n",
      "Max sequence length for inputs: 31\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "# Path to the data txt file on disk.\n",
    "data_path = '../../../RepositoryData/data/cmn.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Sort Dictionary\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# Find maxinum sent lengths \n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of lines:', len(lines))\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens (char):', num_encoder_tokens)\n",
    "print('Number of unique output tokens (char):', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input texts\n",
    "input_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK', char_level=True, filters=None, lower=False)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "encoder_input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_maxlen = np.max([len(l) for l in encoder_input_sequences])\n",
    "encoder_input_sequences = pad_sequences(encoder_input_sequences, padding='post', maxlen=input_maxlen)\n",
    "\n",
    "## target texts\n",
    "\n",
    "target_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK', char_level=True, filters=None, lower=False)\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "target_maxlen = np.max([len(l) for l in target_sequences])\n",
    "decoder_sequences = pad_sequences(target_sequences, padding='post', maxlen = target_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 31)\n",
      "(10000, 22)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of Input and Target Sequences\n",
    "print(encoder_input_sequences.shape)\n",
    "print(decoder_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "2642\n"
     ]
    }
   ],
   "source": [
    "# ### vocab size\n",
    "input_vsize = max(input_tokenizer.index_word.keys()) + 1\n",
    "target_vsize = max(target_tokenizer.index_word.keys()) + 1\n",
    "print(input_vsize)\n",
    "print(target_vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "2640\n"
     ]
    }
   ],
   "source": [
    "print(num_encoder_tokens)\n",
    "print(num_decoder_tokens)\n",
    "\n",
    "## The differences between the tokenizer vocab size and the num_encoder_tokens\n",
    "## came from the padding and unknown character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = to_categorical(encoder_input_sequences, num_classes=input_vsize)\n",
    "decoder_data = to_categorical(decoder_sequences, num_classes=target_vsize)\n",
    "\n",
    "decoder_input_data = decoder_data[:, :-1, :]\n",
    "decoder_output_data = decoder_data[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 31, 75)\n",
      "(10000, 21, 2642)\n",
      "(10000, 21, 2642)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices of Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict(\n",
    "    (char, i) for char, i in enumerate(input_tokenizer.word_index))\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (char, i) for char, i in enumerate(target_tokenizer.word_index))\n",
    "\n",
    "input_token_index = dict(\n",
    "    (i, char) for char, i in enumerate(input_tokenizer.word_index))\n",
    "\n",
    "target_token_index = dict(\n",
    "    (i, char) for char, i in enumerate(target_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char index dictionary\n",
    "## char as the key and index as the value\n",
    "input_token_index2 = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index2 = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index2 = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index2 = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Tokens in Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each token of the input and target sequences, convert it into a one-hot encoding vector.\n",
    "- The size of the one-hot vector is the vocabulary size of the input/target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoder/decoder\n",
    "## Both input output are three dimensional tensors,\n",
    "## consisting of each sentence, with all words encoded in one-hot.\n",
    "\n",
    "## Input tensor dimensions: [input_batch_size, input_sequence_length, input_vecob/char_size]\n",
    "encoder_input_data2 = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "## Output tensor dimensions: [output_batch_size, output_sequence_length, output_vecob/char_size]\n",
    "decoder_input_data2 = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data2 = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Masks\n",
    "## Input tensor dimensions: [input_batch_size, input_sequence_length, input_vecob/char_size]\n",
    "encoder_input_data_mask = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "## Output tensor dimensions: [output_batch_size, output_sequence_length, output_vecob/char_size]\n",
    "decoder_input_data_mask = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data_mask = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_inputs_mask(encoder_input_data[0:1,:,:])._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_mask(encoder_input_data[0:1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_texts[:10])\n",
    "print(target_texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode input and output texts\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    \n",
    "    ## One-hot encode input_text\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    \n",
    "    ## End of encoder_input_data\n",
    "    #encoder_input_data[i, t + 1:, input_token_index[' ']] = 1. \n",
    "    encoder_input_data[i, t + 1:, :] = -1. \n",
    "    encoder_input_data[i, t+1:t+2, input_token_index[' ']]= 1.0\n",
    "    \n",
    "    ## One-hot encode target_text\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        \n",
    "        # When t > 0, this is the starting character of decoder_target_data\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    \n",
    "    ## End of decoder_input_data and decoder_output_data\n",
    "#     decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "#     decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "    decoder_input_data[i, t + 1:, :] = -1.\n",
    "    decoder_target_data[i, t:, :] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Masks\n",
    "# One-hot encode input and output texts\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    \n",
    "    ## One-hot encode input_text\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data_mask[i, t, :] = 1.\n",
    "    \n",
    "    ## End of encoder_input_data\n",
    "#     encoder_input_data_mask[i, t+1:, :] = 0 \n",
    "\n",
    "    ## One-hot encode target_text\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data_mask[i, t, :] = 1.\n",
    "        \n",
    "        # When t > 0, this is the starting character of decoder_target_data\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data_mask[i, t - 1, :] = 1.\n",
    "    \n",
    "    # End of decoder_input_data and decoder_output_data\n",
    "#     decoder_input_data_mask[i, t + 1:, :] = 0\n",
    "#     decoder_target_data_mask[i, t:, :] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check  sequence\n",
    "print([reverse_input_char_index.get(np.argmax(i)) for i in list(encoder_input_data[134,:,:])])\n",
    "print([reverse_target_char_index.get(np.argmax(i)) for i in list(decoder_input_data[134,:,:])])\n",
    "print([reverse_target_char_index.get(np.argmax(i)) for i in list(decoder_target_data[134,:,:])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check ont-hot inputs/targets\n",
    "[reverse_input_char_index.get(np.argmax(i)) for i in encoder_input_data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reverse_target_char_index.get(np.argmax(i)) for i in decoder_input_data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reverse_target_char_index.get(np.argmax(i)) for i in decoder_target_data[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs_mask.compute_mask(encoder_input_data[0:1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model\n",
    "\n",
    "## Set up encoder\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) # one word at a time, with vocab_size dimension, i.e., one-hot encoding\n",
    "\n",
    "encoder_inputs_masking = Masking(mask_value=-1.0, input_shape=(None, num_encoder_tokens)) # one word at a time, with vocab_size dimension, i.e., one-hot encoding\n",
    "encoder_inputs_mask = encoder_inputs_masking(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs_mask)\n",
    "\n",
    "\n",
    "# The `encoder_outputs` will be used in AttentionLayer\n",
    "## By default, in LSTM/GRU, when return_outputs=False, the `encoder_outputs` = `state_h`.\n",
    "encoder_states = [state_h, state_c] # Two tensors, states_h and states_c, from LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens)) # one word at a time, with vocab_size dimension,\n",
    "\n",
    "decoder_inputs_masking = Masking(mask_value=-1.0, input_shape=(None, num_decoder_tokens)) # one word at a time, with vocab_size dimension, i.e., one-hot encoding\n",
    "decoder_inputs_mask = decoder_inputs_masking(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences, (i.e, `return_sequences=True`)\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_lstm_outputs, _, _ = decoder_lstm(decoder_inputs_mask,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "## Attention Layer\n",
    "attn_layer = Attention()\n",
    "attn_outputs= attn_layer([decoder_lstm_outputs,encoder_outputs])\n",
    "    ## attention will output a tensor of same shapes as the first tensor in the input\n",
    "\n",
    "\n",
    "## Use both Attention Outputs and Decoder Outputs to make prediction\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_lstm_outputs,attn_outputs])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_dense_time = TimeDistributed(decoder_dense)\n",
    "decoder_outputs = decoder_dense_time(decoder_concat_input)\n",
    "\n",
    "### Note: This output Dense layer does not need to be wrapped within TimeDistributed.\n",
    "### because the Dense layer takes in the decoder LSTM's outputs as the inputs.\n",
    "\n",
    "### In training, the decoder LSTM's inputs would be the entire target sequences\n",
    "###    and its outputs would be the all the hidden states h's.\n",
    "###    and Dense Layer softmax all h's into one-hot of the target language characters\n",
    "\n",
    "### In inferencing, the decoder LSTM's inputs would be one single character of the previous predicted target char\n",
    "###    and its output would be the all the hidden states h's (but in fact only one becuase intput has only 1 char)\n",
    "###    and Dense Layer softmax the h into one-hot of the target language character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model\n",
    "# model.save('keras_models/s2s-cmn-attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "# Plotting results\n",
    "def plot1(history):\n",
    "\n",
    "    matplotlib.rcParams['figure.dpi'] = 100\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc)+1)\n",
    "    ## Accuracy plot\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    ## Loss plot\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot2(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    #plt.gca().set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the model is loaded via external files\n",
    "## Load the encoder_model, decoder_model this way\n",
    "from keras.models import load_model\n",
    "model.load_weights('keras_models/s2s-cmn-attention.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Encoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When using the trained model (Encoder-Decoder), we need to define Encoder and Decoder for inferencing.\n",
    "- For **Encoder Model**:\n",
    "    - We use the input layer of the trained model's Encoder, which includes the encoder_input_data (input sequences one-hot)\n",
    "    - We use the output last_h and last_c from the trained model's Encoder LSTM.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Inference model\n",
    "encoder_inputs = model.input[0] #input_1\n",
    "\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output # trained encoder's lstm outputs\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, encoder_states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(encoder_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **Decoder Model**:\n",
    "    - We use the input layer of the trained model's Decoder, which includes the decoder_input_data (target sequences one-hot)\n",
    "    - We create two more Inputs, representing the Encoder's LSTM output, [last_h and last_c]\n",
    "    - We use the trained Decoder's LSTM. In the training stage, the decoder LSTM takes in the output of Encoder's last_h and last_c as well as the entire decoder_inputs (complete target sequences). It outputs directly the predicted h's at each time step.\n",
    "    - But in inferencing stage, the Inference Decoder decodes one token at a time, and returns the predicted h, as well as the last_h and last_c. These last_h and last_c will turn out to be the initial states of the Inference Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_inputs = model.input[1]\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,)) # state_h\n",
    "decoder_state_input_c = Input(shape=(latent_dim,)) # state_c\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] # concat state_h and state_c\n",
    "\n",
    "decoder_lstm = model.layers[5] ## trained decoder's LSTM\n",
    "decoder_attention = model.layers[6] ## trained decoder's Attention Layer\n",
    "\n",
    "decoder_encoder_outputs_for_attention = Input(shape=(max_encoder_seq_length, latent_dim,))\n",
    "\n",
    "\n",
    "## In training, we use `decoder_ouputs` only.\n",
    "\n",
    "## In inferencing, we need `decoder_c, and decoder_h`\n",
    "## because these c and h form the basis for next decoder input\n",
    "decoder_lstm_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "\n",
    "\n",
    "attn_outputs, attn_weights = decoder_attention([decoder_lstm_outputs,decoder_encoder_outputs_for_attention], return_attention_scores=True)\n",
    "    ## attention will output a tensor of same shapes as the first tensor in the input\n",
    "\n",
    "\n",
    "## Use both Attention Outputs and Decoder Outputs to make prediction\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_lstm_outputs,attn_outputs])\n",
    "\n",
    "\n",
    "decoder_time_dense = model.layers[8] ## trained decoder's TimeDistributed(Dense)\n",
    "decoder_outputs=decoder_time_dense(decoder_concat_input)\n",
    "\n",
    "\n",
    "## Inference Model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, decoder_encoder_outputs_for_attention] + decoder_states_inputs, # target sentence + encoder output h+c\n",
    "    [decoder_outputs,attn_weights]+ decoder_states ) # decoder predicted char + decoder predicted h+c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     \"\"\" Decoder (Inference) model \"\"\"\n",
    "#     decoder_inf_inputs = Input(batch_shape=(batch_size, 1, fr_vsize), name='decoder_word_inputs')\n",
    "#     encoder_inf_states = Input(batch_shape=(batch_size, en_timesteps, hidden_size), name='encoder_inf_states')\n",
    "#     decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_init')\n",
    "\n",
    "#     decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "#     attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "#     decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "#     decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "#     decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "#                           outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(decoder_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the outputs [`last_h`, `last_c`] from the Inference Encoder given the input sequence.\n",
    "- In the decoding stage, initialize the `target_seq` with `\\t` and the initial states of Inference Decoder to be [`last_h`, `last_c`] from Inference Encoder.\n",
    "- During the decoding stage:\n",
    "    - Inference Decoder takes in one `target_seq` and [`last_h`, `last_c`] to predict next `target_seq`.\n",
    "    - At the same time, Inference Decoder returns its [`last_h`, `last_c`].\n",
    "    - These predicted `target_set` and [`last_h`, `last_c`] are recycled to be the inputs of next-round Inference Decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    all_encoder_states, states_value = encoder_model.predict(input_seq) # output: [encoder_last_h, encoder_last_c]\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    \n",
    "    # Within the WHILE-LOOP\n",
    "    ## Update `target_seq`, `states_value, i.e., [h,c]`\n",
    "    while not stop_condition:\n",
    "        # inference starts at the first target char\n",
    "        # first target char + encoder output h + c\n",
    "        output_tokens, output_weights, h, c = decoder_model.predict(\n",
    "            [target_seq, all_encoder_states ] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        ## Choose the output char of the argmax prob\n",
    "        ## one-hot decode the char and append to the `decoded_sentence`\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        ## everytime the target_seq is the cur_t char, one char a time\n",
    "        ## the shape should be [1, ,1 vocab_size]\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        ## the h and c output from decoder at cur_t\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100,120):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts_len = [len(i) for i in input_texts]\n",
    "input_texts_len[np.argmax(input_texts_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index=500\n",
    "\n",
    "input_seq = encoder_input_data[seq_index:seq_index+1,]\n",
    "\n",
    "input_seq_len = input_texts_len[seq_index:seq_index+1]\n",
    "# Encode the input as state vectors.\n",
    "all_encoder_states, states_value = encoder_model.predict(input_seq) # output: [encoder_last_h, encoder_last_c]\n",
    "\n",
    "print(input_texts[seq_index:seq_index+1])\n",
    "print(input_texts_len[seq_index : seq_index+1])\n",
    "print(input_seq)\n",
    "print(all_encoder_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty target sequence of length 1.\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "\n",
    "# Populate the first character of target sequence with the start character.\n",
    "target_seq[0, 0, target_token_index['\\t']] = 1.0\n",
    "\n",
    "# Sampling loop for a batch of sequences\n",
    "# (to simplify, here we assume a batch of size 1).\n",
    "stop_condition = False\n",
    "decoded_sentence = ''\n",
    "\n",
    "decoder_weights=[]\n",
    "\n",
    "# Within the WHILE-LOOP\n",
    "## Update `target_seq`, `states_value, i.e., [h,c]`\n",
    "while not stop_condition:\n",
    "    # inference starts at the first target char\n",
    "    # first target char + encoder output h + c\n",
    "    output_tokens, output_weights, h, c = decoder_model.predict(\n",
    "        [target_seq, all_encoder_states ] + states_value)\n",
    "\n",
    "    decoder_weights.append(output_weights)\n",
    "    # Sample a token\n",
    "    ## Choose the output char of the argmax prob\n",
    "    ## one-hot decode the char and append to the `decoded_sentence`\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "    decoded_sentence += sampled_char\n",
    "\n",
    "    # Exit condition: either hit max length\n",
    "    # or find stop character.\n",
    "    if (sampled_char == '\\n' or\n",
    "       len(decoded_sentence) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "    ## everytime the target_seq is the cur_t char, one char a time\n",
    "    ## the shape should be [1, ,1 vocab_size]\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "    # Update states\n",
    "    ## the h and c output from decoder at cur_t\n",
    "    states_value = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(decoded_sentence)) # for each character in the target\n",
    "print(len(decoder_weights)) # there is an attention weight\n",
    "\n",
    "# each attention weight measures the relevance of each input sequence's character with the target sequence chacter at time t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(decoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=[\"PingFang HK\"]\n",
    "\n",
    "input_char = [reverse_input_char_index.get(np.argmax(i)) for i in input_seq[0,:,:]] ##y-axis\n",
    "target_char = list(decoded_sentence) ## x-axis\n",
    "atten_weights=np.transpose(np.array(decoder_weights).reshape(len(target_char),max_encoder_seq_length)) ## harvest\n",
    "# atten_weights ## \n",
    "\n",
    "\n",
    "## remove padding characters\n",
    "input_char = input_char[:int(input_seq_len[0])]\n",
    "atten_weights = atten_weights[:input_seq_len[0],:]\n",
    "\n",
    "plt.rcParams['font.sans-serif']=[\"PingFang HK\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "im = ax.imshow(atten_weights)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(target_char)))\n",
    "ax.set_yticks(np.arange(len(input_char)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(target_char)\n",
    "ax.set_yticklabels(input_char)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# # Loop over data dimensions and create text annotations.\n",
    "# for i in range(len(input_char)):\n",
    "#     for j in range(len(target_char)):\n",
    "#         text = ax.text(j, i, atten_weights[i, j],\n",
    "#                        ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Attention Weights of Each Target Char\")\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in_texts: a list of input sequences\n",
    "def trans(in_texts):\n",
    "    ## Input tensor dimensions: [input_batch_size, input_sequence_length, input_vecob/char_size]\n",
    "    in_texts_onehot = np.zeros(\n",
    "        (len(in_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "     \n",
    "        dtype='float32')\n",
    "    \n",
    "    ## Ont-hot encoding in_texts\n",
    "    for i, text in enumerate(in_texts):\n",
    "        ## One-hot encode input_text\n",
    "        for t, char in enumerate(text):\n",
    "            in_texts_onehot[i, t, input_token_index[char]] = 1.\n",
    "        ## End of encoder_input_data\n",
    "        in_texts_onehot[i, t + 1:, input_token_index[' ']] = 1. \n",
    "    \n",
    "    # Char Holder\n",
    "    target_texts = []\n",
    "    \n",
    "    \n",
    "    ## Decoding Sequence\n",
    "    for seq_index in range(len(in_texts)):\n",
    "        input_seq = in_texts_onehot[seq_index: seq_index + 1]\n",
    "        target_texts.append(decode_sequence(input_seq))\n",
    "    \n",
    "    \n",
    "    return target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans(['How are you?', 'Ok!', 'Hurry!',\"How's the weather?\", \"My name is Alvin.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define an input sequence and process it.\n",
    "if batch_size:\n",
    "    encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inputs')\n",
    "    decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "else:\n",
    "    encoder_inputs = Input(shape=(en_timesteps, en_vsize), name='encoder_inputs')\n",
    "    if fr_timesteps:\n",
    "        decoder_inputs = Input(shape=(fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        decoder_inputs = Input(shape=(None, fr_vsize), name='decoder_inputs')\n",
    "\n",
    "# Encoder GRU\n",
    "encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "# Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder GRU output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')\n",
    "dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "full_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Further Improve the Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use word embeddings\n",
    "- Increase the training data size\n",
    "- Try other sequence models (GRU)\n",
    "- Try bidirectional RNN\n",
    "- Increase the widths and depths of the sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a6l96PQ1RXS"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4523,
     "status": "ok",
     "timestamp": 1604270564872,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "93dDeBY41RXT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Concatenate\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras import Input\n",
    "from attention import AttentionLayer\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "\n",
    "def clean_en_data(en_text):\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", en_text)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "  w = w.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  #w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "\n",
    "import jieba\n",
    "def clean_zh_data(fr_text):\n",
    "    return [' '.join(jieba.lcut(l)) for l in fr_text]\n",
    "\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "def get_data(data_path, train_test = 0.9):\n",
    "#   data_path = '../../../RepositoryData/data/cmn.txt'\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    en_text=[clean_en_data(l.split('\\t')[0]) for l in lines]\n",
    "    fr_text=[l.split('\\t')[-1] for l in lines]\n",
    "\n",
    "    #fr_text = ['sos ' + sent[:-1] + 'eos .'  if sent.endswith('。') else 'sos ' + sent + ' eos .' for sent in fr_text]\n",
    "\n",
    "    fr_text = clean_zh_data(fr_text)\n",
    "    fr_text = ['sos ' + sent + ' eos' for sent in fr_text]\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    inds = np.arange(len(en_text))\n",
    "    np.random.shuffle(inds)\n",
    "        \n",
    "    train_size = int(round(len(lines)*train_test))\n",
    "    train_inds = inds[:train_size]\n",
    "    test_inds = inds[train_size:]\n",
    "    tr_en_text = [en_text[ti] for ti in train_inds]\n",
    "    tr_fr_text = [fr_text[ti] for ti in train_inds]\n",
    "\n",
    "    ts_en_text = [en_text[ti] for ti in test_inds]\n",
    "    ts_fr_text = [fr_text[ti] for ti in test_inds]\n",
    "    \n",
    "    return tr_en_text, tr_fr_text, ts_en_text, ts_fr_text\n",
    "\n",
    "\n",
    "## when the max_len is known, use this func to convert text to seq\n",
    "def sents2sequences(tokenizer, sentences, reverse=False, pad_length=None, padding_type='post'):\n",
    "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n",
    "    if reverse:\n",
    "        preproc_text = np.flip(preproc_text, axis=1)\n",
    "    return preproc_text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(en_tokenizer, fr_tokenizer, en_text, fr_text):\n",
    "    en_seq = en_tokenizer.texts_to_sequences(tr_en_text)\n",
    "    en_timesteps = np.max([len(l) for l in en_seq])\n",
    "    en_seq = pad_sequences(en_seq, padding='post', maxlen = en_timesteps)\n",
    "    fr_seq = fr_tokenizer.texts_to_sequences(tr_fr_text)\n",
    "    fr_timesteps = np.max([len(l) for l in fr_seq])\n",
    "    fr_seq = pad_sequences(fr_seq, padding='post', maxlen = fr_timesteps)\n",
    "    return en_seq, fr_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4520,
     "status": "ok",
     "timestamp": 1604270564874,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "4iOdp6C31RXX"
   },
   "outputs": [],
   "source": [
    "def define_nmt(hidden_size, batch_size, en_timesteps, en_vsize, fr_timesteps, fr_vsize):\n",
    "    \"\"\" Defining a NMT model \"\"\"\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    if batch_size:\n",
    "        encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(en_timesteps, en_vsize), name='encoder_inputs')\n",
    "        if fr_timesteps:\n",
    "            decoder_inputs = Input(shape=(fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "        else:\n",
    "            decoder_inputs = Input(shape=(None, fr_vsize), name='decoder_inputs')\n",
    "\n",
    "    # Encoder GRU\n",
    "    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "    # Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')\n",
    "    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "    decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "    # Full model\n",
    "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "    full_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    full_model.summary()\n",
    "\n",
    "    \"\"\" Inference model \"\"\"\n",
    "    batch_size = 1\n",
    "\n",
    "    \"\"\" Encoder (Inference) model \"\"\"\n",
    "    encoder_inf_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inf_inputs')\n",
    "    encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n",
    "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
    "\n",
    "    \"\"\" Decoder (Inference) model \"\"\"\n",
    "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, fr_vsize), name='decoder_word_inputs')\n",
    "    encoder_inf_states = Input(batch_shape=(batch_size, en_timesteps, hidden_size), name='encoder_inf_states')\n",
    "    decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_init')\n",
    "\n",
    "    decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "\n",
    "    return full_model, encoder_model, decoder_model\n",
    "\n",
    "def train(full_model, en_seq, fr_seq, batch_size, n_epochs=10):\n",
    "    \"\"\" Training the model \"\"\"\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        losses = []\n",
    "        for bi in range(0, en_seq.shape[0] - batch_size, batch_size):\n",
    "\n",
    "            en_onehot_seq = to_categorical(\n",
    "                en_seq[bi:bi + batch_size, :], num_classes=en_vsize)\n",
    "            fr_onehot_seq = to_categorical(\n",
    "                fr_seq[bi:bi + batch_size, :], num_classes=fr_vsize)\n",
    "\n",
    "            full_model.train_on_batch(\n",
    "                [en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :])\n",
    "\n",
    "            l,a = full_model.evaluate([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :],\n",
    "                                    batch_size=batch_size, verbose=0)\n",
    "\n",
    "            losses.append(l)\n",
    "        if (ep + 1) % 1 == 0:\n",
    "            print(\"Loss in epoch {}: {}\".format(ep + 1, np.mean(losses)))\n",
    "\n",
    "\n",
    "def infer_nmt(encoder_model, decoder_model, test_en_seq, en_vsize, fr_vsize, fr_timesteps):\n",
    "    \"\"\"\n",
    "    Infer logic\n",
    "    :param encoder_model: keras.Model\n",
    "    :param decoder_model: keras.Model\n",
    "    :param test_en_seq: sequence of word ids\n",
    "    :param en_vsize: int\n",
    "    :param fr_vsize: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    test_fr_seq = sents2sequences(fr_tokenizer, ['sos'], fr_vsize)\n",
    "    test_en_onehot_seq = to_categorical(test_en_seq, num_classes=en_vsize)\n",
    "    test_fr_onehot_seq = np.expand_dims(\n",
    "        to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n",
    "\n",
    "    enc_outs, enc_last_state = encoder_model.predict(test_en_onehot_seq)\n",
    "    dec_state = enc_last_state\n",
    "    attention_weights = []\n",
    "    fr_text = ''\n",
    "    for i in range(fr_timesteps):\n",
    "\n",
    "        dec_out, attention, dec_state = decoder_model.predict(\n",
    "            [enc_outs, dec_state, test_fr_onehot_seq])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "\n",
    "        if dec_ind == 0:\n",
    "            break\n",
    "        test_fr_seq = sents2sequences(\n",
    "            fr_tokenizer, [fr_index2word[dec_ind]], fr_vsize)\n",
    "        test_fr_onehot_seq = np.expand_dims(\n",
    "            to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n",
    "\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        fr_text += fr_index2word[dec_ind] + ' '\n",
    "\n",
    "    return fr_text, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4517,
     "status": "ok",
     "timestamp": 1604270564875,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "ekT96bwH1RXb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=[\"PingFang HK\"]\n",
    "def plot_attention_weights(encoder_inputs, attention_weights, en_id2word, fr_id2word, filename=None):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print('Your attention weights was empty. No attention map saved to the disk. ' +\n",
    "              '\\nPlease check if the decoder produced  a proper translation')\n",
    "        return\n",
    "\n",
    "    mats = []\n",
    "    dec_inputs = []\n",
    "    for dec_ind, attn in attention_weights:\n",
    "        mats.append(attn.reshape(-1))\n",
    "        dec_inputs.append(dec_ind)\n",
    "    attention_mat = np.transpose(np.array(mats))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([fr_id2word[inp] if inp != 0 else \"<Res>\" for inp in dec_inputs])\n",
    "    ax.set_yticklabels([en_id2word[inp] if inp != 0 else \"<Res>\" for inp in encoder_inputs.ravel()])\n",
    "\n",
    "    ax.tick_params(labelsize=32)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "#     if not os.path.exists(config.RESULTS_DIR):\n",
    "#         os.mkdir(config.RESULTS_DIR)\n",
    "#     if filename is None:\n",
    "#         plt.savefig( 'attention.png'))\n",
    "#     else:\n",
    "#         plt.savefig(os.path.join(config.RESULTS_DIR, '{}'.format(filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7762,
     "status": "ok",
     "timestamp": 1604270568125,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "gmfdApDw1RXf",
    "outputId": "61d138db-c530-4699-e6f4-d54c1e8668f4"
   },
   "outputs": [],
   "source": [
    "### Get data\n",
    "tr_en_text, tr_fr_text, ts_en_text, ts_fr_text = get_data(data_path='../../../RepositoryData/data/cmn.txt')\n",
    "\n",
    "\"\"\" Defining tokenizers \"\"\"\n",
    "en_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')\n",
    "en_tokenizer.fit_on_texts(tr_en_text)\n",
    "\n",
    "fr_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')\n",
    "fr_tokenizer.fit_on_texts(tr_fr_text)\n",
    "\n",
    "### Getting sequence integer data\n",
    "en_seq, fr_seq = preprocess_data(en_tokenizer, fr_tokenizer, tr_en_text, tr_fr_text)\n",
    "\n",
    "### timestesps\n",
    "en_timesteps = en_seq.shape[1]\n",
    "fr_timesteps = fr_seq.shape[1]\n",
    "\n",
    "### vocab size\n",
    "en_vsize = max(en_tokenizer.index_word.keys()) + 1\n",
    "fr_vsize = max(fr_tokenizer.index_word.keys()) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14418,
     "status": "ok",
     "timestamp": 1604270574789,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "DiA0aWsC1RXj",
    "outputId": "47763c0e-d7d7-426b-d87a-55793592cd63"
   },
   "outputs": [],
   "source": [
    "###\"\"\" Defining the full model \"\"\"\n",
    "full_model, infer_enc_model, infer_dec_model = define_nmt(\n",
    "    hidden_size=hidden_size,\n",
    "    batch_size=batch_size,\n",
    "    en_timesteps=en_timesteps,\n",
    "    fr_timesteps=fr_timesteps,\n",
    "    en_vsize=en_vsize,\n",
    "    fr_vsize=fr_vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 15325,
     "status": "ok",
     "timestamp": 1604270575703,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "Ag_JZSCu1RXn",
    "outputId": "b90d9838-bb12-4ac7-8126-8578573b8d11"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(full_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpG52mJK1RXq",
    "outputId": "170867cb-f8d9-4701-9df0-64c0615c87e3"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train(full_model, en_seq, fr_seq, batch_size, n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8klqxC_1RXu"
   },
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnIhxTm51RXu"
   },
   "outputs": [],
   "source": [
    "# full_model.save('../../../RepositoryData/output/nmt-en-zh/nmt-en-zh-full-model.h5')\n",
    "# infer_enc_model.save('../../../RepositoryData/output/nmt-en-zh/nmt-en-zh-infer-enc-model.h5')\n",
    "# infer_dec_model.save('../../../RepositoryData/output/nmt-en-zh/nmt-en-zh-infer-dec-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3alO12-1RXx"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "full_model.load_weights('../../../RepositoryData/output/nmt-en-zh/nmt-en-zh-full-model.h5')\n",
    "infer_enc_model.load_weights('../../../RepositoryData/output/nmt-en-zh/nmt-en-zh-infer-enc-model.h5')\n",
    "infer_dec_model.load_weights('../../../RepositoryData/output/nmt-en-zh/nmt-en-zh-infer-dec-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvBvOP8x1RXx",
    "outputId": "755c9bbd-21b2-45fb-83fe-437b64b9e63b"
   },
   "outputs": [],
   "source": [
    "plot_model(infer_enc_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAiolzrH1RX0",
    "outputId": "8d769426-ecb2-4cc8-961b-76fc2bed849f"
   },
   "outputs": [],
   "source": [
    "plot_model(infer_dec_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "error",
     "timestamp": 1604270516417,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "6u5tsB_51RX3",
    "outputId": "ed55432b-ee12-41ad-ccd4-b5a007e01b14"
   },
   "outputs": [],
   "source": [
    "\"\"\" Index2word \"\"\"\n",
    "en_index2word = dict(\n",
    "    zip(en_tokenizer.word_index.values(), en_tokenizer.word_index.keys()))\n",
    "fr_index2word = dict(\n",
    "    zip(fr_tokenizer.word_index.values(), fr_tokenizer.word_index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHLDUdfz1RX5"
   },
   "outputs": [],
   "source": [
    "def translate(infer_enc_model, infer_dec_model, test_en_text, \n",
    "              en_vsize, fr_vsize, en_timesteps, fr_timesteps,\n",
    "              en_tokenizer, fr_tokenizer):\n",
    "    \"\"\" Inferring with trained model \"\"\"\n",
    "    test_en = test_en_text\n",
    "    print('Translating: {}'.format(test_en))\n",
    "\n",
    "    test_en_seq = sents2sequences(\n",
    "        en_tokenizer, [test_en], pad_length=en_timesteps)\n",
    "\n",
    "    test_fr, attn_weights = infer_nmt(\n",
    "        encoder_model=infer_enc_model, decoder_model=infer_dec_model,\n",
    "        test_en_seq=test_en_seq, en_vsize=en_vsize, fr_vsize=fr_vsize, fr_timesteps = fr_timesteps)\n",
    "    print('\\tFrench: {}'.format(test_fr))\n",
    "    return test_en_seq, test_fr, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMjTGMjJ1RX8",
    "outputId": "331432ce-0a61-4183-fcfa-aa8073fc14d1"
   },
   "outputs": [],
   "source": [
    "test_en_seq, test_fr, attn_weights=translate(infer_enc_model=infer_enc_model,\n",
    "          infer_dec_model=infer_dec_model,\n",
    "          test_en_text=ts_en_text[2],\n",
    "          en_vsize=en_vsize,\n",
    "          fr_vsize=fr_vsize,\n",
    "          en_timesteps=en_timesteps,\n",
    "          fr_timesteps=fr_timesteps,\n",
    "          en_tokenizer=en_tokenizer,\n",
    "          fr_tokenizer=fr_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    test_en_seq, test_fr, attn_weights=translate(infer_enc_model=infer_enc_model,\n",
    "              infer_dec_model=infer_dec_model,\n",
    "              test_en_text=ts_en_text[i],\n",
    "              en_vsize=en_vsize,\n",
    "              fr_vsize=fr_vsize,\n",
    "              en_timesteps=en_timesteps,\n",
    "              fr_timesteps=fr_timesteps,\n",
    "              en_tokenizer=en_tokenizer,\n",
    "              fr_tokenizer=fr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fr_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPresXWv1RX_",
    "outputId": "9d229d62-78eb-486a-b2bc-5cda8c8435f8"
   },
   "outputs": [],
   "source": [
    "\"\"\" Attention plotting \"\"\"\n",
    "plot_attention_weights(test_en_seq, attn_weights,\n",
    "                       en_index2word, fr_index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMFrKFDj1RYC",
    "outputId": "4af66723-d13b-48c1-a184-b53c674c72b3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def test(full_model, ts_enc_text, ts_dec_text, enc_tokenizer, dec_tokenizer, enc_vsize, dec_vsize, batch_size):\n",
    "    # ### Getting sequence integer data\n",
    "    ts_enc_seq, ts_dec_seq = preprocess_data(enc_tokenizer, dec_tokenizer, ts_enc_text, ts_dec_text)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for bi in range(0, ts_enc_seq.shape[0] - batch_size, batch_size):\n",
    "        enc_onehot_seq = to_categorical(\n",
    "            ts_enc_seq[bi:bi + batch_size, :], num_classes=enc_vsize)\n",
    "        dec_onehot_seq = to_categorical(\n",
    "            ts_dec_seq[bi:bi + batch_size, :], num_classes=dec_vsize)\n",
    "\n",
    "        # full_model.train_on_batch(\n",
    "        #     [enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :])\n",
    "        l,a = full_model.evaluate([enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :],\n",
    "                                batch_size=batch_size, verbose=0)\n",
    "        losses.append(l)\n",
    "        accuracies.append(a)\n",
    "    print('Average Loss:{}'.format(np.mean(losses)))\n",
    "    print('Average Accuracy:{}'.format(np.mean(accuracies)))\n",
    "\n",
    "test(full_model, ts_enc_text = ts_en_text, ts_dec_text = ts_fr_text, \n",
    "     enc_tokenizer = en_tokenizer, dec_tokenizer = fr_tokenizer, enc_vsize = en_vsize, dec_vsize = fr_vsize, batch_size = batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq-to-seq-attention-tg.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "706px",
    "left": "196.325px",
    "top": "196px",
    "width": "317px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
