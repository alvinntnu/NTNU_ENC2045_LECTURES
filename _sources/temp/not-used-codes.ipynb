{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/neural-network-sample2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural network is a model with weights for data/value transformation.\n",
    "- The input data values will be transformed according to the weights of the neural network.\n",
    "- Given a two-layer network, with two input values $x1$ and $x2$, to get the values of the three outputs in the second layer, $a_1^{(1)}$, $a_2^{(1)}$, $a_3^{(1)}$, we compute the dot product of the *X* and *W*.\n",
    "    - *X* refers to the input vector/matrix\n",
    "    - *W* refers to the network weights, which is a 2 x 3 matrix in the current example\n",
    "    - The weights are represented as the links in-between the first and second layers\n",
    "    - These weights can be mathematically represesnted as a 2 x 3 Matrix *W*\n",
    "- Taking the dot product of the input values *X* and the weight matrix *W* is referred to as the **forward propagation** of the network.\n",
    "- Forward propagation gives us the values of the nodes in the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1,2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,3,5],[2,4,6]])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11 17]\n"
     ]
    }
   ],
   "source": [
    "Y = np.dot(X,W)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights, Biases, and Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output of a node in the network is computed as the sum of the weighted inputs and the bias. Take $a^{(1)}_1 $ for example:\n",
    "\n",
    "$$ a^{(1)}_1 = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1$$\n",
    "\n",
    "- Then the output values go through the activation function and this result would indicate the final output of the node.\n",
    "\n",
    "$$ z^{(1)}_1= h(a^{(1)}_1) $$\n",
    "\n",
    "- Not all the nodes need to have an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1,0.2,0.3])\n",
    "\n",
    "print(X.shape)\n",
    "print(W1.shape)\n",
    "print(B1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n",
      "[0.21198272 0.31624106 0.47177622]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.dot(X, W1) + B1\n",
    "print(A1)\n",
    "\n",
    "\n",
    "Z1 = sigmoid(A1)\n",
    "print(Z1)\n",
    "\n",
    "Z2 = softmax(A1)\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "- One Embedding Layer + LSTM + Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "#model3.add(SpatialDropout1D(0.2))\n",
    "model3.add(LSTM(64))# , dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model3.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model3, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(X_train, y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=50, verbose=2,\n",
    "                   validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4\n",
    "\n",
    "- One Embedding Layer + Two Stacked LSTM + Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "model4.add(LSTM(64, return_sequences=True)) #, dropout=0.2, recurrent_dropout=0.2))\n",
    "model4.add(LSTM(64))\n",
    "model4.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model4.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model4,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model4.fit(X_train, y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=50, verbose=2,\n",
    "                   validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5\n",
    "\n",
    "- One Embedding Layer + LSTM [hidden state of last time step + cell state of last time step] + Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "\n",
    "inputs = keras.Input(shape=(max_len,))\n",
    "x=layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len, mask_zero=True)(inputs)\n",
    "#x=layers.SpatialDropout1D(0.2)(x)\n",
    "x_all_h,x_last_h, x_c = layers.LSTM(64, dropout=0.2, \n",
    "                               recurrent_dropout=0.2, \n",
    "                               return_sequences=False, return_state=True)(x)\n",
    "## LSTM Parameters:\n",
    "#     `return_seqeunces=True`: return the hidden states for each time step\n",
    "#     `return_state=True`: return the cell state of the last time step\n",
    "#     When both are set True, the return values of LSTM are:\n",
    "#     (1) the hidden states of all time steps (when `return_sequences=True`) or the hidden state of the last time step\n",
    "#     (2) the hidden state of the last time step\n",
    "#     (3) the cell state of the last time step\n",
    "\n",
    "x = layers.Concatenate(axis=1)([x_last_h, x_c])\n",
    "outputs=layers.Dense(2, activation='softmax')(x)\n",
    "model5 = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "\n",
    "plot_model(model5, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history5 = model5.fit(X_train, y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=50, verbose=2,\n",
    "                   validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(history5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.evaluate(X_test, y_test, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6\n",
    "\n",
    "- Adding AttentionLayer\n",
    "    - Use the hidden state h of the last time step and the cell state c of the last time step\n",
    "    - Check their attention\n",
    "    - And use [attention out + hidden state h of the last time step] for decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "\n",
    "inputs = keras.Input(shape=(max_len,))\n",
    "x=layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len)(inputs)\n",
    "#x=layers.SpatialDropout1D(0.2)(x)\n",
    "x_all_hs, x_last_h, x_last_c = layers.LSTM(64, dropout=0.2, \n",
    "                               recurrent_dropout=0.2, \n",
    "                               return_sequences=True, return_state=True)(x)\n",
    "## LSTM Parameters:\n",
    "#     `return_seqeunces=True`: return the hidden states for each time step\n",
    "#     `return_state=True`: return the cell state of the last time step\n",
    "#     When both are set True, the return values of LSTM are:\n",
    "#     (1) the hidden state of the last time step\n",
    "#     (2) the hidden states of all time steps (when `return_sequences=True`) or the hidden state of the last time step\n",
    "#     (3) the cell state of the last time step\n",
    "\n",
    "\n",
    "atten_out = layers.Attention()([x_last_h, x_last_c])\n",
    "\n",
    "x = layers.Concatenate(axis=1)([x_last_h, atten_out])\n",
    "outputs=layers.Dense(2, activation='softmax')(x)\n",
    "model6 = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "\n",
    "plot_model(model6, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history6 = model6.fit(X_train, y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=50, verbose=2,\n",
    "                   validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq to Seq Models Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1604360740056,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "en-5vhhf1bcX",
    "outputId": "74d168cd-2db6-4faa-bdaf-ebb821a77a89"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 796,
     "status": "ok",
     "timestamp": 1604360740056,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "n0q4MgpC21qT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/_MySyncDrive/Repository/python-notes/nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1604360740057,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "bGkO78wv3GGc",
    "outputId": "51ca4967-71db-454e-d1bb-e6d6e45cc8f6"
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3236,
     "status": "ok",
     "timestamp": 1604360742510,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "iZdTXFBoMeFi",
    "outputId": "bfcac55a-c37d-4550-c333-246398d3070e"
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3231,
     "status": "ok",
     "timestamp": 1604360742510,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "xE3z8vcNG7ur",
    "outputId": "55433773-bb43-40b7-cf25-7294db9919c0"
   },
   "outputs": [],
   "source": [
    "import tensorflow, keras\n",
    "print(tensorflow.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a6l96PQ1RXS"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3225,
     "status": "ok",
     "timestamp": 1604360742511,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "93dDeBY41RXT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "from tensorflow.keras.layers import AdditiveAttention, Attention\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Concatenate\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras import Input\n",
    "from attention import AttentionLayer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "def get_data(data_path, train_test = 0.9):\n",
    "    data_path = '../../../RepositoryData/data/deep-learning-2/addition.txt'\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    enc_text=[l.split('_')[0] for l in lines]\n",
    "    dec_text=[l.split('_')[-1].strip() for l in lines]\n",
    "\n",
    "    dec_text = ['_' + sent + '_' for sent in dec_text]\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    inds = np.arange(len(enc_text))\n",
    "    np.random.shuffle(inds)\n",
    "        \n",
    "    train_size = int(round(len(lines)*train_test))\n",
    "    train_inds = inds[:train_size]\n",
    "    test_inds = inds[train_size:]\n",
    "    tr_enc_text = [enc_text[ti] for ti in train_inds]\n",
    "    tr_dec_text = [dec_text[ti] for ti in train_inds]\n",
    "\n",
    "    ts_enc_text = [enc_text[ti] for ti in test_inds]\n",
    "    ts_dec_text = [dec_text[ti] for ti in test_inds]\n",
    "    \n",
    "    return tr_enc_text, tr_dec_text, ts_enc_text, ts_dec_text\n",
    "\n",
    "\n",
    "## when the max_len is known, use this func to convert text to seq\n",
    "def sents2sequences(tokenizer, sentences, reverse=False, pad_length=None, padding_type='post'):\n",
    "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n",
    "    if reverse:\n",
    "        preproc_text = np.flip(preproc_text, axis=1)\n",
    "    return preproc_text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(enc_tokenizer, dec_tokenizer, enc_text, dec_text):\n",
    "    enc_seq = enc_tokenizer.texts_to_sequences(tr_enc_text)\n",
    "    enc_timesteps = np.max([len(l) for l in enc_seq])\n",
    "    enc_seq = pad_sequences(enc_seq, padding='post', maxlen = enc_timesteps)\n",
    "    dec_seq = dec_tokenizer.texts_to_sequences(tr_dec_text)\n",
    "    dec_timesteps = np.max([len(l) for l in dec_seq])\n",
    "    dec_seq = pad_sequences(dec_seq, padding='post', maxlen = dec_timesteps)\n",
    "    return enc_seq, dec_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3223,
     "status": "ok",
     "timestamp": 1604360742513,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "4iOdp6C31RXX"
   },
   "outputs": [],
   "source": [
    "def define_nmt(hidden_size, batch_size, enc_timesteps, enc_vsize, dec_timesteps, dec_vsize):\n",
    "    \"\"\" Defining a NMT model \"\"\"\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    if batch_size:\n",
    "        encoder_inputs = Input(batch_shape=(batch_size, enc_timesteps, enc_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(batch_shape=(batch_size, dec_timesteps - 1, dec_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(enc_timesteps, enc_vsize), name='encoder_inputs')\n",
    "        if fr_timesteps:\n",
    "            decoder_inputs = Input(shape=(dec_timesteps - 1, dec_vsize), name='decoder_inputs')\n",
    "        else:\n",
    "            decoder_inputs = Input(shape=(None, dec_vsize), name='decoder_inputs')\n",
    "\n",
    "    # Encoder GRU\n",
    "    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "    # Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "    # Attention layer\n",
    "    # attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_layer = AdditiveAttention(name=\"attention_layer\")\n",
    "\n",
    "    ## The input for AdditiveAttention: query, key\n",
    "    ## It returns a tensor of shape as query\n",
    "    ## This is different from the AttentionLayer developed by Thushan\n",
    "    # attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "    attn_out, attn_states  = attn_layer([decoder_out,encoder_out],return_attention_scores=True)\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(dec_vsize, activation='softmax', name='softmax_layer')\n",
    "    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "    decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "    # Full model\n",
    "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "    full_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    full_model.summary()\n",
    "\n",
    "    \"\"\" Inference model \"\"\"\n",
    "    batch_size = 1\n",
    "\n",
    "    \"\"\" Encoder (Inference) model \"\"\"\n",
    "    encoder_inf_inputs = Input(batch_shape=(batch_size, enc_timesteps, enc_vsize), name='encoder_inf_inputs')\n",
    "    encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n",
    "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
    "\n",
    "    \"\"\" Decoder (Inference) model \"\"\"\n",
    "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, dec_vsize), name='decoder_word_inputs')\n",
    "    encoder_inf_states = Input(batch_shape=(batch_size, enc_timesteps, hidden_size), name='encoder_inf_states')\n",
    "    decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_init')\n",
    "\n",
    "    decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "    # attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    attn_inf_out, attn_inf_states  = attn_layer([decoder_inf_out, encoder_inf_states],return_attention_scores=True)\n",
    "\n",
    "    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "\n",
    "    return full_model, encoder_model, decoder_model\n",
    "\n",
    "def train(full_model, enc_seq, dec_seq, batch_size, n_epochs=10):\n",
    "    \"\"\" Training the model \"\"\"\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    for ep in range(n_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for bi in range(0, enc_seq.shape[0] - batch_size, batch_size):\n",
    "\n",
    "            enc_onehot_seq = to_categorical(\n",
    "                enc_seq[bi:bi + batch_size, :], num_classes=enc_vsize)\n",
    "            dec_onehot_seq = to_categorical(\n",
    "                dec_seq[bi:bi + batch_size, :], num_classes=dec_vsize)\n",
    "\n",
    "            full_model.train_on_batch(\n",
    "                [enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :])\n",
    "\n",
    "            l,a = full_model.evaluate([enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :],\n",
    "                                    batch_size=batch_size, verbose=0)\n",
    "\n",
    "            losses.append(l)\n",
    "            accuracies.append(a)\n",
    "        if (ep + 1) % 1 == 0:\n",
    "            print(\"Loss/Accuracy in epoch {}: {}/{}\".format(ep + 1, np.mean(losses), np.mean(accuracies)))\n",
    "            loss_epoch.append(np.mean(losses))\n",
    "            accuracy_epoch.append(np.mean(accuracies))\n",
    "    return loss_epoch, accuracy_epoch\n",
    "\n",
    "\n",
    "def infer_nmt(encoder_model, decoder_model, test_enc_seq, enc_vsize, dec_vsize, dec_timesteps):\n",
    "    \"\"\"\n",
    "    Infer logic\n",
    "    :param encoder_model: keras.Model\n",
    "    :param decoder_model: keras.Model\n",
    "    :param test_en_seq: sequence of word ids\n",
    "    :param en_vsize: int\n",
    "    :param fr_vsize: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    test_dec_seq = sents2sequences(dec_tokenizer, ['_'], dec_vsize)\n",
    "    test_enc_onehot_seq = to_categorical(test_enc_seq, num_classes=enc_vsize)\n",
    "    test_dec_onehot_seq = np.expand_dims(\n",
    "        to_categorical(test_dec_seq, num_classes=dec_vsize), 1)\n",
    "\n",
    "    enc_outs, enc_last_state = encoder_model.predict(test_enc_onehot_seq)\n",
    "    dec_state = enc_last_state\n",
    "    attention_weights = []\n",
    "    dec_text = ''\n",
    "    for i in range(dec_timesteps):\n",
    "\n",
    "        dec_out, attention, dec_state = decoder_model.predict(\n",
    "            [enc_outs, dec_state, test_dec_onehot_seq])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "\n",
    "        if dec_ind == 0:\n",
    "            break\n",
    "        test_dec_seq = sents2sequences(\n",
    "            dec_tokenizer, [dec_index2word[dec_ind]], dec_vsize)\n",
    "        test_dec_onehot_seq = np.expand_dims(\n",
    "            to_categorical(test_dec_seq, num_classes=dec_vsize), 1)\n",
    "\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        dec_text += dec_index2word[dec_ind]\n",
    "\n",
    "    return dec_text, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3602,
     "status": "ok",
     "timestamp": 1604360742896,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "ekT96bwH1RXb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=[\"PingFang HK\"]\n",
    "def plot_attention_weights(encoder_inputs, attention_weights, enc_id2word, dec_id2word, filename=None):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print('Your attention weights was empty. No attention map saved to the disk. ' +\n",
    "              '\\nPlease check if the decoder produced  a proper translation')\n",
    "        return\n",
    "\n",
    "    mats = []\n",
    "    dec_inputs = []\n",
    "    for dec_ind, attn in attention_weights:\n",
    "        mats.append(attn.reshape(-1))\n",
    "        dec_inputs.append(dec_ind)\n",
    "    attention_mat = np.transpose(np.array(mats))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([dec_id2word[inp] if inp != 0 else \"<Res>\" for inp in dec_inputs])\n",
    "    ax.set_yticklabels([enc_id2word[inp] if inp != 0 else \"<Res>\" for inp in encoder_inputs.ravel()])\n",
    "\n",
    "    ax.tick_params(labelsize=32)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "#     if not os.path.exists(config.RESULTS_DIR):\n",
    "#         os.mkdir(config.RESULTS_DIR)\n",
    "#     if filename is None:\n",
    "#         plt.savefig( 'attention.png'))\n",
    "#     else:\n",
    "#         plt.savefig(os.path.join(config.RESULTS_DIR, '{}'.format(filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7bx-mx71RXe"
   },
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zQxZYUx1RXf"
   },
   "source": [
    "### Data Wrangling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4279,
     "status": "ok",
     "timestamp": 1604360743577,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "gmfdApDw1RXf"
   },
   "outputs": [],
   "source": [
    "#### hyperparameters\n",
    "batch_size = 128\n",
    "hidden_size = 256\n",
    "n_epochs = 50\n",
    "\n",
    "### Get data\n",
    "tr_enc_text, tr_dec_text, ts_enc_text, ts_dec_text = get_data(data_path='../../../RepositoryData/data/deep-learning-2/addition.txt')\n",
    "\n",
    "# \"\"\" Defining tokenizers \"\"\"\n",
    "enc_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK', char_level=True)\n",
    "enc_tokenizer.fit_on_texts(tr_enc_text)\n",
    "\n",
    "dec_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK', char_level=True)\n",
    "dec_tokenizer.fit_on_texts(tr_dec_text)\n",
    "\n",
    "# ### Getting sequence integer data\n",
    "enc_seq, dec_seq = preprocess_data(enc_tokenizer, dec_tokenizer, tr_enc_text, tr_dec_text)\n",
    "\n",
    "# ### timestesps\n",
    "enc_timesteps = enc_seq.shape[1]\n",
    "dec_timesteps = dec_seq.shape[1]\n",
    "\n",
    "# ### vocab size\n",
    "enc_vsize = max(enc_tokenizer.index_word.keys()) + 1\n",
    "dec_vsize = max(dec_tokenizer.index_word.keys()) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4275,
     "status": "ok",
     "timestamp": 1604360743578,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "JG2rXnGMVKch",
    "outputId": "9f6e8b5e-bd5f-4c6a-e9af-27196377d617"
   },
   "outputs": [],
   "source": [
    "print(enc_vsize)\n",
    "print(dec_vsize)\n",
    "print(tr_enc_text[:5])\n",
    "print(tr_dec_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5039,
     "status": "ok",
     "timestamp": 1604360744348,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "DiA0aWsC1RXj",
    "outputId": "38449f54-333c-48cc-f254-5a228c4346fc"
   },
   "outputs": [],
   "source": [
    "###\"\"\" Defining the full model \"\"\"\n",
    "full_model, infer_enc_model, infer_dec_model = define_nmt(\n",
    "    hidden_size=hidden_size,\n",
    "    batch_size=batch_size,\n",
    "    enc_timesteps=enc_timesteps,\n",
    "    dec_timesteps=dec_timesteps,\n",
    "    enc_vsize=enc_vsize,\n",
    "    dec_vsize=dec_vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1604360749513,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "Ag_JZSCu1RXn",
    "outputId": "3976a065-fc73-43e6-ab88-b75bc4b3b9c3"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(full_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2265242,
     "status": "ok",
     "timestamp": 1604363026170,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "qpG52mJK1RXq",
    "outputId": "69441d51-2e6e-49b3-f2ce-2a53df14eee4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "loss, accuracy = train(full_model, enc_seq, dec_seq, batch_size, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1604363216312,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "nJ4qK1iBbQP4",
    "outputId": "bf0fdac9-a275-4aef-8381-4e593d532098"
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.plot(range(len(loss)), loss, label='loss')\n",
    "plt.plot(range(len(accuracy)), accuracy, label='accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8klqxC_1RXu"
   },
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1794,
     "status": "ok",
     "timestamp": 1604363226339,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "LnIhxTm51RXu"
   },
   "outputs": [],
   "source": [
    "# full_model.save('../../../RepositoryData/output/seq2seq-attention-addition/full-model.h5')\n",
    "# infer_enc_model.save('../../../RepositoryData/output/seq2seq-attention-addition/infer-enc-model.h5')\n",
    "# infer_dec_model.save('../../../RepositoryData/output/seq2seq-attention-addition/infer-dec-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3alO12-1RXx"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvWmzTNkFfDk"
   },
   "outputs": [],
   "source": [
    "full_model.load_weights('../../../RepositoryData/output/seq2seq-attention-addition/full-model.h5')\n",
    "infer_enc_model.load_weights('../../../RepositoryData/output/seq2seq-attention-addition/infer-enc-model.h5')\n",
    "infer_dec_model.load_weights('../../../RepositoryData/output/seq2seq-attention-addition/infer-dec-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1604363235049,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "cvBvOP8x1RXx",
    "outputId": "572ebc69-8163-41aa-9188-4067f18822ab"
   },
   "outputs": [],
   "source": [
    "plot_model(infer_enc_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1604363238397,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "GAiolzrH1RX0",
    "outputId": "061dc5f9-8d35-4be3-b5c8-d12c2218896b"
   },
   "outputs": [],
   "source": [
    "plot_model(infer_dec_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1604363243133,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "6u5tsB_51RX3"
   },
   "outputs": [],
   "source": [
    "\"\"\" Index2word \"\"\"\n",
    "enc_index2word = dict(\n",
    "    zip(enc_tokenizer.word_index.values(), enc_tokenizer.word_index.keys()))\n",
    "dec_index2word = dict(\n",
    "    zip(dec_tokenizer.word_index.values(), dec_tokenizer.word_index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1604363245897,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "WHLDUdfz1RX5"
   },
   "outputs": [],
   "source": [
    "def translate(infer_enc_model, infer_dec_model, test_enc_text, \n",
    "              enc_vsize, dec_vsize, enc_timesteps, dec_timesteps,\n",
    "              enc_tokenizer, dec_tokenizer):\n",
    "    \"\"\" Inferring with trained model \"\"\"\n",
    "    test_enc = test_enc_text\n",
    "    print('Translating: {}'.format(test_enc))\n",
    "\n",
    "    test_enc_seq = sents2sequences(\n",
    "        enc_tokenizer, [test_enc], pad_length=enc_timesteps)\n",
    "\n",
    "    test_dec, attn_weights = infer_nmt(\n",
    "        encoder_model=infer_enc_model, decoder_model=infer_dec_model,\n",
    "        test_enc_seq=test_enc_seq, enc_vsize=enc_vsize, dec_vsize=dec_vsize, dec_timesteps = dec_timesteps)\n",
    "    print('\\tFrench: {}'.format(test_dec))\n",
    "    return test_enc_seq, test_dec, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1604363249225,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "jMjTGMjJ1RX8",
    "outputId": "7b6b4d03-8c1a-4160-836c-4a2423df1ad7"
   },
   "outputs": [],
   "source": [
    "test_enc_seq, test_dec, attn_weights=translate(infer_enc_model=infer_enc_model,\n",
    "          infer_dec_model=infer_dec_model,\n",
    "          test_enc_text=ts_enc_text[120],\n",
    "          enc_vsize=enc_vsize,\n",
    "          dec_vsize=dec_vsize,\n",
    "          enc_timesteps=enc_timesteps,\n",
    "          dec_timesteps=dec_timesteps,\n",
    "          enc_tokenizer=enc_tokenizer,\n",
    "          dec_tokenizer=dec_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1604363253002,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "OPresXWv1RX_",
    "outputId": "5aa3fcef-bd1c-4827-aefc-7ceb7369cc5d"
   },
   "outputs": [],
   "source": [
    "\"\"\" Attention plotting \"\"\"\n",
    "plot_attention_weights(test_enc_seq, attn_weights,\n",
    "                       enc_index2word, dec_index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1814,
     "status": "ok",
     "timestamp": 1604296824050,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "JMFrKFDj1RYC",
    "outputId": "a09ceb3e-c4ed-42da-8c7c-ebcdd91c1e44"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(tr_enc_text[:5])\n",
    "print(tr_dec_text[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tMXgkUTiw-n"
   },
   "source": [
    "## Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20698,
     "status": "ok",
     "timestamp": 1604363301928,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "qntLn9jUioZl",
    "outputId": "42271664-7a5d-4f3c-ff9a-0fe11fe8e1db"
   },
   "outputs": [],
   "source": [
    "def test(full_model, ts_enc_text, ts_dec_text, enc_tokenizer, dec_tokenizer, batch_size):\n",
    "    # ### Getting sequence integer data\n",
    "    ts_enc_seq, ts_dec_seq = preprocess_data(enc_tokenizer, dec_tokenizer, ts_enc_text, ts_dec_text)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for bi in range(0, ts_enc_seq.shape[0] - batch_size, batch_size):\n",
    "        enc_onehot_seq = to_categorical(\n",
    "            ts_enc_seq[bi:bi + batch_size, :], num_classes=enc_vsize)\n",
    "        dec_onehot_seq = to_categorical(\n",
    "            ts_dec_seq[bi:bi + batch_size, :], num_classes=dec_vsize)\n",
    "\n",
    "        # full_model.train_on_batch(\n",
    "        #     [enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :])\n",
    "        l,a = full_model.evaluate([enc_onehot_seq, dec_onehot_seq[:, :-1, :]], dec_onehot_seq[:, 1:, :],\n",
    "                                batch_size=batch_size, verbose=0)\n",
    "        losses.append(l)\n",
    "        accuracies.append(a)\n",
    "    print('Average Loss:{}'.format(np.mean(losses)))\n",
    "    print('Average Accuracy:{}'.format(np.mean(accuracies)))\n",
    "\n",
    "test(full_model, ts_enc_text = ts_enc_text, ts_dec_text = ts_dec_text, \n",
    "     enc_tokenizer = enc_tokenizer, dec_tokenizer = dec_tokenizer, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
