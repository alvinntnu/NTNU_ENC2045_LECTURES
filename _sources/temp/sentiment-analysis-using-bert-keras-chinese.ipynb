{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using BERT on Chinese Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to perform text classification of spammed mails using the pre-trained BERT model.\n",
    "\n",
    "This example also shows the effectiveness of **transfer learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT in Short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
    "\n",
    "- The BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. \n",
    "- It’s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
    "- In particular, BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.\n",
    "- The size of the large BERT model:\n",
    "    - Transformer blocks: 24\n",
    "    - Embedding dimension: 1024\n",
    "    - Attention heads: 16\n",
    "    - Total number of parameters: 340M\n",
    "\n",
    "- The size of GPT-2 Model:\n",
    "    - Transformer blocks: 48\n",
    "    - Sequence length: 1024\n",
    "    - Embedding dimension: 1600\n",
    "    - Total number of parameters: 1.5B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "# from keras.models import Model\n",
    "# import keras.backend as K\n",
    "# from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# import itertools\n",
    "# from keras.models import load_model\n",
    "# from sklearn.utils import shuffle\n",
    "# from transformers import *\n",
    "# from transformers import BertTokenizer, TFBertModel, BertConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unicode_to_ascii(s):\n",
    "#     return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# def clean_stopwords_shortwords(w):\n",
    "#     stopwords_list=stopwords.words('english')\n",
    "#     words = w.split() \n",
    "#     clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]\n",
    "#     return \" \".join(clean_words) \n",
    "\n",
    "# def preprocess_sentence(w):\n",
    "#     w = unicode_to_ascii(w.lower().strip())\n",
    "#     w = re.sub(r\"([?.!,¿])\", r\" \", w)\n",
    "#     w = re.sub(r'[\" \"]+', \" \", w)\n",
    "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "#     w=clean_stopwords_shortwords(w)\n",
    "#     w=re.sub(r'@\\w+', '',w)\n",
    "#     return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewID</th>\n",
       "      <th>title_CH</th>\n",
       "      <th>title_EN</th>\n",
       "      <th>genre</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviews_sentiword_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review_1</td>\n",
       "      <td>紫羅蘭永恆花園外傳－永遠與自動手記人偶－</td>\n",
       "      <td>Violet Evergarden - Eternity and the Auto Memo...</td>\n",
       "      <td>動畫</td>\n",
       "      <td>negative</td>\n",
       "      <td>唉，踩雷了，浪費時間，不推 唉，踩雷了，浪費時間，不推</td>\n",
       "      <td>唉 ， 踩 雷 了 ， 浪費 時間 ， 不 推 唉 ， 踩 雷 了 ， 浪費 時間 ， 不 推</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review_2</td>\n",
       "      <td>復仇者聯盟：終局之戰</td>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>動作_冒險</td>\n",
       "      <td>negative</td>\n",
       "      <td>片長三個小時，只有最後半小時能看，前面真的鋪陳太久，我旁邊的都看到打呼</td>\n",
       "      <td>片長 三個 小時 ， 只有 最後 半 小時 能 看 ， 前面 真的 鋪陳 太久 ， 我 旁邊...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review_3</td>\n",
       "      <td>復仇者聯盟：終局之戰</td>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>動作_冒險</td>\n",
       "      <td>negative</td>\n",
       "      <td>史上之最，劇情拖太長，邊看邊想睡覺......  1.浩克竟然學會跟旁人一起合照。 2.索爾...</td>\n",
       "      <td>史上 之 最 ， 劇情 拖 太長 ， 邊看邊 想 睡覺 . . . . . . 1. 浩克 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review_4</td>\n",
       "      <td>復仇者聯盟：終局之戰</td>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>動作_冒險</td>\n",
       "      <td>negative</td>\n",
       "      <td>難看死ㄌ 難看死了 難看死ㄌ 看到睡著 拖戲拖很長 爛到爆</td>\n",
       "      <td>難看 死 ㄌ 難看 死 了 難看 死 ㄌ 看到 睡著 拖戲 拖 很長 爛 到 爆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review_5</td>\n",
       "      <td>復仇者聯盟：終局之戰</td>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>動作_冒險</td>\n",
       "      <td>negative</td>\n",
       "      <td>連續三度睡著，真的演的太好睡了</td>\n",
       "      <td>連續 三度 睡著 ， 真的 演 的 太 好 睡 了</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewID              title_CH  \\\n",
       "0  Review_1  紫羅蘭永恆花園外傳－永遠與自動手記人偶－   \n",
       "1  Review_2            復仇者聯盟：終局之戰   \n",
       "2  Review_3            復仇者聯盟：終局之戰   \n",
       "3  Review_4            復仇者聯盟：終局之戰   \n",
       "4  Review_5            復仇者聯盟：終局之戰   \n",
       "\n",
       "                                            title_EN  genre    rating  \\\n",
       "0  Violet Evergarden - Eternity and the Auto Memo...     動畫  negative   \n",
       "1                                  Avengers: Endgame  動作_冒險  negative   \n",
       "2                                  Avengers: Endgame  動作_冒險  negative   \n",
       "3                                  Avengers: Endgame  動作_冒險  negative   \n",
       "4                                  Avengers: Endgame  動作_冒險  negative   \n",
       "\n",
       "                                             reviews  \\\n",
       "0                        唉，踩雷了，浪費時間，不推 唉，踩雷了，浪費時間，不推   \n",
       "1                片長三個小時，只有最後半小時能看，前面真的鋪陳太久，我旁邊的都看到打呼   \n",
       "2  史上之最，劇情拖太長，邊看邊想睡覺......  1.浩克竟然學會跟旁人一起合照。 2.索爾...   \n",
       "3                      難看死ㄌ 難看死了 難看死ㄌ 看到睡著 拖戲拖很長 爛到爆   \n",
       "4                                    連續三度睡著，真的演的太好睡了   \n",
       "\n",
       "                               reviews_sentiword_seg  \n",
       "0    唉 ， 踩 雷 了 ， 浪費 時間 ， 不 推 唉 ， 踩 雷 了 ， 浪費 時間 ， 不 推  \n",
       "1  片長 三個 小時 ， 只有 最後 半 小時 能 看 ， 前面 真的 鋪陳 太久 ， 我 旁邊...  \n",
       "2  史上 之 最 ， 劇情 拖 太長 ， 邊看邊 想 睡覺 . . . . . . 1. 浩克 ...  \n",
       "3           難看 死 ㄌ 難看 死 了 難看 死 ㄌ 看到 睡著 拖戲 拖 很長 爛 到 爆  \n",
       "4                          連續 三度 睡著 ， 真的 演 的 太 好 睡 了  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file='../../../RepositoryData/data/marc_movie_review_metadata.csv'\n",
    "csv_data=pd.read_csv(csv_file,encoding='utf-8')\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has 3200 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "print('File has {} rows and {} columns'.format(csv_data.shape[0],csv_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_data = csv_data.loc[:, ~csv_data.columns.str.contains('Unnamed: 2', case=False)] \n",
    "# csv_data = csv_data.loc[:, ~csv_data.columns.str.contains('Unnamed: 3', case=False)] \n",
    "# csv_data = csv_data.loc[:, ~csv_data.columns.str.contains('Unnamed: 4', case=False)] \n",
    "# csv_data.head()\n",
    "# csv_data=csv_data.dropna() \n",
    "# csv_data=csv_data.reset_index(drop=True)  # Reset index after dropping the columns/rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.rename(columns={'rating':'label','reviews':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = sklearn.utils.shuffle(csv_data)                                                         # Shuffle the dataset\n",
    "#print('Available labels: ',data.label.unique())                              # Print all the unique labels in the dataset\n",
    "# csv_data['text']=csv_data['text'].map(preprocess_sentence)                           # Clean the text column using preprocess_sentence function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has 3200 rows and 7 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewID</th>\n",
       "      <th>title_CH</th>\n",
       "      <th>title_EN</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>reviews_sentiword_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>Review_2426</td>\n",
       "      <td>葉問4：完結篇</td>\n",
       "      <td>IP MAN 4</td>\n",
       "      <td>動作_劇情</td>\n",
       "      <td>positive</td>\n",
       "      <td>後面直接他媽看哭\\r\\n能用功夫片把我看哭的，大概也只有葉師傅了！</td>\n",
       "      <td>後面 直接 他 媽 看 哭 \\r \\n 能 用 功夫 片 把 我 看 哭 的 ， 大概 也 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>Review_2313</td>\n",
       "      <td>返校</td>\n",
       "      <td>Detention</td>\n",
       "      <td>懸疑/驚悚</td>\n",
       "      <td>positive</td>\n",
       "      <td>看到韓粉洗負評，就知道這一定是好片</td>\n",
       "      <td>看到 韓粉 洗 負評 ， 就 知道 這 一定是 好片</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Review_406</td>\n",
       "      <td>古曼童</td>\n",
       "      <td>Kumanthong</td>\n",
       "      <td>恐怖_懸疑/驚悚</td>\n",
       "      <td>negative</td>\n",
       "      <td>這是在描述神棍的片子\\r\\n整場滿頭問號\\r\\n想學泰國邪降但只有20%\\r\\n真的不用特地...</td>\n",
       "      <td>這是 在 描述 神棍 的 片子 \\r \\n 整場 滿頭 問號 \\r \\n 想學 泰國 邪降 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>Review_2749</td>\n",
       "      <td>返校</td>\n",
       "      <td>Detention</td>\n",
       "      <td>懸疑/驚悚</td>\n",
       "      <td>positive</td>\n",
       "      <td>挖操，電影還沒上映，一堆時空旅人來給一星影評\\r\\n\\r\\n笑死\\r\\n\\r\\n五毛網軍們是...</td>\n",
       "      <td>挖操 ， 電影 還沒 上映 ， 一堆 時 空 旅人 來給 一星 影評 \\r \\n \\r \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Review_1646</td>\n",
       "      <td>花椒之味</td>\n",
       "      <td>Fagaro</td>\n",
       "      <td>劇情</td>\n",
       "      <td>positive</td>\n",
       "      <td>好看啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊</td>\n",
       "      <td>好看 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviewID title_CH    title_EN     genre     label  \\\n",
       "2425  Review_2426  葉問4：完結篇    IP MAN 4     動作_劇情  positive   \n",
       "2312  Review_2313       返校   Detention     懸疑/驚悚  positive   \n",
       "405    Review_406      古曼童  Kumanthong  恐怖_懸疑/驚悚  negative   \n",
       "2748  Review_2749       返校   Detention     懸疑/驚悚  positive   \n",
       "1645  Review_1646     花椒之味      Fagaro        劇情  positive   \n",
       "\n",
       "                                                   text  \\\n",
       "2425                  後面直接他媽看哭\\r\\n能用功夫片把我看哭的，大概也只有葉師傅了！   \n",
       "2312                                  看到韓粉洗負評，就知道這一定是好片   \n",
       "405   這是在描述神棍的片子\\r\\n整場滿頭問號\\r\\n想學泰國邪降但只有20%\\r\\n真的不用特地...   \n",
       "2748  挖操，電影還沒上映，一堆時空旅人來給一星影評\\r\\n\\r\\n笑死\\r\\n\\r\\n五毛網軍們是...   \n",
       "1645                          好看啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊   \n",
       "\n",
       "                                  reviews_sentiword_seg  \n",
       "2425  後面 直接 他 媽 看 哭 \\r \\n 能 用 功夫 片 把 我 看 哭 的 ， 大概 也 ...  \n",
       "2312                         看到 韓粉 洗 負評 ， 就 知道 這 一定是 好片  \n",
       "405   這是 在 描述 神棍 的 片子 \\r \\n 整場 滿頭 問號 \\r \\n 想學 泰國 邪降 ...  \n",
       "2748  挖操 ， 電影 還沒 上映 ， 一堆 時 空 旅人 來給 一星 影評 \\r \\n \\r \\n...  \n",
       "1645   好看 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('File has {} rows and {} columns'.format(csv_data.shape[0],csv_data.shape[1]))\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can find more pre-trained models supported by HuggingFace [here](https://huggingface.co/models?filter=zh).\n",
    "- CKIP has also released their BERT models. Please see [here](https://huggingface.co/ckiplab). - It seems that CKIP only releases the `pytorch` version of the pre-trained models. They are not using Tensorflow unfortunately. But the general Chinese models come with both versions `bert-base-chinese`. So we will use this general one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "In `transformers`, there are several predefined tensorflow models that use BERT for classification. Please see Hugginface transformers's [BERT](https://huggingface.co/transformers/model_doc/bert.html) documentation.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee429340ab3b41f1b23a8a351ae5b646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=109540.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(csv_data.label.unique())\n",
    "\n",
    "\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710f3c70811843ad8451dd0e7297e2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=624.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c20e79e933a46b9b7ad8675b3793aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=478309336.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['天', '阿', '，', '這', '電', '影', '實', '在', '是', '.', '.', '.', '無', '言', '啊', '！']\n"
     ]
    }
   ],
   "source": [
    "sent= '天阿，這電影實在是...無言啊！'\n",
    "tokens=bert_tokenizer.tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of `TFBertForSequenceClassification` model:\n",
    "- `input_ids`: The input ids are often the only required parameters to be passed to the model as input. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model. This can be obtained by the BERT Tokenizer.\n",
    "input_ids (Numpy array or tf.Tensor of shape (batch_size, sequence_length))\n",
    "Indices of input sequence tokens in the vocabulary.\n",
    "- `batch_size` : Number of examples or sentences batch\n",
    "sequence_length : A number of tokens in a sentence.\n",
    "2. attention_mask (Numpy array or tf.Tensor of shape (batch_size, sequence_length)) –\n",
    "Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]: 1 for tokens that are not masked, 0 for tokens that are marked (0 if the token is added by padding).\n",
    "This argument indicates to the model which tokens should be attended to, and which should not.\n",
    "If we have 2 sentences and the sequence length of one sentence is 8 and another one is 10, then we need to make them of equal length and for that, padding is required. To distinguish between the padded and nonpadded input attention mask is used.\n",
    "3. labels (tf.Tensor of shape (batch_size,), optional) – Labels for computing the sequence classification/regression loss.\n",
    "Indices should be in [0, ..., num_classes- 1]. If num_classes == 1 a regression loss is computed (Mean-Square loss), If num_classes > 1 a classification loss is computed (Cross-Entropy).\n",
    "These tokens can then be converted into IDs which are understandable by the model. This can be done by directly feeding the sentence to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_sequence= bert_tokenizer.encode_plus(sent,add_special_tokens = True, max_length =30,padding = True,\n",
    "return_attention_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1921, 7350, 8024, 6857, 7442, 2512, 2179, 1762, 3221, 119, 119, 119, 4192, 6241, 1557, 8013, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 天 阿 ， 這 電 影 實 在 是... 無 言 啊 ！ [SEP]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(tokenized_sequence['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Text to BERT Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewID</th>\n",
       "      <th>title_CH</th>\n",
       "      <th>title_EN</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>reviews_sentiword_seg</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>Review_2426</td>\n",
       "      <td>葉問4：完結篇</td>\n",
       "      <td>IP MAN 4</td>\n",
       "      <td>動作_劇情</td>\n",
       "      <td>positive</td>\n",
       "      <td>後面直接他媽看哭\\r\\n能用功夫片把我看哭的，大概也只有葉師傅了！</td>\n",
       "      <td>後面 直接 他 媽 看 哭 \\r \\n 能 用 功夫 片 把 我 看 哭 的 ， 大概 也 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>Review_2313</td>\n",
       "      <td>返校</td>\n",
       "      <td>Detention</td>\n",
       "      <td>懸疑/驚悚</td>\n",
       "      <td>positive</td>\n",
       "      <td>看到韓粉洗負評，就知道這一定是好片</td>\n",
       "      <td>看到 韓粉 洗 負評 ， 就 知道 這 一定是 好片</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Review_406</td>\n",
       "      <td>古曼童</td>\n",
       "      <td>Kumanthong</td>\n",
       "      <td>恐怖_懸疑/驚悚</td>\n",
       "      <td>negative</td>\n",
       "      <td>這是在描述神棍的片子\\r\\n整場滿頭問號\\r\\n想學泰國邪降但只有20%\\r\\n真的不用特地...</td>\n",
       "      <td>這是 在 描述 神棍 的 片子 \\r \\n 整場 滿頭 問號 \\r \\n 想學 泰國 邪降 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>Review_2749</td>\n",
       "      <td>返校</td>\n",
       "      <td>Detention</td>\n",
       "      <td>懸疑/驚悚</td>\n",
       "      <td>positive</td>\n",
       "      <td>挖操，電影還沒上映，一堆時空旅人來給一星影評\\r\\n\\r\\n笑死\\r\\n\\r\\n五毛網軍們是...</td>\n",
       "      <td>挖操 ， 電影 還沒 上映 ， 一堆 時 空 旅人 來給 一星 影評 \\r \\n \\r \\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Review_1646</td>\n",
       "      <td>花椒之味</td>\n",
       "      <td>Fagaro</td>\n",
       "      <td>劇情</td>\n",
       "      <td>positive</td>\n",
       "      <td>好看啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊</td>\n",
       "      <td>好看 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviewID title_CH    title_EN     genre     label  \\\n",
       "2425  Review_2426  葉問4：完結篇    IP MAN 4     動作_劇情  positive   \n",
       "2312  Review_2313       返校   Detention     懸疑/驚悚  positive   \n",
       "405    Review_406      古曼童  Kumanthong  恐怖_懸疑/驚悚  negative   \n",
       "2748  Review_2749       返校   Detention     懸疑/驚悚  positive   \n",
       "1645  Review_1646     花椒之味      Fagaro        劇情  positive   \n",
       "\n",
       "                                                   text  \\\n",
       "2425                  後面直接他媽看哭\\r\\n能用功夫片把我看哭的，大概也只有葉師傅了！   \n",
       "2312                                  看到韓粉洗負評，就知道這一定是好片   \n",
       "405   這是在描述神棍的片子\\r\\n整場滿頭問號\\r\\n想學泰國邪降但只有20%\\r\\n真的不用特地...   \n",
       "2748  挖操，電影還沒上映，一堆時空旅人來給一星影評\\r\\n\\r\\n笑死\\r\\n\\r\\n五毛網軍們是...   \n",
       "1645                          好看啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊   \n",
       "\n",
       "                                  reviews_sentiword_seg  label_num  \n",
       "2425  後面 直接 他 媽 看 哭 \\r \\n 能 用 功夫 片 把 我 看 哭 的 ， 大概 也 ...          1  \n",
       "2312                         看到 韓粉 洗 負評 ， 就 知道 這 一定是 好片          1  \n",
       "405   這是 在 描述 神棍 的 片子 \\r \\n 整場 滿頭 問號 \\r \\n 想學 泰國 邪降 ...          0  \n",
       "2748  挖操 ， 電影 還沒 上映 ， 一堆 時 空 旅人 來給 一星 影評 \\r \\n \\r \\n...          1  \n",
       "1645   好看 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊 啊          1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data['label_num'] = csv_data['label'].map({'negative':0,'positive':1})\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 3200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=csv_data['text']\n",
    "labels=csv_data['label_num']\n",
    "len(sentences),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/Alvin/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in sentences:\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True, max_length =32,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "## alvin's note:\n",
    "## according to the warning, we should use `padding=True` and `max_length = 32`\n",
    "## It didn't work. the tokenizer won't pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids=np.asarray(input_ids).astype('int32')\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 3200, 3200)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids),len(attention_masks),len(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Tokenizer returns a dictionary from which we can get the input ds and the attention masks.\n",
    "Convert all the encoding to NumPy arrays.\n",
    "Arguments of BERT Tokenizer:\n",
    "text (str, List[str], List[List[str]]) – The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set is_split_into_words=True (to lift the ambiguity with a batch of sequences).\n",
    "2. add_special_tokens (bool, optional, defaults to True) – Whether or not to encode the sequences with the special tokens relative to their model.\n",
    "3. max_length (int, optional) — Controls the maximum length to use by one of the truncation/padding parameters. (max_length≤512)\n",
    "4. pad_to_max_length (bool, optional, defaults to True) – Whether or not to pad the sequences to the maximum length.\n",
    "5. return_attention_mask (bool, optional) –"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inp shape (2560, 32) Val input shape (640, 32)\n",
      "Train label shape (2560,) Val label shape (640,)\n",
      "Train attention mask shape (2560, 32) Val attention mask shape (640, 32)\n"
     ]
    }
   ],
   "source": [
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=sklearn.model_selection.train_test_split(input_ids,labels,attention_masks,test_size=0.2)\n",
    "\n",
    "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "path = \"./sentiment-analysis-using-bert-keras-chinese/models/\"\n",
    "os.mkdir(path)\n",
    "\n",
    "## Callbacks\n",
    "## The model will automatically create the `log_dir` but not `model_save_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  102267648 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 102,269,186\n",
      "Trainable params: 102,269,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "log_dir='./sentiment-analysis-using-bert-keras-chinese/tensorboard_data/tb_bert'\n",
    "model_save_path='./sentiment-analysis-using-bert-keras-chinese/models/bert_model.h5'\n",
    "\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "\n",
    "print('\\nBert Model',bert_model.summary())\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 580s 7s/step - loss: 0.5266 - accuracy: 0.7042 - val_loss: 0.2681 - val_accuracy: 0.8781\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './sentiment-analysis-using-bert-keras-chinese/models/bert_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5a7a7deb3b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                        callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                         'directory: {}'.format(filepath))\n\u001b[1;32m   1417\u001b[0m         \u001b[0;31m# Re-throw the error for any other causes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                 self.model.save_weights(\n\u001b[0;32m-> 1394\u001b[0;31m                     filepath, overwrite=True, options=self._options)\n\u001b[0m\u001b[1;32m   1395\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python-notes/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './sentiment-analysis-using-bert-keras-chinese/models/bert_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "history=bert_model.fit([train_inp,train_mask],\n",
    "                       train_label,\n",
    "                       batch_size=32,\n",
    "                       epochs=1,\n",
    "                       validation_data=([val_inp,val_mask],val_label),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Using Tensorbaord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation: Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier', 'dropout_75']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_save_path='./sentiment-analysis-using-bert-keras/models/bert_model.h5'\n",
    "\n",
    "\n",
    "trained_model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=2)\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights(model_save_path)\n",
    "\n",
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.8888888888888888\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.94      0.80      0.87       312\n",
      "    negative       0.83      0.95      0.89       328\n",
      "\n",
      "    accuracy                           0.88       640\n",
      "   macro avg       0.89      0.88      0.88       640\n",
      "weighted avg       0.89      0.88      0.88       640\n",
      "\n",
      "Training and saving built model.....\n"
     ]
    }
   ],
   "source": [
    "pred_labels = preds[0].argmax(axis=1)\n",
    "f1 = sklearn.metrics.f1_score(val_label,pred_labels)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "\n",
    "target_names=csv_data.label.unique()\n",
    "print(sklearn.metrics.classification_report(val_label,pred_labels,target_names=target_names))\n",
    "\n",
    "print('Training and saving built model.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.878125011920929]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate([val_inp,val_mask],batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [BERT Text Classification Using Keras](https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d)\n",
    "- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
    "- [Text Extraction with BERT](https://keras.io/examples/nlp/text_extraction_with_bert/#text-extraction-with-bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
