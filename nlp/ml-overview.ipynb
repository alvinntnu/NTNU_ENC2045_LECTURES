{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{contents}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simply put, ML is the science of programming computers so that they can learn from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "```{epigraph}\n",
    "\n",
    "[Machine learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "-- Arthur Samuel, 1959\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{epigraph}\n",
    "\n",
    "A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n",
    "\n",
    "-- Tom Mitchell, 1997\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why use machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Limitations of **rules**/**heuristics**-based systems:\n",
    "    - Rules are hard to be **exhaustively** listed.\n",
    "    - Tasks are simply too complex for rule generalization.\n",
    "    - The rule-based **deductive** approach is not helpful in discovering novel things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can categorize ML into four types according to the amount and type of **supervision** it gets during training:\n",
    "\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Semisupervised Learning\n",
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "- The data we feed to the ML algorithm includes the desired solutions, i.e., **labels**.\n",
    "    - **Classification** task (e.g., spam filter): the target is a categorical label.\n",
    "    - **Regression** task (e.g., car price prediction): the target is a numeric value.\n",
    "- Classification and regression are two sides of the coin.\n",
    "    - We can sometimes utilize regression algorithms for classification.\n",
    "    - A classic example is Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Examples of Supervised Learning\n",
    "    - K-Nearest Neighbors\n",
    "    - Linear Regression\n",
    "    - Naive Bayes\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machines (SVMs)\n",
    "    - Decision Trees and Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "- The data is **unlabeled**. We are more interested in the underlying grouping patterns of the data.\n",
    "    - Clustering\n",
    "    - Anomaly/Novelty detection\n",
    "    - Dimensionality reduction\n",
    "    - Association learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Examples of Unsupervised Learning\n",
    "    - Clustering\n",
    "        - K-means\n",
    "        - Hierarchical Clustering\n",
    "    - Dimensionality reduction\n",
    "        - Principal Component Analysis\n",
    "        - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Semisupervised Learning\n",
    "\n",
    "- It's a combination of supervised and unsupervised learning.\n",
    "- For example, we start with unlabeled training data and use unsupervised learning to find groups. Users then label these groups with meaningful labels and then transform the task into a supervised learning.\n",
    "- A classific example is the photo-hosting service (e.g., face tagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "- This type of learning is often used in robots.\n",
    "- The learning system, called an Agent, will learn based on its observation of the environment. During the learning process, the agent will select and perform actions, and get rewards or penalties in return. Through this trial-and-error process, it will figure the most optimal strategy (i.e., policy) for the target task.\n",
    "- A classific example is DeepMind's AlphaGo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workflow for Building a Machine Learning Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In NLP, most often we deal with **classification** problems. In particular, we  deal with **supervised classifcation** learning problems.\n",
    "\n",
    "- Given a dataset of **texts** and their corresponding **labels**, the objectives of the classifier are:\n",
    "    - How can we identify particular **features** of language data that are **salient** for texts of each **label**?\n",
    "    - How can we construct **models** of language that can be used to perform the classification **automatically**?\n",
    "    - What can we learn about language from these classifiers?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A common workflow for classifier building is shown as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/nltk-fig-6-1-classification-workflow.png)\n",
    "<small>(Source: from NLTK Book Ch 6, Figure 6-1)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Most classification methods require that features be encoded using simple value types, such as booleans, numbers, and strings.\n",
    "- But note that just because a feature has a simple type, this does not necessarily mean that the feature's value is simple to express or compute. \n",
    "- Indeed, it is even possible to use very complex and informative values, such as the output of a second supervised classifier, as features. (i.e., **boosting** techniques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is feature engineering?\n",
    "\n",
    "- It refers to a process to feed the extracted and preprocessed texts into a machine-learning algorithm.\n",
    "- It aims at capturing the **characteristics** of the text into a **numeric vector** that can be understood by the ML algorithms. (Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., the **vectorized representation** of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for Classical ML\n",
    "\n",
    "- Word-based frequency lists\n",
    "- Bag-of-words representations (TF-IDF)\n",
    "- Domain-specific word frequency lists (e.g., Sentiment Dictionary)\n",
    "- Hand-crafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "- The DL model is capable of learning features from the texts (e.g., embeddings)\n",
    "- The price is that the model is often less interpretable.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strengths and Weakness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/feature-engineer-strengths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/feature-engineer-weakness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Insufficient **quantity** of training data\n",
    "- Non-**representative** training data\n",
    "- Poor **quality** data\n",
    "- Irrelevant **features**\n",
    "- **Overfitting** the training data\n",
    "- **Underfitting** the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing and Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Hyperparameter tuning and model selection\n",
    "    - Many ML algorithms are available.\n",
    "    - Algorithms often come with many parameter settings.\n",
    "    - It is usually not clear which algorithms would perform better.\n",
    "    - **Cross-validation** comes to rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Cross-Validation\n",
    "\n",
    "![](../images/ml-kfold.png)\n",
    "(Source: https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Cross Validation**\n",
    "    - Before testing our model on the testing dataset, we can utilize **$k$-fold cross-validation** to first evaluate our trained model within the **training dataset** and at the same time fine-tune the **hyperparameters**.\n",
    "    - Specifically, we often split the **training set** into **$k$** distinct subsets called **folds**, and trains the model on the **${k-1}$** folds and test on the remaining 1 folds. A **$k$-fold** split allows us to do this training-testing for $k$ times.\n",
    "    - Based on the distribution of the evaluation scores in all $k$ folds of datasets, we get to see the **average** performance of our model.\n",
    "        - Which ML algorithm performs the best?\n",
    "        - Which sets of hyperparameters yield the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    ":class: dropdown\n",
    "\n",
    "Hyperparameters are parameters that are set before the learning process begins and determine the behavior of a machine learning algorithm. They are not learned from the data but are specified by the user. Hyperparameters control aspects of the learning process such as the complexity of the model, the learning rate, the regularization strength, and the number of iterations.\n",
    "\n",
    "Here are some common examples of hyperparameters in machine learning algorithms:\n",
    "\n",
    "1. **Regularization parameter (C)** in Support Vector Machines (SVM): This hyperparameter controls the trade-off between maximizing the margin and minimizing the classification error. A larger value of C allows for more complex decision boundaries, potentially leading to overfitting.\n",
    "\n",
    "2. **Learning rate (alpha)** in Gradient Descent: This hyperparameter determines the step size taken during each iteration of gradient descent. A smaller learning rate may lead to slower convergence but can help prevent overshooting the minimum, while a larger learning rate may speed up convergence but risk overshooting.\n",
    "\n",
    "3. **Number of hidden layers and units** in Neural Networks: These hyperparameters define the architecture of the neural network, including the number of hidden layers and the number of neurons in each layer. The choice of these hyperparameters can significantly impact the model's capacity to learn complex patterns.\n",
    "\n",
    "4. **Number of trees and maximum depth** in Random Forest: The number of trees and the maximum depth of each tree are hyperparameters that control the complexity of the ensemble model. Increasing the number of trees and maximum depth can lead to higher model complexity and potentially better performance, but may also increase computational cost and risk overfitting.\n",
    "\n",
    "5. **Number of neighbors (k)** in k-Nearest Neighbors (kNN): This hyperparameter specifies the number of nearest neighbors to consider when making predictions. A larger value of k leads to a smoother decision boundary but may reduce the model's ability to capture local patterns.\n",
    "\n",
    "Examples of hyperparameters in common machine learning algorithms include:\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - **Regularization parameter (C)**: Controls the strength of regularization, with larger values indicating weaker regularization.\n",
    "   - **Penalty**: Specifies the type of regularization (e.g., L1 or L2 regularization).\n",
    "   - **Solver**: Determines the optimization algorithm used to fit the logistic regression model (e.g., 'liblinear', 'lbfgs', 'newton-cg', 'sag', or 'saga').\n",
    "\n",
    "2. **Naive Bayes**:\n",
    "   - **Smoothing parameter (alpha)**: Controls the degree of smoothing applied to the estimated probabilities of features.\n",
    "   - **Distribution assumption**: Specifies the distribution assumption for the features (e.g., Gaussian Naive Bayes assumes a Gaussian distribution).\n",
    "\n",
    "3. **Clustering (e.g., K-Means)**:\n",
    "   - **Number of clusters (k)**: Specifies the number of clusters to partition the data into.\n",
    "   - **Initialization method**: Determines how initial cluster centroids are selected (e.g., 'k-means++', 'random').\n",
    "   - **Maximum number of iterations**: Sets the maximum number of iterations for the algorithm to converge.\n",
    "\n",
    "4. **Topic Modeling (e.g., Latent Dirichlet Allocation - LDA)**:\n",
    "   - **Number of topics**: Specifies the number of topics to be identified in the corpus.\n",
    "   - **Alpha parameter**: Controls the distribution of topics per document.\n",
    "   - **Beta parameter**: Controls the distribution of words per topic.\n",
    "   - **Number of iterations**: Sets the number of iterations for the model to converge.\n",
    "\n",
    "5. **Support Vector Machines (SVM)**:\n",
    "   - **Kernel type**: Specifies the type of kernel function to be used (e.g., linear, polynomial, radial basis function).\n",
    "   - **Kernel coefficient (gamma)**: Controls the influence of each training example on the decision boundary.\n",
    "   - **Degree**: Specifies the degree of the polynomial kernel function (for polynomial kernels).\n",
    "   - **Class weights**: Assigns weights to different classes to address class imbalance.\n",
    "\n",
    "Tuning these hyperparameters appropriately can significantly impact the performance of the models on unseen data.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Project Checklist\n",
    "\n",
    "A machine learning project often involves a few important steps: {cite}`geron2019hands`\n",
    "\n",
    "- Frame the problem and look at the big picture.\n",
    "- Get the data.\n",
    "- Explore the data to gain insights.\n",
    "- Prepare the data tp better explore the underlying data patterns to machine learning algorithms.\n",
    "- Explore many different models and shortlist the best ones.\n",
    "- Fine-tune the models amd combine them into a great solution.\n",
    "- Present your solution.\n",
    "- (Interpret the insights from the best-performing model).\n",
    "- Launch, monitor, and maintain your system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- GÃ©ron (2023), Ch 1-2"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
