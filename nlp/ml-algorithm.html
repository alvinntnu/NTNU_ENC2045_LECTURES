

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Classification Models &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/ml-algorithm';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sentiment Analysis Using Bag-of-Words" href="ml-sklearn-classification.html" />
    <link rel="prev" title="Machine Learning: A Simple Example" href="ml-simple-case.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-neural-network-from-scratch.html">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">Sequence Model with Attention for Addition Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6-machine-learning.html">5. Assignment V: Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7-topic-modeling.html">6. Assignment VI: Topic Modeling</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/ml-algorithm.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/ml-algorithm.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classification Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-naive-bayes">Intuition of Naive Bayes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine-svm">Support Vector Machine (SVM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-discriminative-models-skipped">Other Discriminative Models (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-entropy-model-maxent">Maximum Entropy Model (Maxent)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-classification-models-skipped">Other Classification Models (skipped)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#emsemble-methods">Emsemble Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="classification-models">
<h1>Classification Models<a class="headerlink" href="#classification-models" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>Before the era of deep learning, probability-based classifiers are commonly used in many ML NLP tasks.</p></li>
<li><p>There are two types of probability-based models: <strong>generative</strong> and <strong>disriminative</strong> models.</p></li>
<li><p>Let’s assume that we have our data as <em><strong>d</strong></em>, and their class labels as <em><strong>c</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Generative</strong>:</p>
<ul>
<li><p>The goal of training is NOT to find <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Rather, based on the <strong>Bayes Theorem</strong>, we can estimate the joint probability of <span class="math notranslate nohighlight">\(P(c,d)\)</span>, which, according to Bayes Theorem, can be reformulated as <span class="math notranslate nohighlight">\(P(c,d)=P(d)\times P(c|d) =P(c) \times P(d|c)\)</span></p></li>
<li><p>That is, the training in generative models is based on the <strong>joint</strong> probability of the data and the class, i.e., <span class="math notranslate nohighlight">\(P(c,d)\)</span></p></li>
<li><p>Examples: N-gram Language Models, Naive Bayes, Hidden Markov Model, Probabilistic Context-Free Grammars.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Discriminative</strong>:</p>
<ul>
<li><p>The goal of training is to directly find <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Training is based on the <strong>conditional</strong> Probability of the class given the data, i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Examples: Logistic regression, Maximum Entropy Models, Conditional Random Field, Support Vector Machine, Perceptron.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Strengths of <strong>discriminative</strong> models:</p>
<ul>
<li><p>They give high accuracy performance.</p></li>
<li><p>They make it easier to include linguistically relevant features.</p></li>
</ul>
</li>
<li><p>Other classification models:</p>
<ul>
<li><p>Tree-based methods (Decision tree, Random Forest)</p></li>
<li><p>Neural Network</p></li>
</ul>
</li>
</ul>
<section id="naive-bayes">
<h2>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/naive-bayes.png" /></p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In Bayes, the <span class="math notranslate nohighlight">\(P(Y)\)</span> is referred to as the <strong>prior probability</strong> of Y and the <span class="math notranslate nohighlight">\(P(Y|X)\)</span> is referred to as the <strong>posterior probability</strong> of Y.</p>
</div>
</aside>
<ul class="simple">
<li><p>Naive Bayes features the Bayes Theorem:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes1">
<span class="eqno">(1)<a class="headerlink" href="#equation-naive-bayes1" title="Permalink to this equation">#</a></span>\[
P(Y|X) = P(Y) \frac{P(X|Y)}{P(X)} 
\]</div>
<section id="intuition-of-naive-bayes">
<h3>Intuition of Naive Bayes<a class="headerlink" href="#intuition-of-naive-bayes" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/ml-naive-bayes.gif" /></p>
<ul class="simple">
<li><p>In Naive Bayes, given a document <span class="math notranslate nohighlight">\(d\)</span> and a class <span class="math notranslate nohighlight">\(c\)</span>, the goal is to find the <strong>maximum joint probability</strong> <span class="math notranslate nohighlight">\(P(c,d)\)</span>. And according bayes rule, the goal (of finding the maximum joint probability) can be reformulated as finding the maximum of the <strong>posterior probability</strong> of the class, <span class="math notranslate nohighlight">\(P(c|d)\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-navie-bayes2">
<span class="eqno">(2)<a class="headerlink" href="#equation-navie-bayes2" title="Permalink to this equation">#</a></span>\[\begin{split}
P(c,d) = P(d|c)\times P(c) = P(c|d) \times P(d)\\
P(c|d) = \frac{P(d|c)\times P(c)}{P(d)}
\end{split}\]</div>
<ul class="simple">
<li><p>Because the <span class="math notranslate nohighlight">\(P(d)\)</span> is a constant for all classes estimation, we can drop the denominator. And now the goal is to find the class <span class="math notranslate nohighlight">\(c\)</span> that maximizes the <strong>posterior probability</strong> of the class, i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>. (MAP = Maximum A Posterior Estimation)</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes3">
<span class="eqno">(3)<a class="headerlink" href="#equation-naive-bayes3" title="Permalink to this equation">#</a></span>\[\begin{split}
C_{MAP} = \underset{c \in C}{\arg\max}P(c|d)\\
C_{MAP}= \underset{c \in C}{\arg\max}\frac{P(d|c)\times P(c)}{P(d)} \\
= \underset{c \in C}{\arg\max}P(d|c)\times P(c) \\
\end{split}\]</div>
<ul class="simple">
<li><p>In Naive Bayes, the probabilities <span class="math notranslate nohighlight">\(P(C=c_i)\)</span> and <span class="math notranslate nohighlight">\(P(X_i|C=c_i)\)</span> are <strong>parameters</strong>.</p></li>
<li><p>The standard, maximum likelihood, approach is to calculate these parameters (probabilities) using MLE estimators based on the training data.</p></li>
</ul>
<ul class="simple">
<li><p>For example, for class prior probabilities, <span class="math notranslate nohighlight">\(P(C=c_i)\)</span>, we count the cases where <span class="math notranslate nohighlight">\(C=c_i\)</span> and divide by the sample size.</p></li>
<li><p>Similarly, each feature <span class="math notranslate nohighlight">\(x_i\)</span> of the document is in the form of a probability value of an observed event given the class, <span class="math notranslate nohighlight">\(P(x_i|C=c_i)\)</span>. For example, the probability of observing the word ‘好看’ (<strong><span class="math notranslate nohighlight">\(x_i\)</span></strong>) when the document class is <strong><span class="math notranslate nohighlight">\(c_i\)</span></strong>. Same for conditional probabilities <span class="math notranslate nohighlight">\(P(好看|C=c_i)\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>A document needs to be vectorized through feature engineering into a numeric representation, i.e., a set of <strong>n</strong> features from the document, <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span></p></li>
<li><p>Given a class <span class="math notranslate nohighlight">\(c\)</span> and a set of <strong><span class="math notranslate nohighlight">\(n\)</span></strong> features from the document, <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span>, we can denote the <strong>posterior probability</strong> of each class <span class="math notranslate nohighlight">\(c\)</span> as follows:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes4">
<span class="eqno">(4)<a class="headerlink" href="#equation-naive-bayes4" title="Permalink to this equation">#</a></span>\[\begin{split}
C_{MAP}= \underset{c \in C}{\arg\max}P(c|d)\\\\
\underset{c \in C}{\arg\max}  P(d|c) \times P(c) \\
= \underset{c \in C}{\arg\max} P(x_1,x_2,...,x_n|c) \times P(c) \\
\end{split}\]</div>
<ul class="simple">
<li><p>Important assumptions in Naive Bayes:</p>
<ul>
<li><p>Independence Assumption: Assume features are independent of each other.</p></li>
<li><p>Conditional Independence: Assume the feature probabilities <span class="math notranslate nohighlight">\(P(x_i|C=c_i)\)</span> are independent given the label <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes5">
<span class="eqno">(5)<a class="headerlink" href="#equation-naive-bayes5" title="Permalink to this equation">#</a></span>\[
P(x_1,x_2,...,x_n|c)=P(x_1|c)\times P(x_2|c)\times P(x_3|c) \times ... \times P(x_n|c)
\]</div>
<ul class="simple">
<li><p>Then we can simply the equation:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-naive-bayes6">
<span class="eqno">(6)<a class="headerlink" href="#equation-naive-bayes6" title="Permalink to this equation">#</a></span>\[\begin{split}
C_{MAP} = \underset{c \in C}{\arg\max} P(x_1,x_2,...,x_n|c) \times P(c) \\
= \underset{c \in C}{\arg\max} P(c) \prod_{x\in X}P(x|c)
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Multinomial Naive Bayes</strong> is an extension of the Naive Bayes for predicting and classifying data points, where the number of distinct labels or outcomes are more than two.</p></li>
</ul>
<ul class="simple">
<li><p>Issues of <strong>Smoothing</strong></p>
<ul>
<li><p>For unseen features in a new document, e.g., <span class="math notranslate nohighlight">\(P(x|c)\)</span>, this would render the entire <strong>posterior probability</strong> of the class to be zero.</p></li>
<li><p>Laplace smoothing (Add-one)</p></li>
<li><p>Lidstone smooting (Self-defined <span class="math notranslate nohighlight">\(\alpha\)</span> &lt; 1)</p></li>
<li><p>cf. <code class="docutils literal notranslate"><span class="pre">sklearn.naive_bayes.GaussianNB(*,</span> <span class="pre">priors=None,</span> <span class="pre">var_smoothing=1e-09)</span></code></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression</strong> is also a form of probabilistic statistical classification model.</p></li>
<li><p>The model is trained by maximizing directly the probability of the <strong>class</strong> (c) given the observed <strong>data</strong> (d), i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>Logistic Regression is similar to Linear Regression, whose predicted values are both numeric (cf. <a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Models</a>).</p></li>
</ul>
<ul class="simple">
<li><p>A document needs to be vectorized through feature engineering into a numeric representation, i.e., a set of <strong><span class="math notranslate nohighlight">\(n\)</span></strong> features characterizing the semantics of the document, <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span></p></li>
<li><p>Then Logistic Regression models the <strong>probability</strong> of the class given these observed values as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + ... + \beta_ix_i
= \sum_{i}\beta_ix_i
\]</div>
<ul>
<li><p>Each feature <span class="math notranslate nohighlight">\(x_i\)</span> of the document is a function that characterizes the relevant linguistic properties of the document. These features can be all <strong>manually</strong> annotated or created <strong>automatically</strong>.</p></li>
<li><p>These features are often in simple forms that are of <strong>binary</strong> values or <strong>numeric</strong> values within a range:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    x_1 &amp;= f_1(x_1) = [Contains(&quot;好看&quot;) ] = \{0, 1\} \\
    x_2 &amp;= f_2(x_2) = [Contains(&quot;絕配&quot;) ] = \{0, 1\} \\
    x_3 &amp;= f_3(x_3) = [FreqOf(&quot;覺得&quot;)] = \{\textit{Any Positive Number}\}\\
    &amp;...\\
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>But so far the model prediction (<span class="math notranslate nohighlight">\(y\)</span>) could be any number (<span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>) based on the current linear model.</p></li>
<li><p>In order to make sure the predicted values of the model are within the range of 0 and 1 (i.e., a probability-like value), the dependent variable <span class="math notranslate nohighlight">\(y\)</span> is transformed with a so-called <strong>link function</strong>.</p></li>
<li><p>That is, <strong>link functions</strong> transform the predicted <span class="math notranslate nohighlight">\(y\)</span> into a range more appropriate for linear modeling.</p></li>
</ul>
<ul class="simple">
<li><p>In particular, in Binary Logistic Regression, the <strong>inverse logit transformation</strong> is used, transforming <span class="math notranslate nohighlight">\(y\)</span> from the range of <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span> to into probability values ranging from 0 to 1.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + ... + \beta_ix_i
= \sum_{i}\beta_ix_i
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}\\
P(c|d) &amp;= \frac{1}{1+e^{-y}}\\
                    &amp;= \frac{1}{1+e^{-(\sum_{i}\beta_ix_i)}}\\
                    &amp;= P(c|x_1,x_2,...,x_i)\\
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>And we can re-formulate the model equation as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}\\
P(c|d) &amp;= P(c|x_1,x_2,...,x_i)\\
&amp;= \frac{1}{1+e^{-(\sum_{i}{\beta_ix_i})}}
\end{align}
\end{split}\]</div>
<ul class="simple">
<li><p>In model prediction, the model will then determine the <strong>class</strong> of the document by choosing the class that maximizes the conditional probability of <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>In training:</p>
<ul>
<li><p>Given a document in a training set, <span class="math notranslate nohighlight">\(d\)</span>, the initial Logistic Regression model will output the predicted conditional probability of <span class="math notranslate nohighlight">\(d\)</span> being the class, <span class="math notranslate nohighlight">\(c\)</span>, i.e., <span class="math notranslate nohighlight">\(P(c|d)\)</span>.</p></li>
<li><p>This is the <strong>likelihood</strong> of the document.</p></li>
<li><p>And the optimal Logistic Regression is the one that <strong>maximizes the likelihoods</strong> of the documents in the entire training set.</p></li>
<li><p>This <strong>maximum likelihood</strong> approach is similar to the least squares method in linear regression.</p></li>
</ul>
</li>
</ul>
<ul>
<li><p>Interpreting the parameters (coefficients <span class="math notranslate nohighlight">\(\beta_i\)</span>)</p>
<ul class="simple">
<li><p>The coefficient refers to the change of the odds of having the target class label in relation to a specific predictor.</p></li>
<li><p>Odds are defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align}\\
    odds &amp;=\frac{\textit{The probability that the event will occur}}{\textit{The probability that the event will NOT occur}} \\&amp;= \frac{P_{\textit{relevant class}}}{1 - P_{\textit{relevant class}}}
    \end{align}
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>A quick example (based on Stefan Gries’ data): (skipped)</p>
<ul>
<li><p>We use the type of the subordinate clauses (<code class="docutils literal notranslate"><span class="pre">SUBORDTYPE</span></code>) to predict the <code class="docutils literal notranslate"><span class="pre">ORDER</span></code> of the main and subordinate clauses.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(X: \{caus, temp\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y: \{mc-sc, sc-mc\}\)</span></p></li>
</ul>
</li>
<li><p>Unit = sentences; Label = main-subordinate clauses orders.</p></li>
<li><p>We run a logistic regression, using SUBORDTYPE as the predictor and ORDER as the response variable.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> rpy2.ipython
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1">## data from Stefan Gries&#39; &quot;Statistics for Linguistics with R&quot;</span>
<span class="n">csv</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;../../../RepositoryData/data/gries_sflwr/_inputfiles/05-3_clauseorders.csv&#39;</span><span class="p">)</span>
<span class="n">csv</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CASE</th>
      <th>ORDER</th>
      <th>SUBORDTYPE</th>
      <th>LEN_MC</th>
      <th>LEN_SC</th>
      <th>LENGTH_DIFF</th>
      <th>CONJ</th>
      <th>MORETHAN2CL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4777</td>
      <td>sc-mc</td>
      <td>temp</td>
      <td>4</td>
      <td>10</td>
      <td>-6</td>
      <td>als/when</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1698</td>
      <td>mc-sc</td>
      <td>temp</td>
      <td>7</td>
      <td>6</td>
      <td>1</td>
      <td>als/when</td>
      <td>no</td>
    </tr>
    <tr>
      <th>2</th>
      <td>953</td>
      <td>sc-mc</td>
      <td>temp</td>
      <td>12</td>
      <td>7</td>
      <td>5</td>
      <td>als/when</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1681</td>
      <td>mc-sc</td>
      <td>temp</td>
      <td>6</td>
      <td>15</td>
      <td>-9</td>
      <td>als/when</td>
      <td>no</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4055</td>
      <td>sc-mc</td>
      <td>temp</td>
      <td>9</td>
      <td>5</td>
      <td>4</td>
      <td>als/when</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>398</th>
      <td>1794</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>13</td>
      <td>23</td>
      <td>-10</td>
      <td>weil/because</td>
      <td>no</td>
    </tr>
    <tr>
      <th>399</th>
      <td>4095</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>10</td>
      <td>7</td>
      <td>3</td>
      <td>weil/because</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>400</th>
      <td>2733</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>8</td>
      <td>5</td>
      <td>3</td>
      <td>weil/because</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>401</th>
      <td>350</td>
      <td>mc-sc</td>
      <td>caus</td>
      <td>8</td>
      <td>11</td>
      <td>-3</td>
      <td>weil/because</td>
      <td>no</td>
    </tr>
    <tr>
      <th>402</th>
      <td>208</td>
      <td>sc-mc</td>
      <td>caus</td>
      <td>9</td>
      <td>6</td>
      <td>3</td>
      <td>weil/because</td>
      <td>yes</td>
    </tr>
  </tbody>
</table>
<p>403 rows × 8 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>To explain the model in a more comprehensive way, I like to switch back to R for the statistical outputs. I think they are more intuitive.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">R</span> -i csv

# library(readr)
# csv= read_delim(&#39;../../../RepositoryData/data/gries_sflwr/_inputfiles/05-3_clauseorders.csv&#39;,&#39;\t&#39;)
print(head(csv))
print(table(csv$SUBORDTYPE, csv$ORDER))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF     CONJ MORETHAN2CL
0 4777 sc-mc       temp      4     10          -6 als/when          no
1 1698 mc-sc       temp      7      6           1 als/when          no
2  953 sc-mc       temp     12      7           5 als/when         yes
3 1681 mc-sc       temp      6     15          -9 als/when          no
4 4055 sc-mc       temp      9      5           4 als/when         yes
5  967 sc-mc       temp      9      5           4 als/when         yes
      
       mc-sc sc-mc
  caus   184    15
  temp    91   113
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">R</span>
lg = glm(factor(csv$ORDER)~factor(csv$SUBORDTYPE), family=&quot;binomial&quot;)
summary(lg) 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = factor(csv$ORDER) ~ factor(csv$SUBORDTYPE), family = &quot;binomial&quot;)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2706  -0.3959  -0.3959   1.0870   2.2739  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -2.5069     0.2685  -9.336   &lt;2e-16 ***
factor(csv$SUBORDTYPE)temp   2.7234     0.3032   8.982   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 503.80  on 402  degrees of freedom
Residual deviance: 386.82  on 401  degrees of freedom
AIC: 390.82

Number of Fisher Scoring iterations: 5
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Based on the model parameters, we get the formula for the probability prediction of our response variable:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 = -2.50 + 2.72x_1
\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To interpret the coefficients we need to know the order of the two class labels in the outcome variable. The most straightforward way to do this is to create a table of the outcome variable.</p>
<p>As shown above, the second level of <code class="docutils literal notranslate"><span class="pre">ORDER</span></code> is <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code>, this tells us that the coefficients in the Logistic Regression are predicting whether or not the clause order is <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code>.</p>
</div>
<ul>
<li><p>Now we can estimate the following probabilities:</p>
<ul class="simple">
<li><p>Probability of <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code> when subordinate is <code class="docutils literal notranslate"><span class="pre">cause</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}\\
    P(type=scmc|x_1=cause) &amp;= \frac{1}{1+e^{-y}}\\
    &amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times x_1})}} \\
    &amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times 0})}}\\
    &amp;= 0.0758
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>Probability of <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code> when subordinate is <code class="docutils literal notranslate"><span class="pre">temp</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}\\
P(type=scmc|x_1=temp) &amp;= \frac{1}{1+e^{-y}}\\
&amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times x_1})}} \\
&amp;=\frac{1}{1+e^{-({-2.50 + 2.72 \times 1})}}\\
&amp;= 0.5547
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Now we can also compute the <strong>odds</strong> of the predicted probability.</p></li>
<li><p>The odds of an event is the ratio of:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\textit{The probability that the event will occur}}{\textit{The probability that the event will NOT occur}}
\]</div>
<ul class="simple">
<li><p>Simply put, odds are just a ratio of two complementary probability values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Odds = \frac{P(c)}{1-P(c)} = \frac{P(type=scmc)}{1-P(type=scmc)}
\]</div>
<ul>
<li><p>Now we can compute the two odds:</p>
<ul class="simple">
<li><p>The odds of the probabilities when the subordinate is <code class="docutils literal notranslate"><span class="pre">caus</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    odds_1 = \frac{0.07}{1-0.07} = 0.08
    \]</div>
<ul class="simple">
<li><p>The odds of the probabilities when the subordinate is <code class="docutils literal notranslate"><span class="pre">temp</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    odds_2 = \frac{0.55}{1-0.55} = 1.25
    \]</div>
</li>
</ul>
<ul>
<li><p>Finally, we can compute the <strong>log odds ratios</strong> (the log of the ratios between two odds):</p>
<div class="math notranslate nohighlight">
\[
    \textit{Odds Ratio} = \frac{odds_2}{odds_1} = \frac{1.25}{0.08} = 15.18
    \]</div>
<div class="math notranslate nohighlight">
\[
    \textit{Log Odds Ratio} = log\left(\frac{odds_2}{odds_1}\right) = log\left(\frac{1.25}{0.08}\right) = 2.72
    \]</div>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1x_1 = -2.50 + 2.72x_1
\]</div>
<ul class="simple">
<li><p>That is the meaning of the coefficient <span class="math notranslate nohighlight">\(\beta_1, 2.72\)</span>:</p>
<ul>
<li><p>The <strong>odds</strong> (of having <code class="docutils literal notranslate"><span class="pre">sc-mc</span></code> order) when the subordinate clause is <code class="docutils literal notranslate"><span class="pre">temp</span></code> are <span class="math notranslate nohighlight">\(e^{2.7234}= 15.23\)</span> times more than the <strong>odds</strong> when the subordinate clause is <code class="docutils literal notranslate"><span class="pre">caus</span></code>.</p></li>
<li><p>Or the <strong>log odds ratio</strong> increases 2.72 times when the subordinate clause is <code class="docutils literal notranslate"><span class="pre">temp</span></code> as compared to when the subordinate is <code class="docutils literal notranslate"><span class="pre">caus</span></code>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">R</span>
y1= 1/(1+exp(-(-2.50 + 2.72*0))) # prob of sc-mc when SUBORDTYPE = 0 (caus)
y2= 1/(1+exp(-(-2.50 + 2.72*1))) # prob of sc-mc when SUBORDTYPE = 1 (temp)
odds1 = y1/(1-y1) # odds when SUBORDTYPE = 0
odds2 = y2/(1-y2) # odds when SUBORDTYPE = 1

cat(&quot;Prob(type=sc-mc|subordtype=cause):&quot;, y1, &quot;\n&quot;)
cat(&quot;The odds of Prob(type=mc-sc|subordtype=cause) vs. Prob(type=sc-mc|subordtype=cause):&quot;, odds1,&quot;\n\n&quot;)

cat(&quot;Prob(type=sc-mc|subordtype=temp):&quot;, y2, &quot;\n&quot;)
cat(&quot;The odds of Prob(type=mc-sc|subordtype=temp) vs. Prob(type=sc-mc|subordtype=temp):&quot;, odds2, &quot;\n\n&quot;)

odds_ratio = odds2/odds1
log_odds_ratio = log(odds_ratio)
cat(&quot;Odds Ratioos of subordtype=temp vs. subordtype=cause:&quot;, odds_ratio, &quot;\n&quot;)
cat(&quot;Log Odds Ratios of subordtype=temp vs. subordtype=cause:&quot;, log_odds_ratio)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prob(type=sc-mc|subordtype=cause): 0.07585818 
The odds of Prob(type=mc-sc|subordtype=cause) vs. Prob(type=sc-mc|subordtype=cause): 0.082085 

Prob(type=sc-mc|subordtype=temp): 0.5547792 
The odds of Prob(type=mc-sc|subordtype=temp) vs. Prob(type=sc-mc|subordtype=temp): 1.246077 

Odds Ratioos of subordtype=temp vs. subordtype=cause: 15.18032 
Log Odds Ratios of subordtype=temp vs. subordtype=cause: 2.72
</pre></div>
</div>
</div>
</div>
</section>
<section id="support-vector-machine-svm">
<h2>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>To understand what SVM is, we probably need to introduce the idea of <strong>support vectors</strong>.</p></li>
<li><p>Given a linearly separable dataset, it is easy to find a hyperplane (in 2D, that would be a line) that separates data into two distinct sub-groups.</p>
<ul>
<li><p>The shortest distance between the observations and the threshold (hyperplane) is called the <strong>margin</strong>.</p></li>
<li><p>A hyperplane that gives us the largest margin to make classification is referred to as <strong>Maximum Margin Classifier</strong>.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>These threshold cases are the vectors that support the maximum margin classifier, henceforth, known as the “support vectors”.
<img alt="" src="../_images/svm.002.png" /></p></li>
</ul>
<ul class="simple">
<li><p>Issues with Maximum Margin Classifier:</p>
<ul>
<li><p>Maximum Margin Classifier is very sensitive to outliers.</p></li>
<li><p>The classifier may be too biased toward the class with fewer outliers.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/svm.003.png" /></p>
<ul class="simple">
<li><p>If we choose a hyperplane which allows misclassifications, we may be able to find a better classifier.</p>
<ul>
<li><p>When we allow misclassifications, the distance between the observations and the hyperplane is called a <strong>soft margin</strong>.</p></li>
<li><p>A classifier allowing misclassifications (i.e., based on soft margins) is referred to as <strong>soft margin classifier</strong>, or <strong>support vector classifier</strong>.</p></li>
<li><p>The observations on the edge and within the <strong>soft margin</strong> are called <strong>support vectors</strong>.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/svm.004.png" /></p>
<ul class="simple">
<li><p>Support Vector Classifiers can deal with observations with outliers and the assumption is that the observations are linearly separable.</p></li>
<li><p>What if the observations are not linearly separable in the first place? This is where <strong>Support Vector Machine</strong> comes in!
<img alt="" src="../_images/svm.005.png" /></p></li>
</ul>
<ul class="simple">
<li><p>Intuition of Support Vector Machine:</p>
<ul>
<li><p>Start with data in a relatively low dimension and seek linear solutions</p></li>
<li><p>Move the data into a higher dimension (when no obvious linear classifier found)</p></li>
<li><p>Find a <strong>Support Vector Classifier</strong> that separates the higher dimensional data into two groups</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/svm-kernel.gif" /></p>
<ul class="simple">
<li><p>The mathematical tricks of moving data into a higher dimension are the <strong>kernel functions</strong>. This process is called the <strong>Kernel Tricks</strong>.</p></li>
<li><p>SVM often uses two kernel functions to find support vector classifiers in higher dimensions.</p>
<ul>
<li><p>Polynomial Kernel</p></li>
<li><p>Radial Basis Function Kernel (RBF)</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>SVM has a <strong>cost function (C)</strong>, which controls the compromise of the <strong>misclassifications</strong> and <strong>margin</strong> sizes.</p></li>
<li><p>C is the model penalty for the data points that fall on the wrong side of the hyperplane.</p>
<ul>
<li><p>A smaller C creates a softer (larger) margin, allowing more misclassifications.</p></li>
<li><p>A larger C creates a narrower margin, allowing fewer misclassifications.</p></li>
</ul>
</li>
<li><p>The greater the cost parameter, the harder the optimization will try to achieve 100 percent separation.</p></li>
</ul>
<ul class="simple">
<li><p>Heuristics for SVM</p>
<ul>
<li><p>There is no reliable rule for matching a kernel to a particular learning task.</p></li>
<li><p>The fit depends heavily on the concept to be learned as well as the amount of training data and the relationships among the features.</p></li>
<li><p>Trial and error is required by training and evaluating several SVMs on a validation dataset (i.e., <em><strong>k</strong></em>-fold cross validation)</p></li>
<li><p>The choice of kernel is often arbitrary because the performances with different kernels may vary only slightly.</p></li>
</ul>
</li>
</ul>
</section>
<section id="other-discriminative-models-skipped">
<h2>Other Discriminative Models (skipped)<a class="headerlink" href="#other-discriminative-models-skipped" title="Permalink to this headline">#</a></h2>
<p>There are many more ways to learn the weights (i.e., <span class="math notranslate nohighlight">\(\beta_i\)</span>) in the discriminative models. Different algorithms may prioritize different <strong>link functions</strong> to train the model parameters.</p>
<p>In this course, we will not cover all the details of these variants. Please refer to machine learning courses for more details on these algorithms.</p>
<section id="maximum-entropy-model-maxent">
<h3>Maximum Entropy Model (Maxent)<a class="headerlink" href="#maximum-entropy-model-maxent" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Maxent is very similar to logistic regression, and differs in mainly its way of parameterization (e.g., feature functions, link functions)</p></li>
<li><p>In Maxent, each feature function is a function of both the text feature and the class, i.e., <span class="math notranslate nohighlight">\(f(x_i, c_i)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
f_1(c_i, x_1) \equiv [C = c_i \land Contains(&quot;好看&quot;)]\\
f_2(c_i, x_2) \equiv [C = c_i \land Contains(&quot;絕配&quot;)]\\
f_3(c_i, x_3) \equiv [C = c_1 \land Ends(w,&quot;c&quot;)]\\
...
\end{split}\]</div>
<ul class="simple">
<li><p>The model will assign to each feature a <strong>weight</strong>.</p></li>
<li><p>Maxent uses a <em>softmax</em> function to transform the sum of the weigthed feature values into probabilities. (The exponetial transformation of the sum of the weighted feature values make all predicted values positive.)</p>
<ul>
<li><p>Numerator = The probability of the feature i co-occuring with specific class.</p></li>
<li><p>Denominator = The sum of the probability of the feature co-occurring with the other classes.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
p(c|d,\lambda) = \frac{e^{\sum_{i}\lambda_if_i(c,x_i)}}{\sum_{c'}{e^{\sum_{i}\lambda_if_i(c',x_i)}}}\\
\end{split}\]</div>
</section>
</section>
<section id="other-classification-models-skipped">
<h2>Other Classification Models (skipped)<a class="headerlink" href="#other-classification-models-skipped" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>There are other classification models that are not probabilistic</p>
<ul>
<li><p><strong>Decision Tree</strong></p>
<ul>
<li><p>It uses a split condition to predict class labels based on one or multiple input features.</p></li>
<li><p>The classification process starts from the root node of the tree; at each node, the classifier will check which input feature will most effectively split the data into sub-groups.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="emsemble-methods">
<h2>Emsemble Methods<a class="headerlink" href="#emsemble-methods" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Bagging</p>
<ul>
<li><p>Take subsets of data (e.g., bootstrap samples) and train a model on each subset in parallel</p></li>
<li><p>The subsets are allowed to simultaneously vote on the outcome</p></li>
<li><p>The final outcome is often an average aggregation.</p></li>
<li><p>Example: <strong>Random Forests</strong></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Boosting</p>
<ul>
<li><p>Utilize sequential modeling by improving one model from the errors of the previous model</p></li>
<li><p>Often use the output of one model as an input into the next in a form of sequential processing</p></li>
<li><p>Example: <strong>Gradient boosting machines</strong></p></li>
</ul>
</li>
</ul>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Feature engineering is a lot more important than algorithm selection.</p></li>
<li><p>With good feature representations of the data, usually it doesn’t really matter which machine learning algorithm we choose.</p></li>
<li><p>Generally, <strong>discriminative</strong> models out-perform <strong>generative</strong> models.</p></li>
<li><p>Maybe one day the optimal algorithm can be determined <em>automatically</em> via machine learning as well!!</p></li>
<li><p>Please see <a class="reference external" href="https://www.marktechpost.com/2021/02/28/google-ai-introduces-model-search-an-open-source-platform-for-finding-optimal-machine-learning-ml-models/?fbclid=IwAR3FkqJzXGIqG-X25BXslDZ077F9s4onOaQzlosc9u7qeto9LNd7prt_PNE">Google’s AutoML effort: Google AI Introduces Model Search, An Open Source Platform for Finding Optimal Machine Learning Models</a>.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>NLTK Book, Chapter 6.</p></li>
<li><p>Sarkar (2020), Chapter 5.</p></li>
<li><p>Geron (2019), Chapter 5-7</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ml-simple-case.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Machine Learning: A Simple Example</p>
      </div>
    </a>
    <a class="right-next"
       href="ml-sklearn-classification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sentiment Analysis Using Bag-of-Words</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-naive-bayes">Intuition of Naive Bayes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine-svm">Support Vector Machine (SVM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-discriminative-models-skipped">Other Discriminative Models (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-entropy-model-maxent">Maximum Entropy Model (Maxent)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-classification-models-skipped">Other Classification Models (skipped)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#emsemble-methods">Emsemble Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>