

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural Language Model: A Start &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/dl-neural-language-model-primer';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Word Embeddings" href="text-vec-embedding.html" />
    <link rel="prev" title="Sequence Models Intuition" href="dl-sequence-models-intuition.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-neural-network-from-scratch.html">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">Sequence Model with Attention for Addition Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-neural-language-model-primer.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/dl-neural-language-model-primer.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural Language Model: A Start</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-of-neural-language-model">Workflow of Neural Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram-model">Bigram Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-sequences-and-training-testing-sets">Text-to-Sequences and Training-Testing Sets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-the-next-word">One-hot Representation of the Next-Word</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-rnn-language-model">Define RNN Language Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-using-the-model">Text Generation Using the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-strategies-for-text-generation">Sampling Strategies for Text Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search-skipped">Beam Search (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#searching-in-nlp">Searching in NLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search-decoding">Beam Search Decoding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-language-model-a-start">
<h1>Neural Language Model: A Start<a class="headerlink" href="#neural-language-model-a-start" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>In this tutorial, we will look at a naive example of <strong>neural language model</strong>.</p></li>
<li><p>Given a corpus, we can build a neural language model, which will learn to predict the next word given a specified limited context.</p></li>
</ul>
<ul class="simple">
<li><p>Depending on the size of the <strong>limited context</strong>, we can implement different types of neural language model:</p>
<ul>
<li><p><strong>Bigram</strong>-based neural language model: The model uses one preceding word for the next-word prediction.</p></li>
<li><p><strong>Trigram</strong>-based neural language model: The model uses two preceding words for the next-word prediction.</p></li>
<li><p><strong>Line</strong>-based neural language model: The model uses all the existing fore-going words in the “sequence” for the next-word prediction.</p></li>
<li><p><strong>Discourse</strong>-based neural language model: The model uses inter-sentential information for next-word prediction (e.g., BERT).</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>This tutorial will demonstarte how to build a bigram-based language model.</p></li>
<li><p>In the Assignments, you need to extend the same rationale to other types of language models.</p></li>
</ul>
<section id="workflow-of-neural-language-model">
<h2>Workflow of Neural Language Model<a class="headerlink" href="#workflow-of-neural-language-model" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/neural-language-model-flowchart.png" /></p>
<div class="dropdown admonition seealso">
<p class="admonition-title">See also</p>
<p>We frequently encounter the need to train models on large datasets, which can be memory-intensive and difficult to load entirely into the local machine’s memory at once. Therefore, it’s essential to master techniques for optimizing the data loading process during model training.</p>
<p>Tensorflow provides an effective Dataset API for this. Using TensorFlow Dataset API involves several steps to create and manipulate datasets efficiently. Here’s a step-by-step guide:</p>
<ol class="arabic">
<li><p><strong>Import TensorFlow:</strong>
Make sure you have TensorFlow installed and import it into your Python script or notebook.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</li>
<li><p><strong>Create Dataset from Data:</strong>
There are different ways to create a dataset, such as from a list, NumPy array, tensors, or files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From a list</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># From a NumPy array</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

<span class="c1"># From tensors</span>
<span class="n">tensor1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">((</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">))</span>

<span class="c1"># From files</span>
<span class="n">file_paths</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;file1.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;file2.txt&#39;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TextLineDataset</span><span class="p">(</span><span class="n">file_paths</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Transformations:</strong>
Apply transformations to the dataset using methods like <code class="docutils literal notranslate"><span class="pre">map()</span></code>, <code class="docutils literal notranslate"><span class="pre">filter()</span></code>, <code class="docutils literal notranslate"><span class="pre">batch()</span></code>, <code class="docutils literal notranslate"><span class="pre">shuffle()</span></code>, etc.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Map a function to each element</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Filter elements</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Batch elements</span>
<span class="n">batched_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># Shuffle elements</span>
<span class="n">shuffled_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Repeat dataset</span>
<span class="n">repeated_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Iterate Over the Dataset:</strong>
Use a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop or iterator to iterate over the dataset and process the elements.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

<span class="c1"># If you need to batch the dataset first</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batched_dataset</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Use in Model Training:</strong>
Pass the dataset directly to the model’s <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Performance Optimization:</strong>
For better performance, consider using prefetching, caching, and parallelism.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Handling Complex Data:</strong>
For more complex data processing, you can use <code class="docutils literal notranslate"><span class="pre">tf.py_function</span></code> to apply Python functions to each element of the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Your custom Python code here</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">my_function</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
<p>These are the basic steps to use TensorFlow Dataset API effectively. Adjustments can be made based on the specific requirements of your project and the characteristics of your data.</p>
</div>
</section>
<section id="bigram-model">
<h2>Bigram Model<a class="headerlink" href="#bigram-model" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>A bigram-based language model assumes that the next word (to be predicted) depends only on one preceding word.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Dependencies</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span><span class="p">,</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span>
</pre></div>
</div>
</div>
</div>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A quick reminder of important parameters for <code class="docutils literal notranslate"><span class="pre">Tokenzier()</span></code>:</p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">num_words</span></code></strong>: the maximum number of words to keep, based on word frequency. Only the most common <code class="docutils literal notranslate"><span class="pre">num_words-1</span></code> words will be kept.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">filters</span></code></strong>: a string where each element is a character that will be filtered from the texts. The default includes all punctuations, plus tabs and line breaks (except for the <code class="docutils literal notranslate"><span class="pre">'</span></code> character).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">lower</span></code></strong>: boolean. Whether to convert the texts to lowercase.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">split</span></code></strong>: string. Separator for word splitting.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">char_level</span></code></strong>: if True, every character will be treated as a token.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">oov_token</span></code></strong>: if given, it will be added to <code class="docutils literal notranslate"><span class="pre">word_index</span></code> and used to replace out-of-vocabulary words during <code class="docutils literal notranslate"><span class="pre">text_to_sequence</span></code> calls</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># source text</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot; Jack and Jill went up the hill</span><span class="se">\n</span>
<span class="s2">		To fetch a pail of water</span><span class="se">\n</span>
<span class="s2">		Jack fell down and broke his crown</span><span class="se">\n</span>
<span class="s2">		And Jill came tumbling after</span><span class="se">\n</span><span class="s2"> &quot;&quot;&quot;</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

<span class="c1"># integer encode text</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># now the data consists of a sequence of word index integers</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># determine the vocabulary size</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Size: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary Size: 22
{&#39;and&#39;: 1, &#39;jack&#39;: 2, &#39;jill&#39;: 3, &#39;went&#39;: 4, &#39;up&#39;: 5, &#39;the&#39;: 6, &#39;hill&#39;: 7, &#39;to&#39;: 8, &#39;fetch&#39;: 9, &#39;a&#39;: 10, &#39;pail&#39;: 11, &#39;of&#39;: 12, &#39;water&#39;: 13, &#39;fell&#39;: 14, &#39;down&#39;: 15, &#39;broke&#39;: 16, &#39;his&#39;: 17, &#39;crown&#39;: 18, &#39;came&#39;: 19, &#39;tumbling&#39;: 20, &#39;after&#39;: 21}
</pre></div>
</div>
</div>
</div>
</section>
<section id="text-to-sequences-and-training-testing-sets">
<h3>Text-to-Sequences and Training-Testing Sets<a class="headerlink" href="#text-to-sequences-and-training-testing-sets" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Principles for bigrams extraction</p>
<ul>
<li><p>When we create bigrams as the input sequences for network training, we need to make sure that we do not include <strong>unmeaningful</strong> bigrams, such as bigrams spanning the text boundaries, or sentence boundaries.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create bigrams sequences</span>

<span class="c1">## bigrams holder</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>


<span class="c1">## Extract bigrams from each text</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">encoded</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">e</span><span class="p">)):</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Sequences: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total Sequences: 21
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[2, 1],
       [1, 3],
       [3, 4],
       [4, 5],
       [5, 6]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>A sequence contains both our input and also output of the network.</p></li>
<li><p>That is, for bigram-based LM, the first word is the input <em>X</em> and the second word is the expected output <em>y</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split into X and y elements</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 1]
 [1 3]
 [3 4]
 [4 5]
 [5 6]]
[2 1 3 4 5]
[1 3 4 5 6]
</pre></div>
</div>
</div>
</div>
</section>
<section id="one-hot-representation-of-the-next-word">
<h3>One-hot Representation of the Next-Word<a class="headerlink" href="#one-hot-representation-of-the-next-word" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Because the neural language model is going to be a multi-class classifier (for word prediction), we need to convert our <code class="docutils literal notranslate"><span class="pre">y</span></code> into one-hot encoding.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># one hot encode outputs</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(21, 22)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-rnn-language-model">
<h3>Define RNN Language Model<a class="headerlink" href="#define-rnn-language-model" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>  <span class="c1"># LSTM Complexity</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 1, 10)             220       
_________________________________________________________________
lstm (LSTM)                  (None, 50)                12200     
_________________________________________________________________
dense (Dense)                (None, 22)                1122      
=================================================================
Total params: 13,542
Trainable params: 13,542
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compile network</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="c1"># fit network</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/500
1/1 - 2s - loss: 3.0898 - accuracy: 0.2381
Epoch 2/500
1/1 - 0s - loss: 3.0889 - accuracy: 0.2381
Epoch 3/500
1/1 - 0s - loss: 3.0881 - accuracy: 0.2381
Epoch 4/500
1/1 - 0s - loss: 3.0872 - accuracy: 0.2381
Epoch 5/500
1/1 - 0s - loss: 3.0863 - accuracy: 0.2857
Epoch 6/500
1/1 - 0s - loss: 3.0855 - accuracy: 0.2857
Epoch 7/500
1/1 - 0s - loss: 3.0846 - accuracy: 0.2857
Epoch 8/500
1/1 - 0s - loss: 3.0837 - accuracy: 0.3333
Epoch 9/500
1/1 - 0s - loss: 3.0828 - accuracy: 0.3333
Epoch 10/500
1/1 - 0s - loss: 3.0819 - accuracy: 0.3333
Epoch 11/500
1/1 - 0s - loss: 3.0810 - accuracy: 0.3333
Epoch 12/500
1/1 - 0s - loss: 3.0800 - accuracy: 0.3333
Epoch 13/500
1/1 - 0s - loss: 3.0791 - accuracy: 0.3333
Epoch 14/500
1/1 - 0s - loss: 3.0781 - accuracy: 0.3333
Epoch 15/500
1/1 - 0s - loss: 3.0771 - accuracy: 0.3333
Epoch 16/500
1/1 - 0s - loss: 3.0760 - accuracy: 0.3333
Epoch 17/500
1/1 - 0s - loss: 3.0750 - accuracy: 0.3333
Epoch 18/500
1/1 - 0s - loss: 3.0739 - accuracy: 0.3333
Epoch 19/500
1/1 - 0s - loss: 3.0728 - accuracy: 0.3333
Epoch 20/500
1/1 - 0s - loss: 3.0717 - accuracy: 0.3333
Epoch 21/500
1/1 - 0s - loss: 3.0706 - accuracy: 0.3333
Epoch 22/500
1/1 - 0s - loss: 3.0694 - accuracy: 0.2857
Epoch 23/500
1/1 - 0s - loss: 3.0682 - accuracy: 0.2381
Epoch 24/500
1/1 - 0s - loss: 3.0669 - accuracy: 0.2381
Epoch 25/500
1/1 - 0s - loss: 3.0657 - accuracy: 0.2381
Epoch 26/500
1/1 - 0s - loss: 3.0644 - accuracy: 0.2381
Epoch 27/500
1/1 - 0s - loss: 3.0630 - accuracy: 0.2381
Epoch 28/500
1/1 - 0s - loss: 3.0617 - accuracy: 0.2381
Epoch 29/500
1/1 - 0s - loss: 3.0602 - accuracy: 0.2381
Epoch 30/500
1/1 - 0s - loss: 3.0588 - accuracy: 0.2381
Epoch 31/500
1/1 - 0s - loss: 3.0573 - accuracy: 0.2381
Epoch 32/500
1/1 - 0s - loss: 3.0557 - accuracy: 0.2381
Epoch 33/500
1/1 - 0s - loss: 3.0542 - accuracy: 0.2381
Epoch 34/500
1/1 - 0s - loss: 3.0525 - accuracy: 0.2381
Epoch 35/500
1/1 - 0s - loss: 3.0509 - accuracy: 0.2381
Epoch 36/500
1/1 - 0s - loss: 3.0491 - accuracy: 0.2381
Epoch 37/500
1/1 - 0s - loss: 3.0474 - accuracy: 0.2381
Epoch 38/500
1/1 - 0s - loss: 3.0455 - accuracy: 0.2381
Epoch 39/500
1/1 - 0s - loss: 3.0436 - accuracy: 0.2381
Epoch 40/500
1/1 - 0s - loss: 3.0417 - accuracy: 0.2381
Epoch 41/500
1/1 - 0s - loss: 3.0397 - accuracy: 0.2381
Epoch 42/500
1/1 - 0s - loss: 3.0377 - accuracy: 0.2381
Epoch 43/500
1/1 - 0s - loss: 3.0355 - accuracy: 0.2381
Epoch 44/500
1/1 - 0s - loss: 3.0334 - accuracy: 0.2381
Epoch 45/500
1/1 - 0s - loss: 3.0311 - accuracy: 0.1905
Epoch 46/500
1/1 - 0s - loss: 3.0288 - accuracy: 0.1905
Epoch 47/500
1/1 - 0s - loss: 3.0264 - accuracy: 0.2381
Epoch 48/500
1/1 - 0s - loss: 3.0240 - accuracy: 0.2381
Epoch 49/500
1/1 - 0s - loss: 3.0215 - accuracy: 0.2381
Epoch 50/500
1/1 - 0s - loss: 3.0189 - accuracy: 0.2381
Epoch 51/500
1/1 - 0s - loss: 3.0162 - accuracy: 0.2381
Epoch 52/500
1/1 - 0s - loss: 3.0135 - accuracy: 0.2381
Epoch 53/500
1/1 - 0s - loss: 3.0106 - accuracy: 0.2381
Epoch 54/500
1/1 - 0s - loss: 3.0077 - accuracy: 0.2381
Epoch 55/500
1/1 - 0s - loss: 3.0047 - accuracy: 0.2381
Epoch 56/500
1/1 - 0s - loss: 3.0017 - accuracy: 0.2381
Epoch 57/500
1/1 - 0s - loss: 2.9985 - accuracy: 0.2381
Epoch 58/500
1/1 - 0s - loss: 2.9952 - accuracy: 0.2381
Epoch 59/500
1/1 - 0s - loss: 2.9919 - accuracy: 0.2381
Epoch 60/500
1/1 - 0s - loss: 2.9884 - accuracy: 0.2381
Epoch 61/500
1/1 - 0s - loss: 2.9849 - accuracy: 0.2381
Epoch 62/500
1/1 - 0s - loss: 2.9812 - accuracy: 0.2381
Epoch 63/500
1/1 - 0s - loss: 2.9774 - accuracy: 0.2381
Epoch 64/500
1/1 - 0s - loss: 2.9736 - accuracy: 0.2381
Epoch 65/500
1/1 - 0s - loss: 2.9696 - accuracy: 0.2381
Epoch 66/500
1/1 - 0s - loss: 2.9655 - accuracy: 0.2381
Epoch 67/500
1/1 - 0s - loss: 2.9613 - accuracy: 0.2381
Epoch 68/500
1/1 - 0s - loss: 2.9570 - accuracy: 0.2381
Epoch 69/500
1/1 - 0s - loss: 2.9526 - accuracy: 0.2381
Epoch 70/500
1/1 - 0s - loss: 2.9480 - accuracy: 0.2857
Epoch 71/500
1/1 - 0s - loss: 2.9433 - accuracy: 0.2857
Epoch 72/500
1/1 - 0s - loss: 2.9385 - accuracy: 0.2857
Epoch 73/500
1/1 - 0s - loss: 2.9336 - accuracy: 0.2857
Epoch 74/500
1/1 - 0s - loss: 2.9285 - accuracy: 0.2857
Epoch 75/500
1/1 - 0s - loss: 2.9233 - accuracy: 0.2857
Epoch 76/500
1/1 - 0s - loss: 2.9179 - accuracy: 0.2857
Epoch 77/500
1/1 - 0s - loss: 2.9125 - accuracy: 0.2857
Epoch 78/500
1/1 - 0s - loss: 2.9068 - accuracy: 0.2857
Epoch 79/500
1/1 - 0s - loss: 2.9010 - accuracy: 0.2857
Epoch 80/500
1/1 - 0s - loss: 2.8951 - accuracy: 0.2857
Epoch 81/500
1/1 - 0s - loss: 2.8890 - accuracy: 0.2857
Epoch 82/500
1/1 - 0s - loss: 2.8828 - accuracy: 0.2857
Epoch 83/500
1/1 - 0s - loss: 2.8764 - accuracy: 0.2857
Epoch 84/500
1/1 - 0s - loss: 2.8698 - accuracy: 0.2857
Epoch 85/500
1/1 - 0s - loss: 2.8631 - accuracy: 0.2857
Epoch 86/500
1/1 - 0s - loss: 2.8562 - accuracy: 0.2857
Epoch 87/500
1/1 - 0s - loss: 2.8492 - accuracy: 0.2857
Epoch 88/500
1/1 - 0s - loss: 2.8420 - accuracy: 0.2857
Epoch 89/500
1/1 - 0s - loss: 2.8346 - accuracy: 0.2857
Epoch 90/500
1/1 - 0s - loss: 2.8270 - accuracy: 0.2857
Epoch 91/500
1/1 - 0s - loss: 2.8193 - accuracy: 0.2857
Epoch 92/500
1/1 - 0s - loss: 2.8114 - accuracy: 0.2857
Epoch 93/500
1/1 - 0s - loss: 2.8033 - accuracy: 0.2857
Epoch 94/500
1/1 - 0s - loss: 2.7951 - accuracy: 0.2857
Epoch 95/500
1/1 - 0s - loss: 2.7866 - accuracy: 0.2857
Epoch 96/500
1/1 - 0s - loss: 2.7780 - accuracy: 0.2857
Epoch 97/500
1/1 - 0s - loss: 2.7692 - accuracy: 0.2857
Epoch 98/500
1/1 - 0s - loss: 2.7602 - accuracy: 0.2857
Epoch 99/500
1/1 - 0s - loss: 2.7510 - accuracy: 0.2857
Epoch 100/500
1/1 - 0s - loss: 2.7416 - accuracy: 0.2857
Epoch 101/500
1/1 - 0s - loss: 2.7321 - accuracy: 0.2857
Epoch 102/500
1/1 - 0s - loss: 2.7224 - accuracy: 0.2857
Epoch 103/500
1/1 - 0s - loss: 2.7124 - accuracy: 0.2857
Epoch 104/500
1/1 - 0s - loss: 2.7023 - accuracy: 0.2857
Epoch 105/500
1/1 - 0s - loss: 2.6920 - accuracy: 0.2857
Epoch 106/500
1/1 - 0s - loss: 2.6816 - accuracy: 0.2857
Epoch 107/500
1/1 - 0s - loss: 2.6709 - accuracy: 0.2857
Epoch 108/500
1/1 - 0s - loss: 2.6601 - accuracy: 0.2857
Epoch 109/500
1/1 - 0s - loss: 2.6491 - accuracy: 0.2857
Epoch 110/500
1/1 - 0s - loss: 2.6379 - accuracy: 0.2857
Epoch 111/500
1/1 - 0s - loss: 2.6266 - accuracy: 0.2857
Epoch 112/500
1/1 - 0s - loss: 2.6151 - accuracy: 0.2857
Epoch 113/500
1/1 - 0s - loss: 2.6034 - accuracy: 0.2857
Epoch 114/500
1/1 - 0s - loss: 2.5915 - accuracy: 0.2857
Epoch 115/500
1/1 - 0s - loss: 2.5795 - accuracy: 0.2857
Epoch 116/500
1/1 - 0s - loss: 2.5673 - accuracy: 0.2857
Epoch 117/500
1/1 - 0s - loss: 2.5550 - accuracy: 0.2857
Epoch 118/500
1/1 - 0s - loss: 2.5426 - accuracy: 0.2857
Epoch 119/500
1/1 - 0s - loss: 2.5300 - accuracy: 0.2857
Epoch 120/500
1/1 - 0s - loss: 2.5172 - accuracy: 0.2857
Epoch 121/500
1/1 - 0s - loss: 2.5044 - accuracy: 0.3333
Epoch 122/500
1/1 - 0s - loss: 2.4914 - accuracy: 0.3333
Epoch 123/500
1/1 - 0s - loss: 2.4782 - accuracy: 0.3333
Epoch 124/500
1/1 - 0s - loss: 2.4650 - accuracy: 0.3333
Epoch 125/500
1/1 - 0s - loss: 2.4517 - accuracy: 0.3810
Epoch 126/500
1/1 - 0s - loss: 2.4382 - accuracy: 0.3810
Epoch 127/500
1/1 - 0s - loss: 2.4247 - accuracy: 0.4286
Epoch 128/500
1/1 - 0s - loss: 2.4111 - accuracy: 0.4286
Epoch 129/500
1/1 - 0s - loss: 2.3974 - accuracy: 0.4286
Epoch 130/500
1/1 - 0s - loss: 2.3836 - accuracy: 0.4286
Epoch 131/500
1/1 - 0s - loss: 2.3697 - accuracy: 0.4762
Epoch 132/500
1/1 - 0s - loss: 2.3558 - accuracy: 0.4762
Epoch 133/500
1/1 - 0s - loss: 2.3419 - accuracy: 0.4762
Epoch 134/500
1/1 - 0s - loss: 2.3278 - accuracy: 0.5238
Epoch 135/500
1/1 - 0s - loss: 2.3138 - accuracy: 0.5238
Epoch 136/500
1/1 - 0s - loss: 2.2997 - accuracy: 0.5238
Epoch 137/500
1/1 - 0s - loss: 2.2856 - accuracy: 0.5238
Epoch 138/500
1/1 - 0s - loss: 2.2714 - accuracy: 0.5238
Epoch 139/500
1/1 - 0s - loss: 2.2572 - accuracy: 0.5238
Epoch 140/500
1/1 - 0s - loss: 2.2431 - accuracy: 0.5238
Epoch 141/500
1/1 - 0s - loss: 2.2289 - accuracy: 0.5238
Epoch 142/500
1/1 - 0s - loss: 2.2147 - accuracy: 0.5238
Epoch 143/500
1/1 - 0s - loss: 2.2005 - accuracy: 0.5238
Epoch 144/500
1/1 - 0s - loss: 2.1863 - accuracy: 0.5238
Epoch 145/500
1/1 - 0s - loss: 2.1721 - accuracy: 0.5238
Epoch 146/500
1/1 - 0s - loss: 2.1579 - accuracy: 0.5238
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 147/500
1/1 - 0s - loss: 2.1438 - accuracy: 0.5238
Epoch 148/500
1/1 - 0s - loss: 2.1296 - accuracy: 0.5238
Epoch 149/500
1/1 - 0s - loss: 2.1155 - accuracy: 0.5238
Epoch 150/500
1/1 - 0s - loss: 2.1014 - accuracy: 0.5238
Epoch 151/500
1/1 - 0s - loss: 2.0874 - accuracy: 0.5238
Epoch 152/500
1/1 - 0s - loss: 2.0733 - accuracy: 0.5238
Epoch 153/500
1/1 - 0s - loss: 2.0593 - accuracy: 0.5238
Epoch 154/500
1/1 - 0s - loss: 2.0454 - accuracy: 0.5238
Epoch 155/500
1/1 - 0s - loss: 2.0314 - accuracy: 0.5238
Epoch 156/500
1/1 - 0s - loss: 2.0175 - accuracy: 0.5238
Epoch 157/500
1/1 - 0s - loss: 2.0037 - accuracy: 0.5238
Epoch 158/500
1/1 - 0s - loss: 1.9898 - accuracy: 0.5714
Epoch 159/500
1/1 - 0s - loss: 1.9760 - accuracy: 0.5714
Epoch 160/500
1/1 - 0s - loss: 1.9622 - accuracy: 0.5714
Epoch 161/500
1/1 - 0s - loss: 1.9485 - accuracy: 0.5714
Epoch 162/500
1/1 - 0s - loss: 1.9348 - accuracy: 0.5714
Epoch 163/500
1/1 - 0s - loss: 1.9211 - accuracy: 0.5714
Epoch 164/500
1/1 - 0s - loss: 1.9075 - accuracy: 0.5714
Epoch 165/500
1/1 - 0s - loss: 1.8939 - accuracy: 0.5714
Epoch 166/500
1/1 - 0s - loss: 1.8803 - accuracy: 0.5714
Epoch 167/500
1/1 - 0s - loss: 1.8668 - accuracy: 0.5714
Epoch 168/500
1/1 - 0s - loss: 1.8533 - accuracy: 0.6190
Epoch 169/500
1/1 - 0s - loss: 1.8398 - accuracy: 0.6190
Epoch 170/500
1/1 - 0s - loss: 1.8263 - accuracy: 0.6190
Epoch 171/500
1/1 - 0s - loss: 1.8129 - accuracy: 0.6190
Epoch 172/500
1/1 - 0s - loss: 1.7995 - accuracy: 0.6190
Epoch 173/500
1/1 - 0s - loss: 1.7861 - accuracy: 0.6190
Epoch 174/500
1/1 - 0s - loss: 1.7728 - accuracy: 0.6190
Epoch 175/500
1/1 - 0s - loss: 1.7595 - accuracy: 0.6190
Epoch 176/500
1/1 - 0s - loss: 1.7462 - accuracy: 0.6667
Epoch 177/500
1/1 - 0s - loss: 1.7329 - accuracy: 0.6667
Epoch 178/500
1/1 - 0s - loss: 1.7196 - accuracy: 0.6667
Epoch 179/500
1/1 - 0s - loss: 1.7064 - accuracy: 0.6667
Epoch 180/500
1/1 - 0s - loss: 1.6932 - accuracy: 0.6667
Epoch 181/500
1/1 - 0s - loss: 1.6800 - accuracy: 0.6667
Epoch 182/500
1/1 - 0s - loss: 1.6668 - accuracy: 0.6667
Epoch 183/500
1/1 - 0s - loss: 1.6537 - accuracy: 0.6667
Epoch 184/500
1/1 - 0s - loss: 1.6406 - accuracy: 0.6667
Epoch 185/500
1/1 - 0s - loss: 1.6275 - accuracy: 0.6667
Epoch 186/500
1/1 - 0s - loss: 1.6144 - accuracy: 0.6667
Epoch 187/500
1/1 - 0s - loss: 1.6013 - accuracy: 0.6667
Epoch 188/500
1/1 - 0s - loss: 1.5883 - accuracy: 0.7143
Epoch 189/500
1/1 - 0s - loss: 1.5753 - accuracy: 0.7619
Epoch 190/500
1/1 - 0s - loss: 1.5623 - accuracy: 0.7619
Epoch 191/500
1/1 - 0s - loss: 1.5493 - accuracy: 0.7619
Epoch 192/500
1/1 - 0s - loss: 1.5364 - accuracy: 0.7619
Epoch 193/500
1/1 - 0s - loss: 1.5235 - accuracy: 0.7619
Epoch 194/500
1/1 - 0s - loss: 1.5106 - accuracy: 0.7619
Epoch 195/500
1/1 - 0s - loss: 1.4977 - accuracy: 0.7619
Epoch 196/500
1/1 - 0s - loss: 1.4849 - accuracy: 0.7619
Epoch 197/500
1/1 - 0s - loss: 1.4721 - accuracy: 0.7619
Epoch 198/500
1/1 - 0s - loss: 1.4593 - accuracy: 0.7619
Epoch 199/500
1/1 - 0s - loss: 1.4466 - accuracy: 0.7619
Epoch 200/500
1/1 - 0s - loss: 1.4338 - accuracy: 0.8095
Epoch 201/500
1/1 - 0s - loss: 1.4212 - accuracy: 0.8095
Epoch 202/500
1/1 - 0s - loss: 1.4085 - accuracy: 0.8095
Epoch 203/500
1/1 - 0s - loss: 1.3959 - accuracy: 0.8095
Epoch 204/500
1/1 - 0s - loss: 1.3833 - accuracy: 0.8095
Epoch 205/500
1/1 - 0s - loss: 1.3708 - accuracy: 0.8095
Epoch 206/500
1/1 - 0s - loss: 1.3583 - accuracy: 0.8095
Epoch 207/500
1/1 - 0s - loss: 1.3459 - accuracy: 0.8095
Epoch 208/500
1/1 - 0s - loss: 1.3335 - accuracy: 0.8095
Epoch 209/500
1/1 - 0s - loss: 1.3211 - accuracy: 0.8095
Epoch 210/500
1/1 - 0s - loss: 1.3088 - accuracy: 0.8095
Epoch 211/500
1/1 - 0s - loss: 1.2966 - accuracy: 0.8095
Epoch 212/500
1/1 - 0s - loss: 1.2844 - accuracy: 0.8095
Epoch 213/500
1/1 - 0s - loss: 1.2722 - accuracy: 0.8095
Epoch 214/500
1/1 - 0s - loss: 1.2601 - accuracy: 0.8095
Epoch 215/500
1/1 - 0s - loss: 1.2481 - accuracy: 0.8095
Epoch 216/500
1/1 - 0s - loss: 1.2361 - accuracy: 0.8095
Epoch 217/500
1/1 - 0s - loss: 1.2242 - accuracy: 0.8095
Epoch 218/500
1/1 - 0s - loss: 1.2123 - accuracy: 0.8095
Epoch 219/500
1/1 - 0s - loss: 1.2005 - accuracy: 0.8095
Epoch 220/500
1/1 - 0s - loss: 1.1888 - accuracy: 0.8095
Epoch 221/500
1/1 - 0s - loss: 1.1772 - accuracy: 0.8095
Epoch 222/500
1/1 - 0s - loss: 1.1656 - accuracy: 0.8095
Epoch 223/500
1/1 - 0s - loss: 1.1541 - accuracy: 0.8095
Epoch 224/500
1/1 - 0s - loss: 1.1427 - accuracy: 0.8095
Epoch 225/500
1/1 - 0s - loss: 1.1313 - accuracy: 0.8571
Epoch 226/500
1/1 - 0s - loss: 1.1200 - accuracy: 0.8571
Epoch 227/500
1/1 - 0s - loss: 1.1088 - accuracy: 0.8571
Epoch 228/500
1/1 - 0s - loss: 1.0977 - accuracy: 0.8571
Epoch 229/500
1/1 - 0s - loss: 1.0867 - accuracy: 0.8571
Epoch 230/500
1/1 - 0s - loss: 1.0757 - accuracy: 0.8571
Epoch 231/500
1/1 - 0s - loss: 1.0649 - accuracy: 0.8571
Epoch 232/500
1/1 - 0s - loss: 1.0541 - accuracy: 0.8571
Epoch 233/500
1/1 - 0s - loss: 1.0434 - accuracy: 0.8571
Epoch 234/500
1/1 - 0s - loss: 1.0328 - accuracy: 0.8571
Epoch 235/500
1/1 - 0s - loss: 1.0223 - accuracy: 0.8571
Epoch 236/500
1/1 - 0s - loss: 1.0119 - accuracy: 0.8571
Epoch 237/500
1/1 - 0s - loss: 1.0015 - accuracy: 0.8571
Epoch 238/500
1/1 - 0s - loss: 0.9913 - accuracy: 0.8571
Epoch 239/500
1/1 - 0s - loss: 0.9812 - accuracy: 0.8571
Epoch 240/500
1/1 - 0s - loss: 0.9711 - accuracy: 0.8571
Epoch 241/500
1/1 - 0s - loss: 0.9612 - accuracy: 0.8571
Epoch 242/500
1/1 - 0s - loss: 0.9513 - accuracy: 0.8571
Epoch 243/500
1/1 - 0s - loss: 0.9416 - accuracy: 0.8571
Epoch 244/500
1/1 - 0s - loss: 0.9319 - accuracy: 0.8571
Epoch 245/500
1/1 - 0s - loss: 0.9223 - accuracy: 0.8571
Epoch 246/500
1/1 - 0s - loss: 0.9129 - accuracy: 0.8571
Epoch 247/500
1/1 - 0s - loss: 0.9035 - accuracy: 0.8571
Epoch 248/500
1/1 - 0s - loss: 0.8943 - accuracy: 0.8571
Epoch 249/500
1/1 - 0s - loss: 0.8851 - accuracy: 0.8571
Epoch 250/500
1/1 - 0s - loss: 0.8760 - accuracy: 0.8571
Epoch 251/500
1/1 - 0s - loss: 0.8671 - accuracy: 0.8571
Epoch 252/500
1/1 - 0s - loss: 0.8582 - accuracy: 0.8571
Epoch 253/500
1/1 - 0s - loss: 0.8495 - accuracy: 0.8571
Epoch 254/500
1/1 - 0s - loss: 0.8408 - accuracy: 0.8571
Epoch 255/500
1/1 - 0s - loss: 0.8322 - accuracy: 0.8571
Epoch 256/500
1/1 - 0s - loss: 0.8238 - accuracy: 0.8571
Epoch 257/500
1/1 - 0s - loss: 0.8154 - accuracy: 0.8571
Epoch 258/500
1/1 - 0s - loss: 0.8071 - accuracy: 0.8571
Epoch 259/500
1/1 - 0s - loss: 0.7990 - accuracy: 0.8571
Epoch 260/500
1/1 - 0s - loss: 0.7909 - accuracy: 0.8571
Epoch 261/500
1/1 - 0s - loss: 0.7829 - accuracy: 0.8571
Epoch 262/500
1/1 - 0s - loss: 0.7750 - accuracy: 0.8571
Epoch 263/500
1/1 - 0s - loss: 0.7673 - accuracy: 0.8571
Epoch 264/500
1/1 - 0s - loss: 0.7596 - accuracy: 0.8571
Epoch 265/500
1/1 - 0s - loss: 0.7520 - accuracy: 0.8571
Epoch 266/500
1/1 - 0s - loss: 0.7445 - accuracy: 0.8571
Epoch 267/500
1/1 - 0s - loss: 0.7371 - accuracy: 0.8571
Epoch 268/500
1/1 - 0s - loss: 0.7298 - accuracy: 0.8571
Epoch 269/500
1/1 - 0s - loss: 0.7226 - accuracy: 0.8571
Epoch 270/500
1/1 - 0s - loss: 0.7155 - accuracy: 0.8571
Epoch 271/500
1/1 - 0s - loss: 0.7085 - accuracy: 0.8571
Epoch 272/500
1/1 - 0s - loss: 0.7016 - accuracy: 0.8571
Epoch 273/500
1/1 - 0s - loss: 0.6948 - accuracy: 0.8571
Epoch 274/500
1/1 - 0s - loss: 0.6880 - accuracy: 0.8571
Epoch 275/500
1/1 - 0s - loss: 0.6814 - accuracy: 0.8571
Epoch 276/500
1/1 - 0s - loss: 0.6748 - accuracy: 0.8571
Epoch 277/500
1/1 - 0s - loss: 0.6683 - accuracy: 0.8571
Epoch 278/500
1/1 - 0s - loss: 0.6620 - accuracy: 0.8571
Epoch 279/500
1/1 - 0s - loss: 0.6557 - accuracy: 0.8571
Epoch 280/500
1/1 - 0s - loss: 0.6494 - accuracy: 0.8571
Epoch 281/500
1/1 - 0s - loss: 0.6433 - accuracy: 0.8571
Epoch 282/500
1/1 - 0s - loss: 0.6373 - accuracy: 0.8571
Epoch 283/500
1/1 - 0s - loss: 0.6313 - accuracy: 0.8571
Epoch 284/500
1/1 - 0s - loss: 0.6255 - accuracy: 0.8571
Epoch 285/500
1/1 - 0s - loss: 0.6197 - accuracy: 0.8571
Epoch 286/500
1/1 - 0s - loss: 0.6140 - accuracy: 0.8571
Epoch 287/500
1/1 - 0s - loss: 0.6084 - accuracy: 0.8571
Epoch 288/500
1/1 - 0s - loss: 0.6028 - accuracy: 0.8571
Epoch 289/500
1/1 - 0s - loss: 0.5974 - accuracy: 0.8571
Epoch 290/500
1/1 - 0s - loss: 0.5920 - accuracy: 0.8571
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 291/500
1/1 - 0s - loss: 0.5867 - accuracy: 0.8571
Epoch 292/500
1/1 - 0s - loss: 0.5815 - accuracy: 0.8571
Epoch 293/500
1/1 - 0s - loss: 0.5763 - accuracy: 0.8571
Epoch 294/500
1/1 - 0s - loss: 0.5712 - accuracy: 0.8571
Epoch 295/500
1/1 - 0s - loss: 0.5663 - accuracy: 0.8571
Epoch 296/500
1/1 - 0s - loss: 0.5613 - accuracy: 0.8571
Epoch 297/500
1/1 - 0s - loss: 0.5565 - accuracy: 0.8571
Epoch 298/500
1/1 - 0s - loss: 0.5517 - accuracy: 0.8571
Epoch 299/500
1/1 - 0s - loss: 0.5470 - accuracy: 0.8571
Epoch 300/500
1/1 - 0s - loss: 0.5424 - accuracy: 0.8571
Epoch 301/500
1/1 - 0s - loss: 0.5378 - accuracy: 0.8571
Epoch 302/500
1/1 - 0s - loss: 0.5333 - accuracy: 0.8571
Epoch 303/500
1/1 - 0s - loss: 0.5289 - accuracy: 0.8571
Epoch 304/500
1/1 - 0s - loss: 0.5245 - accuracy: 0.8571
Epoch 305/500
1/1 - 0s - loss: 0.5203 - accuracy: 0.8571
Epoch 306/500
1/1 - 0s - loss: 0.5160 - accuracy: 0.8571
Epoch 307/500
1/1 - 0s - loss: 0.5119 - accuracy: 0.8571
Epoch 308/500
1/1 - 0s - loss: 0.5078 - accuracy: 0.8571
Epoch 309/500
1/1 - 0s - loss: 0.5038 - accuracy: 0.8571
Epoch 310/500
1/1 - 0s - loss: 0.4998 - accuracy: 0.8571
Epoch 311/500
1/1 - 0s - loss: 0.4959 - accuracy: 0.8571
Epoch 312/500
1/1 - 0s - loss: 0.4921 - accuracy: 0.8571
Epoch 313/500
1/1 - 0s - loss: 0.4883 - accuracy: 0.8571
Epoch 314/500
1/1 - 0s - loss: 0.4846 - accuracy: 0.8571
Epoch 315/500
1/1 - 0s - loss: 0.4809 - accuracy: 0.8571
Epoch 316/500
1/1 - 0s - loss: 0.4773 - accuracy: 0.8571
Epoch 317/500
1/1 - 0s - loss: 0.4738 - accuracy: 0.8571
Epoch 318/500
1/1 - 0s - loss: 0.4703 - accuracy: 0.8571
Epoch 319/500
1/1 - 0s - loss: 0.4668 - accuracy: 0.8571
Epoch 320/500
1/1 - 0s - loss: 0.4635 - accuracy: 0.8571
Epoch 321/500
1/1 - 0s - loss: 0.4602 - accuracy: 0.8571
Epoch 322/500
1/1 - 0s - loss: 0.4569 - accuracy: 0.8571
Epoch 323/500
1/1 - 0s - loss: 0.4537 - accuracy: 0.8571
Epoch 324/500
1/1 - 0s - loss: 0.4505 - accuracy: 0.8571
Epoch 325/500
1/1 - 0s - loss: 0.4474 - accuracy: 0.8571
Epoch 326/500
1/1 - 0s - loss: 0.4443 - accuracy: 0.8571
Epoch 327/500
1/1 - 0s - loss: 0.4413 - accuracy: 0.8571
Epoch 328/500
1/1 - 0s - loss: 0.4384 - accuracy: 0.8571
Epoch 329/500
1/1 - 0s - loss: 0.4354 - accuracy: 0.8571
Epoch 330/500
1/1 - 0s - loss: 0.4326 - accuracy: 0.8571
Epoch 331/500
1/1 - 0s - loss: 0.4298 - accuracy: 0.8571
Epoch 332/500
1/1 - 0s - loss: 0.4270 - accuracy: 0.8571
Epoch 333/500
1/1 - 0s - loss: 0.4243 - accuracy: 0.8571
Epoch 334/500
1/1 - 0s - loss: 0.4216 - accuracy: 0.8571
Epoch 335/500
1/1 - 0s - loss: 0.4189 - accuracy: 0.8571
Epoch 336/500
1/1 - 0s - loss: 0.4164 - accuracy: 0.8571
Epoch 337/500
1/1 - 0s - loss: 0.4138 - accuracy: 0.8571
Epoch 338/500
1/1 - 0s - loss: 0.4113 - accuracy: 0.8571
Epoch 339/500
1/1 - 0s - loss: 0.4088 - accuracy: 0.8571
Epoch 340/500
1/1 - 0s - loss: 0.4064 - accuracy: 0.8571
Epoch 341/500
1/1 - 0s - loss: 0.4040 - accuracy: 0.8571
Epoch 342/500
1/1 - 0s - loss: 0.4017 - accuracy: 0.8571
Epoch 343/500
1/1 - 0s - loss: 0.3994 - accuracy: 0.8571
Epoch 344/500
1/1 - 0s - loss: 0.3971 - accuracy: 0.8571
Epoch 345/500
1/1 - 0s - loss: 0.3948 - accuracy: 0.8571
Epoch 346/500
1/1 - 0s - loss: 0.3927 - accuracy: 0.8571
Epoch 347/500
1/1 - 0s - loss: 0.3905 - accuracy: 0.8571
Epoch 348/500
1/1 - 0s - loss: 0.3884 - accuracy: 0.8571
Epoch 349/500
1/1 - 0s - loss: 0.3863 - accuracy: 0.8571
Epoch 350/500
1/1 - 0s - loss: 0.3842 - accuracy: 0.8571
Epoch 351/500
1/1 - 0s - loss: 0.3822 - accuracy: 0.8571
Epoch 352/500
1/1 - 0s - loss: 0.3802 - accuracy: 0.8571
Epoch 353/500
1/1 - 0s - loss: 0.3783 - accuracy: 0.8571
Epoch 354/500
1/1 - 0s - loss: 0.3763 - accuracy: 0.8571
Epoch 355/500
1/1 - 0s - loss: 0.3744 - accuracy: 0.8571
Epoch 356/500
1/1 - 0s - loss: 0.3726 - accuracy: 0.8571
Epoch 357/500
1/1 - 0s - loss: 0.3707 - accuracy: 0.8571
Epoch 358/500
1/1 - 0s - loss: 0.3689 - accuracy: 0.8571
Epoch 359/500
1/1 - 0s - loss: 0.3672 - accuracy: 0.8571
Epoch 360/500
1/1 - 0s - loss: 0.3654 - accuracy: 0.8571
Epoch 361/500
1/1 - 0s - loss: 0.3637 - accuracy: 0.8571
Epoch 362/500
1/1 - 0s - loss: 0.3620 - accuracy: 0.8571
Epoch 363/500
1/1 - 0s - loss: 0.3604 - accuracy: 0.8571
Epoch 364/500
1/1 - 0s - loss: 0.3587 - accuracy: 0.8571
Epoch 365/500
1/1 - 0s - loss: 0.3571 - accuracy: 0.8571
Epoch 366/500
1/1 - 0s - loss: 0.3556 - accuracy: 0.8571
Epoch 367/500
1/1 - 0s - loss: 0.3540 - accuracy: 0.8571
Epoch 368/500
1/1 - 0s - loss: 0.3525 - accuracy: 0.8571
Epoch 369/500
1/1 - 0s - loss: 0.3510 - accuracy: 0.8571
Epoch 370/500
1/1 - 0s - loss: 0.3495 - accuracy: 0.8571
Epoch 371/500
1/1 - 0s - loss: 0.3480 - accuracy: 0.8571
Epoch 372/500
1/1 - 0s - loss: 0.3466 - accuracy: 0.8571
Epoch 373/500
1/1 - 0s - loss: 0.3452 - accuracy: 0.8571
Epoch 374/500
1/1 - 0s - loss: 0.3438 - accuracy: 0.8571
Epoch 375/500
1/1 - 0s - loss: 0.3424 - accuracy: 0.8571
Epoch 376/500
1/1 - 0s - loss: 0.3411 - accuracy: 0.8571
Epoch 377/500
1/1 - 0s - loss: 0.3398 - accuracy: 0.8571
Epoch 378/500
1/1 - 0s - loss: 0.3385 - accuracy: 0.8571
Epoch 379/500
1/1 - 0s - loss: 0.3372 - accuracy: 0.8571
Epoch 380/500
1/1 - 0s - loss: 0.3359 - accuracy: 0.8571
Epoch 381/500
1/1 - 0s - loss: 0.3347 - accuracy: 0.8571
Epoch 382/500
1/1 - 0s - loss: 0.3335 - accuracy: 0.8571
Epoch 383/500
1/1 - 0s - loss: 0.3322 - accuracy: 0.8571
Epoch 384/500
1/1 - 0s - loss: 0.3311 - accuracy: 0.8571
Epoch 385/500
1/1 - 0s - loss: 0.3299 - accuracy: 0.8571
Epoch 386/500
1/1 - 0s - loss: 0.3287 - accuracy: 0.8571
Epoch 387/500
1/1 - 0s - loss: 0.3276 - accuracy: 0.8571
Epoch 388/500
1/1 - 0s - loss: 0.3265 - accuracy: 0.8571
Epoch 389/500
1/1 - 0s - loss: 0.3254 - accuracy: 0.8571
Epoch 390/500
1/1 - 0s - loss: 0.3243 - accuracy: 0.8571
Epoch 391/500
1/1 - 0s - loss: 0.3233 - accuracy: 0.8571
Epoch 392/500
1/1 - 0s - loss: 0.3222 - accuracy: 0.8571
Epoch 393/500
1/1 - 0s - loss: 0.3212 - accuracy: 0.8571
Epoch 394/500
1/1 - 0s - loss: 0.3202 - accuracy: 0.8571
Epoch 395/500
1/1 - 0s - loss: 0.3192 - accuracy: 0.8571
Epoch 396/500
1/1 - 0s - loss: 0.3182 - accuracy: 0.8571
Epoch 397/500
1/1 - 0s - loss: 0.3172 - accuracy: 0.8571
Epoch 398/500
1/1 - 0s - loss: 0.3162 - accuracy: 0.8571
Epoch 399/500
1/1 - 0s - loss: 0.3153 - accuracy: 0.8571
Epoch 400/500
1/1 - 0s - loss: 0.3144 - accuracy: 0.8571
Epoch 401/500
1/1 - 0s - loss: 0.3134 - accuracy: 0.8571
Epoch 402/500
1/1 - 0s - loss: 0.3125 - accuracy: 0.8571
Epoch 403/500
1/1 - 0s - loss: 0.3117 - accuracy: 0.8571
Epoch 404/500
1/1 - 0s - loss: 0.3108 - accuracy: 0.8571
Epoch 405/500
1/1 - 0s - loss: 0.3099 - accuracy: 0.8571
Epoch 406/500
1/1 - 0s - loss: 0.3091 - accuracy: 0.8571
Epoch 407/500
1/1 - 0s - loss: 0.3082 - accuracy: 0.8571
Epoch 408/500
1/1 - 0s - loss: 0.3074 - accuracy: 0.8571
Epoch 409/500
1/1 - 0s - loss: 0.3066 - accuracy: 0.8571
Epoch 410/500
1/1 - 0s - loss: 0.3058 - accuracy: 0.8571
Epoch 411/500
1/1 - 0s - loss: 0.3050 - accuracy: 0.8571
Epoch 412/500
1/1 - 0s - loss: 0.3042 - accuracy: 0.8571
Epoch 413/500
1/1 - 0s - loss: 0.3034 - accuracy: 0.8571
Epoch 414/500
1/1 - 0s - loss: 0.3027 - accuracy: 0.8571
Epoch 415/500
1/1 - 0s - loss: 0.3019 - accuracy: 0.8571
Epoch 416/500
1/1 - 0s - loss: 0.3012 - accuracy: 0.8571
Epoch 417/500
1/1 - 0s - loss: 0.3004 - accuracy: 0.8571
Epoch 418/500
1/1 - 0s - loss: 0.2997 - accuracy: 0.8571
Epoch 419/500
1/1 - 0s - loss: 0.2990 - accuracy: 0.8571
Epoch 420/500
1/1 - 0s - loss: 0.2983 - accuracy: 0.8571
Epoch 421/500
1/1 - 0s - loss: 0.2976 - accuracy: 0.8571
Epoch 422/500
1/1 - 0s - loss: 0.2969 - accuracy: 0.8571
Epoch 423/500
1/1 - 0s - loss: 0.2963 - accuracy: 0.8571
Epoch 424/500
1/1 - 0s - loss: 0.2956 - accuracy: 0.8571
Epoch 425/500
1/1 - 0s - loss: 0.2950 - accuracy: 0.8571
Epoch 426/500
1/1 - 0s - loss: 0.2943 - accuracy: 0.8571
Epoch 427/500
1/1 - 0s - loss: 0.2937 - accuracy: 0.8571
Epoch 428/500
1/1 - 0s - loss: 0.2931 - accuracy: 0.8571
Epoch 429/500
1/1 - 0s - loss: 0.2924 - accuracy: 0.8571
Epoch 430/500
1/1 - 0s - loss: 0.2918 - accuracy: 0.8571
Epoch 431/500
1/1 - 0s - loss: 0.2912 - accuracy: 0.8571
Epoch 432/500
1/1 - 0s - loss: 0.2906 - accuracy: 0.8571
Epoch 433/500
1/1 - 0s - loss: 0.2900 - accuracy: 0.8571
Epoch 434/500
1/1 - 0s - loss: 0.2895 - accuracy: 0.8571
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 435/500
1/1 - 0s - loss: 0.2889 - accuracy: 0.8571
Epoch 436/500
1/1 - 0s - loss: 0.2883 - accuracy: 0.8571
Epoch 437/500
1/1 - 0s - loss: 0.2878 - accuracy: 0.8571
Epoch 438/500
1/1 - 0s - loss: 0.2872 - accuracy: 0.8571
Epoch 439/500
1/1 - 0s - loss: 0.2867 - accuracy: 0.8571
Epoch 440/500
1/1 - 0s - loss: 0.2861 - accuracy: 0.8571
Epoch 441/500
1/1 - 0s - loss: 0.2856 - accuracy: 0.8571
Epoch 442/500
1/1 - 0s - loss: 0.2851 - accuracy: 0.8571
Epoch 443/500
1/1 - 0s - loss: 0.2846 - accuracy: 0.8571
Epoch 444/500
1/1 - 0s - loss: 0.2840 - accuracy: 0.8571
Epoch 445/500
1/1 - 0s - loss: 0.2835 - accuracy: 0.8571
Epoch 446/500
1/1 - 0s - loss: 0.2830 - accuracy: 0.8571
Epoch 447/500
1/1 - 0s - loss: 0.2826 - accuracy: 0.8571
Epoch 448/500
1/1 - 0s - loss: 0.2821 - accuracy: 0.8571
Epoch 449/500
1/1 - 0s - loss: 0.2816 - accuracy: 0.8571
Epoch 450/500
1/1 - 0s - loss: 0.2811 - accuracy: 0.8571
Epoch 451/500
1/1 - 0s - loss: 0.2806 - accuracy: 0.8571
Epoch 452/500
1/1 - 0s - loss: 0.2802 - accuracy: 0.8571
Epoch 453/500
1/1 - 0s - loss: 0.2797 - accuracy: 0.8571
Epoch 454/500
1/1 - 0s - loss: 0.2793 - accuracy: 0.8571
Epoch 455/500
1/1 - 0s - loss: 0.2788 - accuracy: 0.8571
Epoch 456/500
1/1 - 0s - loss: 0.2784 - accuracy: 0.8571
Epoch 457/500
1/1 - 0s - loss: 0.2780 - accuracy: 0.8571
Epoch 458/500
1/1 - 0s - loss: 0.2775 - accuracy: 0.8571
Epoch 459/500
1/1 - 0s - loss: 0.2771 - accuracy: 0.8571
Epoch 460/500
1/1 - 0s - loss: 0.2767 - accuracy: 0.8571
Epoch 461/500
1/1 - 0s - loss: 0.2763 - accuracy: 0.8571
Epoch 462/500
1/1 - 0s - loss: 0.2759 - accuracy: 0.8571
Epoch 463/500
1/1 - 0s - loss: 0.2754 - accuracy: 0.8571
Epoch 464/500
1/1 - 0s - loss: 0.2750 - accuracy: 0.8571
Epoch 465/500
1/1 - 0s - loss: 0.2747 - accuracy: 0.8571
Epoch 466/500
1/1 - 0s - loss: 0.2743 - accuracy: 0.8571
Epoch 467/500
1/1 - 0s - loss: 0.2739 - accuracy: 0.8571
Epoch 468/500
1/1 - 0s - loss: 0.2735 - accuracy: 0.8571
Epoch 469/500
1/1 - 0s - loss: 0.2731 - accuracy: 0.8571
Epoch 470/500
1/1 - 0s - loss: 0.2727 - accuracy: 0.8571
Epoch 471/500
1/1 - 0s - loss: 0.2724 - accuracy: 0.8571
Epoch 472/500
1/1 - 0s - loss: 0.2720 - accuracy: 0.8571
Epoch 473/500
1/1 - 0s - loss: 0.2716 - accuracy: 0.8571
Epoch 474/500
1/1 - 0s - loss: 0.2713 - accuracy: 0.8571
Epoch 475/500
1/1 - 0s - loss: 0.2709 - accuracy: 0.8571
Epoch 476/500
1/1 - 0s - loss: 0.2706 - accuracy: 0.8571
Epoch 477/500
1/1 - 0s - loss: 0.2702 - accuracy: 0.8571
Epoch 478/500
1/1 - 0s - loss: 0.2699 - accuracy: 0.8571
Epoch 479/500
1/1 - 0s - loss: 0.2696 - accuracy: 0.8571
Epoch 480/500
1/1 - 0s - loss: 0.2692 - accuracy: 0.8571
Epoch 481/500
1/1 - 0s - loss: 0.2689 - accuracy: 0.8571
Epoch 482/500
1/1 - 0s - loss: 0.2686 - accuracy: 0.8571
Epoch 483/500
1/1 - 0s - loss: 0.2682 - accuracy: 0.8571
Epoch 484/500
1/1 - 0s - loss: 0.2679 - accuracy: 0.8571
Epoch 485/500
1/1 - 0s - loss: 0.2676 - accuracy: 0.8571
Epoch 486/500
1/1 - 0s - loss: 0.2673 - accuracy: 0.8571
Epoch 487/500
1/1 - 0s - loss: 0.2670 - accuracy: 0.8571
Epoch 488/500
1/1 - 0s - loss: 0.2667 - accuracy: 0.8571
Epoch 489/500
1/1 - 0s - loss: 0.2664 - accuracy: 0.8571
Epoch 490/500
1/1 - 0s - loss: 0.2661 - accuracy: 0.8571
Epoch 491/500
1/1 - 0s - loss: 0.2658 - accuracy: 0.8571
Epoch 492/500
1/1 - 0s - loss: 0.2655 - accuracy: 0.8571
Epoch 493/500
1/1 - 0s - loss: 0.2652 - accuracy: 0.8571
Epoch 494/500
1/1 - 0s - loss: 0.2649 - accuracy: 0.8571
Epoch 495/500
1/1 - 0s - loss: 0.2646 - accuracy: 0.8571
Epoch 496/500
1/1 - 0s - loss: 0.2643 - accuracy: 0.8571
Epoch 497/500
1/1 - 0s - loss: 0.2640 - accuracy: 0.8571
Epoch 498/500
1/1 - 0s - loss: 0.2638 - accuracy: 0.8571
Epoch 499/500
1/1 - 0s - loss: 0.2635 - accuracy: 0.8571
Epoch 500/500
1/1 - 0s - loss: 0.2632 - accuracy: 0.8571
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x7ff300c072e8&gt;
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/71d3336deba2135b2510e970845e93e6e3fb3d9c506cdae89203305a3392466b.png" src="../_images/71d3336deba2135b2510e970845e93e6e3fb3d9c506cdae89203305a3392466b.png" />
</div>
</div>
</section>
<section id="text-generation-using-the-model">
<h3>Text Generation Using the Model<a class="headerlink" href="#text-generation-using-the-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>After we trained the bigram-based LM, we can use the model for text generation (e.g., a one-to-many sequence-to-sequence application).</p></li>
<li><p>We can implement a simple text generator: the model always outputs the next-word that has the highest predicted probability value from the neural LM.</p></li>
<li><p>At every time step, the model will use the newly predicted word as the input for another next-word prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a sequence from the model</span>
<span class="k">def</span> <span class="nf">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
    <span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">seed_text</span>
    <span class="c1"># generate a fixed number of words</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="c1"># encode the text as integer</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">in_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        
        <span class="c1"># predict a word in the vocabulary</span>
        <span class="n">yhat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># map predicted word index to word</span>
        <span class="n">out_word</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">:</span>
                <span class="n">out_word</span> <span class="o">=</span> <span class="n">word</span>
                <span class="k">break</span>
        <span class="c1"># append to input</span>
        <span class="n">in_text</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">out_word</span><span class="p">,</span> <span class="n">result</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">out_word</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In the above <code class="docutils literal notranslate"><span class="pre">generate_seq()</span></code>, we use a <strong>greedy search</strong>, which selects the most likely word at each time step in the output sequence.</p></li>
<li><p>While this approach features its efficiency, the quality of the final output sequences may not necessarily be optimal.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s1">&#39;Jill&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jill came tumbling after and jill came tumbling after and jill
</pre></div>
</div>
</div>
</div>
</section>
<section id="sampling-strategies-for-text-generation">
<h3>Sampling Strategies for Text Generation<a class="headerlink" href="#sampling-strategies-for-text-generation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Given a trained language model and a <strong>seed</strong> text chunk, we can generate new text by greedy-search like we’ve seen above.</p></li>
<li><p>But we may sometimes have to add a certain degree of <strong>variation</strong> to the robotic texts for linguistic <strong>creativity</strong>.</p></li>
</ul>
<ul class="simple">
<li><p>Possible alternatives:</p>
<ul>
<li><p>We can re-normalize the predicted probability distributions of all next-words to reduce probability differences between the highest and the lowest. (Please see Ch.8.1 Text Generation with LSTM in Chollet’s Deep Learning with Python. You will need this strategy for the assignment.)</p></li>
<li><p>We can use non-greedy search by keeping the top <em>k</em> probable candidates in the list for next-word prediction. (cf. <strong>Beam Search</strong> below) and determine the tokens by choosing the sequence of the maximum probability.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="beam-search-skipped">
<h2>Beam Search (skipped)<a class="headerlink" href="#beam-search-skipped" title="Permalink to this headline">#</a></h2>
<section id="searching-in-nlp">
<h3>Searching in NLP<a class="headerlink" href="#searching-in-nlp" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In the previous demonstration, when we generate the predicted next word, we adopt a naive approach, i.e., always choosing the word of the highest probability.</p></li>
<li><p>It is common in NLP for models to output a probability distribution over words in the vocabulary.</p></li>
<li><p>This step involves searching through all the possible output sequences based on their likelihood.</p></li>
<li><p>Choosing the next word of highest probability does not guarantee us the most optimal sequence.</p></li>
<li><p>The search problem is exponential in the length of the output sequence given the large size of vocabulary.</p></li>
</ul>
</section>
<section id="beam-search-decoding">
<h3>Beam Search Decoding<a class="headerlink" href="#beam-search-decoding" title="Permalink to this headline">#</a></h3>
<p>The beam search expands all possible next steps and keeps the <strong><span class="math notranslate nohighlight">\(k\)</span></strong> most likely, where <strong><span class="math notranslate nohighlight">\(k\)</span></strong> is a researcher-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.</p>
<p>The search process can stop for each candidate independently either by:</p>
<ul class="simple">
<li><p>reaching a maximum length</p></li>
<li><p>reaching an end-of-sequence token</p></li>
<li><p>reaching a threshold likelihood</p></li>
</ul>
<div class="highlight-{note] notranslate"><div class="highlight"><pre><span></span>Please see Jason Brownlee&#39;s blog post [How to Implement a Beam Search Decoder for Natural Language Processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/) for the python implementation.

The following codes are based on Jason&#39;s code.
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The following codes may not work properly. In Beam Search, when the model predicts <code class="docutils literal notranslate"><span class="pre">None</span></code> as the next character, we should set it as a stopping condition. The following codes have not be optimized with respect to this.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a sequence from the model</span>
<span class="k">def</span> <span class="nf">generate_seq_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">in_text</span> <span class="o">=</span> <span class="n">seed_text</span> 
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[[[</span><span class="n">in_text</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">]]</span>
    <span class="c1"># prepare id_2_word map</span>
    <span class="n">id_2_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">([(</span><span class="n">i</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
    
    <span class="c1"># start next-word generating</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="n">all_candidates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>        
        <span class="c1">#print(&quot;Next word &quot;, _+1)</span>
        <span class="c1"># temp list to hold all possible candidates</span>
        <span class="c1"># `sequence + next words`</span>


        <span class="c1"># for each existing sequence</span>
        <span class="c1"># take the last word of the sequence</span>
        <span class="c1"># find probs of all words in the next position</span>
        <span class="c1"># save the top k</span>
        <span class="c1"># all_candidates should have 3 * 22 = 66 candidates</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)):</span>
            <span class="c1"># for the current sequence</span>
            <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>            
            <span class="c1"># next word probablity distribution</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
            <span class="n">model_pred_prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="c1"># compute all probabilities for `curent_sequence + all_possible_next_word`</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_pred_prob</span><span class="p">)):</span>
                <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">id_2_word</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">score</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_pred_prob</span><span class="p">[</span><span class="n">j</span><span class="p">])]</span>
                <span class="n">all_candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>

            <span class="n">all_candidates</span><span class="o">=</span> <span class="p">[[</span><span class="n">seq</span><span class="p">,</span> <span class="n">score</span><span class="p">]</span> <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">all_candidates</span> <span class="k">if</span> <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

            <span class="c1"># order all candidates (seqence + nextword) by score</span>
            <span class="c1">#print(&quot;all_condidates length:&quot;, len(all_candidates))</span>
            <span class="n">ordered</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_candidates</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># default ascending</span>
            <span class="c1"># select k best</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="n">ordered</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="c1">## choose top k</span>

    <span class="k">return</span> <span class="n">sequences</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_seq_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="s1">&#39;Jill&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;broke&#39;], 5.70021602883935],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;pail&#39;], 9.975788600742817],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;pail&#39;, &#39;water&#39;],
  10.006022842600942],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;came&#39;], 10.572875507175922],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;came&#39;, &#39;after&#39;], 10.62351381778717],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;tumbling&#39;], 10.633405216038227],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;up&#39;], 10.789379127323627],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;up&#39;, &#39;hill&#39;], 10.838647928088903],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;of&#39;], 11.02272368222475],
 [[&#39;Jill&#39;, &#39;tumbling&#39;, &#39;a&#39;, &#39;of&#39;, &#39;fell&#39;, &#39;of&#39;, &#39;fell&#39;], 11.054580122232437]]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Chollet (2017): Ch 8.1</p></li>
<li><p>Check Jason Brownlee’s blog post <a class="reference external" href="https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/">How to Develop Word-Based Neural Language Models in Python with Keras</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="dl-sequence-models-intuition.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Sequence Models Intuition</p>
      </div>
    </a>
    <a class="right-next"
       href="text-vec-embedding.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Word Embeddings</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-of-neural-language-model">Workflow of Neural Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bigram-model">Bigram Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-sequences-and-training-testing-sets">Text-to-Sequences and Training-Testing Sets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-the-next-word">One-hot Representation of the Next-Word</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-rnn-language-model">Define RNN Language Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-using-the-model">Text Generation Using the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-strategies-for-text-generation">Sampling Strategies for Text Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search-skipped">Beam Search (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#searching-in-nlp">Searching in NLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search-decoding">Beam Search Decoding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>