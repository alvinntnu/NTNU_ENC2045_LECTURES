{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing: A Primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Computational Linguistics is a very active subject in linguistics.\n",
    "- Natural Language Processing (NLP): It is a sub-field of computer science that deals with methods to analyze, model, and understand human language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nlp-apps](../images/nlp-apps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Tasks and Language Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![lg-blocks](../images/lg-blocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ambiguity\n",
    "    - I made her duck.\n",
    "    - He is as good as John Doe.\n",
    "    - The trophy doesn't fit into the brown suitcase because it's too [small/large]. What is too [small/large]?\n",
    "- Creativity\n",
    "- Diversity\n",
    "- Common Knowledge (Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{note}\n",
    "Please check the examples from [Winograd Schema Challenge](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html). These examples include sentences that differ in only a few words, but these minor differences often lead to drastic meaning changes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Intelligence, Machine Learning, Deep Learning and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Artificial Intelligence** is a branch of computer science that aims to build systems that can perform tasks that require human intelligence.\n",
    "- **Machine Learning** is a branch of AI that deals with the development of algorithms that can learn to perform tasks automatically based on large number of examples, without requiring handcrafted rules.\n",
    "- **Deep Learning** is a branch of machine learning that is based on the artificial neural network architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Approaches to NLP\n",
    "    - Heuristics-based NLP\n",
    "    - Machine Learning NLP\n",
    "    - Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heuristics-based NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples:\n",
    "   - Dictionary-based sentiment analysis\n",
    "   - WordNet for lexical relations\n",
    "   - Common sense world knowledge (Open Mind Common Sense, Ontology)\n",
    "   - Regular Expressions\n",
    "   - Context-free grammar\n",
    "\n",
    "Strengths:\n",
    "   - Rules based on domain-specific knowledge can efficiently reduce the mistakes that are sometimes very expensive.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Types of machine learning:\n",
    "    - Supervised vs. Unsupervised\n",
    "    - Classification vs. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Three common steps for machine learning\n",
    "    - Extracting features from texts\n",
    "    - Using the feature representation to learn a model\n",
    "    - Evaluating and improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Common methods:\n",
    "   - Naive Bayes\n",
    "   - Logistic Regression\n",
    "   - Support Vector Machine\n",
    "   - Hidden Markov Model\n",
    "   - Conditional Random Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Convolutional Neural Network (CNN)\n",
    "- Sequence Models\n",
    "    - Recurrent Neural Network (RNN)\n",
    "    - Long-Term Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn](../images/s2s-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Strengths of Sequence Models\n",
    "    - It reflects the fact that a sentence in language flows from one direction to another.\n",
    "    - The model can progressively read an input text from one end to another.\n",
    "    - The model have neural units capable of remembering what it has processed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Transformers\n",
    "    - The state-of-the-art model in major NLP tasks\n",
    "    - It models the textual context in a non-sequential manner.\n",
    "    - Given a word in the input, the model looks at all the words around it and represent each word with respect to its context. This is referred to as *self-attention*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{tip}\n",
    "Simply put, now the deep learning NLP capitalizes on the power of **attention**.\n",
    "\n",
    "It is a mechanism that allows the network to focus its attention on some parts of the input when it's producing the output.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Transfer Learning\n",
    "    - It is a technique in AI where the knowledge gained while solving one problem is applied to a different but related problem.\n",
    "    - We can use unsupervised methods to train a transformer-based model for predicting a part of a sentence given the rest of the content. \n",
    "    - This model can encode high-level nuances of the language, which can be applied to other relevant downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning is NOT Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The trend now is to leverage large transformer models and huge datasets for generic NLP tasks like language models, and then adapt these pre-trained models to smaller downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Challenges\n",
    "    - Overfitting on small datasets\n",
    "        - Occam's razor (model complexity vs. model generalizability)\n",
    "        - Data accessibility (few-shot learning)\n",
    "    - Domain adaptation (genres)\n",
    "    - Interpretable models\n",
    "    - Common sense and world knowledge\n",
    "    - High cost of DL\n",
    "        - Data guzzler\n",
    "        - Specialized hardware (GPUs)\n",
    "        - Deployment and maintenance\n",
    "    - On-device deployment\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Deep learning is not always the go-to solution for all industrial NLP applications.\n",
    "- Many deep learning models are not interpretable enough to indicate the sources of empirical gains.\n",
    "- Always try the traditional machine learning methods first with the target tasks to check the baseline performance of the to-be-developed system.\n",
    "- How to incorporate linguistic knowledge into the computational modeling is something we (linguists) can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- Chapter 1 of Practical Natural Language Processing. {cite}`vajjala2020`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
