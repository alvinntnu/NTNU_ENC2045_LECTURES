{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    " # NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A General NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nlp-pipeline](../images/nlp-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varations of the NLP Pipelines\n",
    "\n",
    "- The process may not always be linear.\n",
    "- There are loops in between.\n",
    "- These procedures may depend on specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Acquisition: Heart of ML System\n",
    "\n",
    "- Ideal Setting: We have everything needed.\n",
    "- Labels and Annotations\n",
    "- Very often we are dealing with less-than-idea scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Less-than-ideal Scenarios\n",
    "\n",
    "- Initial datasets with limited annotations/labels\n",
    "- Initial datasets labeled based on regular expressions or heuristics\n",
    "- Public datasets (cf. [Google Dataset Search](https://datasetsearch.research.google.com/))\n",
    "- Scrape data\n",
    "- Product intervention\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "- It is a technique to exploit language properties to create texts that are syntactically similar to the source text data.\n",
    "- Types of strategies:\n",
    "    - synonym replacement\n",
    "    - Related word replacement (based on association metrics)\n",
    "    - Back translation\n",
    "    - Replacing entities\n",
    "    - Adding noise to data (e.g. spelling errors, random words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Extraction and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "- Extracting raw texts from the input data\n",
    "    - HTML\n",
    "    - PDF\n",
    "- Relevant vs. irrelevant information\n",
    "    - non-textual information\n",
    "    - markup\n",
    "    - metadata\n",
    "- Encoding format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Extracting texts from webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ç‚ºé”æœ€ä½³ç€è¦½æ•ˆæœï¼Œå»ºè­°ä½¿ç”¨ Chromeã€Firefox æˆ– Microsoft Edge çš„ç€è¦½å™¨ã€‚', '', 'çˆ†', 'ä¸­åœ‹çªç„¶å®£å¸ƒä»Šæ—¥èµ·æš«åœé€²å£å°ç£é³³æ¢¨ï¼Œæ€èµ·åœ‹å…§æŒºæœè¾²çš„èªè³¼ç†±æ½®ï¼Œåˆæ­¥çµ±è¨ˆï¼Œå…©å¤©å…§ï¼Œåœ‹å…§èªè³¼é‡å·²ç ´äºŒè¬äº”åƒå…¬å™¸ï¼Œé è¨ˆä»Šå¤©å¯é”ä¸‰è¬å…¬å™¸ï¼›ä»¥æ­¤ä¼°è¨ˆï¼Œä¸­åœ‹æ¯€ç´„æ£„è³¼çš„é³³æ¢¨è¨‚å–®å¯æœ›å¾ˆå¿«å°±èƒ½è½‰éŠ·è§£æ±ºã€‚ï¼ˆè³‡æ–™ç…§ï¼Œè¨˜è€…åŠ‰ä¿¡å¾·æ”ï¼‰', 'é¦–æ¬¡ä¸Šç¨¿ 01:00æ›´æ–°æ™‚é–“ 06:08', 'ã€”è¨˜è€…ç¾…ç¶ºã€é»ƒä»¥æ•¬ï¼å°åŒ—å ±å°ã€•ä¸­åœ‹ä¸Šé€±å…­çªç„¶å®£å¸ƒä»Šï¼ˆä¸€æ—¥ï¼‰æ—¥èµ·æš«åœé€²å£å°ç£é³³æ¢¨ï¼Œå¼•ç™¼åœ‹äººè­°è«–åŠä¸æ»¿ï¼Œç›¸å°æ€èµ·åœ‹å…§æŒºå°ç£æœè¾²çš„èªè³¼ç†±æ½®ã€‚é­ä¸­åœ‹ç‰‡é¢æ£„å–®çš„å››è¬å¤šå…¬å™¸é³³æ¢¨ï¼Œè¾²å§”æœƒåŸè¦åŠƒè¦è½‰éŠ·ä»–åœ‹ç´„ä¸‰è¬å™¸ã€åœ‹å…§èªè³¼ç´„å…©è¬å™¸ï¼Œä½†åˆæ­¥çµ±è¨ˆï¼Œå…©å¤©å…§ï¼Œåœ‹å…§èªè³¼é‡å·²ç ´äºŒè¬äº”åƒå…¬å™¸ï¼Œé è¨ˆä»Šå¤©å¯é”ä¸‰è¬å…¬å™¸ï¼›ä»¥æ­¤ä¼°è¨ˆï¼Œä¸­åœ‹æ¯€ç´„æ£„è³¼çš„é³³æ¢¨è¨‚å–®å¯æœ›å¾ˆå¿«å°±èƒ½è½‰éŠ·è§£æ±ºã€‚', 'è«‹ç¹¼çºŒå¾€ä¸‹é–±è®€...', '', 'è¾²å§”æœƒåˆæ­¥çµ±è¨ˆï¼Œå…‰æ˜¯åœ‹å…§èªè³¼ä¼°å·²ç ´å…©è¬äº”åƒå…¬å™¸ï¼ŒåŒ…æ‹¬æœä¹¾ã€æœæ±ç­‰åŠ å·¥æ¥­è€…ï¼Œä¸å°‘åŸæœ¬ç”¨è¶Šå—é³³æ¢¨ï¼Œéƒ½åŒæ„å„ªå…ˆæˆ–æ“´å¤§æ¡è³¼åœ‹å…§é³³æ¢¨ï¼Œä¼°å¯å¢è³¼ä¸€è¬äº”åƒå…¬å™¸ï¼Œå¦æœ‰èŒ¶é£²ã€é¤å»³æ¥­è€…æ“´å¤§æ¡è³¼è‡³å°‘å·²é”äº”åƒå…¬å™¸ã€‚åŠ ä¸Šé€£ç¹«è¾²å§”æœƒè¡¨é”èªè³¼æ„é¡˜çš„ä¼æ¥­å·²ç ´ç™¾å®¶ï¼Œä¾‹å¦‚å…¨è¯æ¡è³¼é‡è¦å¾åŸæœ¬å››åƒå…¬å™¸å¢åˆ°ä¸Šè¬å…¬å™¸ï¼Œé‚„æœ‰è¨±å¤šå¤§ä¼æ¥­èªè³¼åå–®åœ¨æŒçºŒå¢åŠ ï¼Œé è¨ˆé€™å…©å¤©èªè³¼æ•¸é‚„æœƒå¢åŠ ã€‚', 'è¾²å§”æœƒé€™å…©å¤©ä¹Ÿåˆæ­¥è¯ç¹«å¤–éŠ·å•†ï¼Œå·²æœ‰æ¥­è€…æ‰¿è«¾è¦æ“´å¤§å¤–éŠ·æ—¥æœ¬ã€æ–°åŠ å¡ç­‰äº”åƒå¤šå…¬å™¸ï¼Œæ›´æœ‰æµ·å¤–åƒ‘æ°‘ç´›ç´›è¡¨é”è¦èªè³¼ä¸éœ€ç¶“æµ·é—œæª¢ç–«çš„åœ‹å…§é³³æ¢¨åŠ å·¥å“ï¼›å¤–éŠ·ç®¡é“è¼ƒè²»æ™‚ï¼Œä½†é è¨ˆåœ¨ä¸‰ã€å››æœˆé³³æ¢¨ç”¢å­£æ”¶æˆå‰ï¼Œæµ·å¤–è¨‚å–®é™¸çºŒä¹Ÿæœƒå¢åŠ ã€‚', 'è¾²å§”æœƒæ˜¨æ—¥ä¸¦å·²å»ºæ§‹è¨‚è³¼åœ‹ç”¢é³³æ¢¨çš„é€šè·¯å¹³å°ï¼Œæ°‘çœ¾å¯ä¸Šè¾²å§”æœƒç¶²ç«™ç›´æ¥è¨‚è³¼æŒºè¾²æ°‘ã€‚', 'è¾²å§”æœƒä¸»å§”é™³å‰ä»²è¡¨ç¤ºï¼Œå¾ˆæ„Ÿè¬åœ‹äººå°å°ç£é³³æ¢¨åŠæœè¾²çš„ç†±æƒ…ç›¸æŒºï¼›ä»–ä¹Ÿå¼·èª¿ï¼Œè¾²å§”æœƒä¸æœƒè®“å°ç£é³³æ¢¨çš„ç†±è³¼åªæ˜¯çŸ­æœŸç†±æƒ…ï¼Œè€Œå¸Œæœ›è¦è—‰æ­¤æ›´ç©æ¥µæ‹“å±•æ›´å¤šå…ƒåŠæ›´é•·æœŸçš„åœ‹å…§å¤–é³³æ¢¨å¤šå…ƒéŠ·å”®ç®¡é“ï¼Œã€Œé›è›‹ä¸è¦éƒ½æ”¾åœ¨ä¸€å€‹ç±ƒå­è£¡ã€ï¼Œå°ç£è¾²æ¥­ä¹Ÿæ‡‰èˆ‡æ¾³æ´²ã€æ—¥æœ¬ã€æ±å—äºç­‰åœ‹å®¶å»ºç«‹èµ·æ›´å¤§çš„éŠ·å”®ç¶²çµ¡ã€‚', 'è¾²å§”æœƒæ˜¨æ—¥ä¸¦å·²å»ºæ§‹è¨‚è³¼åœ‹ç”¢é³³æ¢¨çš„é€šè·¯å¹³å°ï¼Œåˆæ­¥çµ±è¨ˆï¼Œåˆ°2æœˆ28æ—¥å…‰æ˜¯åœ‹å…§èªè³¼ä¼°å·²ç ´å…©è¬äº”åƒå…¬å™¸ã€‚ï¼ˆåœ–æ“·å–è‡ªè¾²å§”æœƒä¸»å§”é™³å‰ä»²è‡‰æ›¸ï¼‰', '\\n    ä¸ç”¨æŠ½ ä¸ç”¨æ¶ ç¾åœ¨ç”¨APPçœ‹æ–°è ä¿è­‰å¤©å¤©ä¸­ç\\u3000\\n    é»æˆ‘ä¸‹è¼‰APP\\u3000\\n    æŒ‰æˆ‘çœ‹æ´»å‹•è¾¦æ³•\\n', 'ç¨å®¶ã€‹çœŸå·§ï¼ä¸­åœ‹é˜»å°ç£é³³æ¢¨é€²å£ ç•¶å¤©åŠ›æ¨å»£æ±ç”¢é‡‘é‘½é³³æ¢¨', 'ä¸­åœ‹ä¸è¦æˆ‘å€‘è‡ªå·±åƒï¼æ±æ¸¯æµ·ç”¢é¤å»³æƒé€¾å™¸é³³æ¢¨ æ¯æ¡Œçš†ã€Œæ—ºä¾†ã€', 'é…åˆè¾²å§”æœƒä¿ƒéŠ· åœ‹é˜²éƒ¨ï¼šé¼“å‹µåœ‹è»ä¼™é£Ÿå¤šè²·é³³æ¢¨', 'ä¸€éŠ€ã€å°ä¼éŠ€æ¡è³¼å°ç£é³³æ¢¨ åŠ›æŒºæœè¾²è¡å…§éŠ·', 'å…¨æ°‘åƒèµ·ä¾†ï¼é€£å‡éŠå®¢æ¹§è‡³ é—œå»Ÿé³³æ¢¨è²·æ°£å¢6æˆ', 'è¾²å§”æœƒè¦åŠƒé³³æ¢¨å…§å¤–éŠ·ç­–ç•¥ è¾²æ°‘ï¼š3æœˆä¸­æ—¬å°±çŸ¥è¼¸è´', 'æœ‰äººè¸¢éµæ¿ï¼è²·éºµåŒ…é€€è²¨è¢«å¥½å¸‚å¤šæ‹’çµ• åŸå› æ›å…‰', 'ç¨å®¶ã€‹çœŸå·§ï¼ä¸­åœ‹é˜»å°ç£é³³æ¢¨é€²å£ ç•¶å¤©åŠ›æ¨å»£æ±ç”¢é‡‘é‘½é³³æ¢¨', '\\n                    æ·±å¤œç‰¹é›†ã€‹30é‚„å¤ åŠ›ï¼Ÿ40æƒ‘ä¸æƒ‘ï¼Ÿã€Œå¹´é½¡é‡Œç¨‹ç¢‘ã€ä½ ä¿¡å—ï¼Ÿ                                            \\n', '\\n                    ä»Šæ˜å›ç©©é€±äºŒåˆæœ‰å†·ç©ºæ°£ æ°£æº«é©Ÿé™10åº¦                                    ', '\\n                    æ‡·å¿µå´‘æ¿±ä¼¯   è‘‰æµ·ç…™ï¼šçœ‹è¦‹å°ç£äºŸéœ€çš„5ç¨®åŠ›é‡                                    ', '\\n                    ä¸­åœ‹å°æ®ºå°ç£é³³æ¢¨ åŒ—è¾²ï¼šåƒ¹æ ¼ç©©å®šã€å·²æ“¬è¡ŒéŠ·é³³æ¢¨æ–¹æ¡ˆ                                    ', ' 2021å¹´3æœˆ1æ—¥â€§æ˜ŸæœŸä¸€â€§è¾›ä¸‘å¹´æ­£æœˆåå…«æ—¥', 'è‡ªç”±æ™‚å ±ç‰ˆæ¬Šæ‰€æœ‰ä¸å¾—è½‰è¼‰Â© 2021 The Liberty Times. All Rights Reserved.', 'ç†±é–€æ¨æ’­']\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "url = 'https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFptTXpJU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant'\n",
    "r = requests.get(url)\n",
    "web_content = r.text\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "title = soup.find_all('a', class_='DY5T1d')\n",
    "first_art_link = title[0]['href'].replace('.','https://news.google.com',1)\n",
    "\n",
    "#print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding='utf8'\n",
    "soup_art = BeautifulSoup(art_request.text,'html.parser')\n",
    "\n",
    "art_content = soup_art.find_all('p')\n",
    "art_texts = [p.text for p in art_content]\n",
    "print(art_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Extracting texts from scanned PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stellenbosch Papers in Linguistics, Vol. 15, 1986, 31-60 doi: 10.5774/15-0-96\n",
      "\n",
      "SPIL 14 (1986) 31- 6Â¢ 31\n",
      "\n",
      "THE LINGUISTIC THOUGHT OF J.R. FIRTH\n",
      "\n",
      "Nigel Love\n",
      "\n",
      "\"The study of the living votce of a\n",
      "man tn aectton ts a very btg job in-\n",
      "\n",
      "ii\n",
      "deed.\" --- J.R. Firth\n",
      "\n",
      "John Rupert Firth was born in 1890. After serving as Pro-\n",
      "fessor of English at the University of the Punjab from 1919\n",
      "to 1928, he took up a pest in the phonetics department of\n",
      "University College, London. In 1938 he moved to the lin-\n",
      "guistics department of the School of Oriental and African\n",
      "Studies in London, where from 1944 until his retirement in\n",
      "1956 he was Professor of Generali Linguistics. He died in\n",
      "1960. He was an influential teacher, some of whose doctrines\n",
      "(especially those concerning phonology) were widely propa-~\n",
      "gated and developed by his students in what came to be known\n",
      "\n",
      "as the \"London schoolâ€ of linguistics.\n",
      "\n",
      "\"The business of linguistics\", according to Firth, \"is to\n",
      "\n",
      "1}\n",
      "\n",
      "describe languages\". In saying as much he would have the\n",
      "assent of most twentieth-century linguistic theorists.\n",
      "\n",
      "Where he parts company with many is in holding that this\n",
      "enterprise is not incompatible with, or even separable from,\n",
      "studying â€œthe living voice of a man in action\"; and his\n",
      "chief interest as a linguistic thinker lies in his attempt\n",
      "to resist the idea that synchronic descriptive linguistics\n",
      "should treat what he calis â€œspeech-events\" as no more than\n",
      "a means of access to what really interests the linguist:\n",
      "\n",
      "the Language-system underlying them.\n",
      "\n",
      "Languages, according to many theorists, are to be envisaged\n",
      "as systems of abstract entities. These entities are units\n",
      "\n",
      "of linguistic â€œform. Units of linguistic form are of two\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    " \n",
    "YOUR_DEMO_DATA_PATH = \"../../../RepositoryData/data/\"  # please change your file path\n",
    "filename = YOUR_DEMO_DATA_PATH+'pdf-firth-text.png'\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Unicode normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel really ğŸ˜¡. GOGOGO!! ğŸ’ªğŸ’ªğŸ’ª  ğŸ¤£ğŸ¤£ È€Ã†ÄÇ¦Æ“\n",
      "b'I feel really \\xf0\\x9f\\x98\\xa1. GOGOGO!! \\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa  \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f\\xa4\\xa3 \\xc8\\x80\\xc3\\x86\\xc4\\x8e\\xc7\\xa6\\xc6\\x93'\n"
     ]
    }
   ],
   "source": [
    "text = 'I feel really ğŸ˜¡. GOGOGO!! ğŸ’ªğŸ’ªğŸ’ª  ğŸ¤£ğŸ¤£ È€Ã†ÄÇ¦Æ“'\n",
    "print(text)\n",
    "text2 = text.encode('utf-8')\n",
    "print(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel really . GOGOGO!!    ADG'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Other useful libraries\n",
    "    - Spelling check: pyenchant, Microsoft REST API\n",
    "    - PDF:  PyPDF, PDFMiner\n",
    "    - OCR: pytesseract\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cleanup\n",
    "\n",
    "- Preliminaries\n",
    "    - Sentence segmentation\n",
    "    - Word tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Segmentation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is an interpreted, high-level and general-purpose programming language.\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.']\n",
      "Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
      "['Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.']\n",
      "Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
      "['Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Frequent preprocessing\n",
    "    - Stopword removal\n",
    "    - Stemming and/or lemmatization\n",
    "    - Digits/Punctuaions removal\n",
    "    - Case normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Removing stopwords, punctuations, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"O'Neil\", 'works', 'at', 'Wonderland', ',', 'located', 'at', '245', 'Goleta', 'Avenue', ',', 'CA.', ',', '74208', '.']\n",
      "Mr.\n",
      "John\n",
      "O'Neil\n",
      "works\n",
      "Wonderland\n",
      "located\n",
      "Goleta\n",
      "Avenue\n",
      "CA.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'revolut', 'better']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "revolution\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Task-specific preprocessing\n",
    "    - Unicode normalization\n",
    "    - language detection\n",
    "    - code mixing\n",
    "    - transliteration\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Automatic annotations\n",
    "    - POS tagging\n",
    "    - Parsing\n",
    "    - Named entity recognition\n",
    "    - coreference resolution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Important Reminders for Preprocessing\n",
    "\n",
    "- Not all steps are necessary\n",
    "- These steps are NOT sequential\n",
    "- These steps are task-dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is feature engineering?\n",
    "\n",
    "- It refers to a process to feed the extracted and preprocessed texts into a machine-learning algorithm.\n",
    "- It aims at capturing the characteristics of the text into a numeric vector that can be understood by the ML algorithms. (Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for Classical ML\n",
    "\n",
    "- word-based frequency lists\n",
    "- bag-of-words representations\n",
    "- domain-specific word frequency lists\n",
    "- handcrafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "- The DL model is capable of learning features from the texts (e.g., embeddings)\n",
    "- Less interpretable.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Simple to Complex\n",
    "\n",
    "- Start with heuristics or rules\n",
    "- Experiment with different ML models\n",
    "    - from heuristics to features\n",
    "    - from manual annotation to automatic extraction\n",
    "    - feature importance (weights)\n",
    "- Find the most optimal model\n",
    "    - Ensemble and stacking\n",
    "    - Redo feature engineering\n",
    "    - Transfer learning\n",
    "    - Reapply heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why evaluation?\n",
    "\n",
    "- We need to know how *good* the model we've built is -- \"Goodness\"\n",
    "- Factors relating to the evaluation methods\n",
    "    - model building\n",
    "    - deployment\n",
    "    - production\n",
    "- ML metrics vs. business metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intrinsic vs. Extrinsic Evaluation\n",
    "\n",
    "- Take spam-classification system as an example\n",
    "- Intrinsic:\n",
    "    - the precision and recall of the spam classification/prediction\n",
    "- Extrinsic:\n",
    "    - the amount of time users spent on a spam email\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General Principles\n",
    "\n",
    "- Do intrinsic evaluation before extrinsic.\n",
    "- Extrinsic evaluation is more expensive because it often invovles project stakeholders outside the AI team.\n",
    "- Only when we get consistently good results in intrinsic evaluation should we go for extrinsic evaluation.\n",
    "- Bad results in intrinsic often implies bad results in extrinsic as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Intrinsic Metrics\n",
    "\n",
    "- Principles for Evaluation Metrics Selection\n",
    "- Data type of the labels (ground truths)\n",
    "    - Binary (e.g., sentiment)\n",
    "    - Ordinal (e.g., informational retrieval)\n",
    "    - Categorical (e.g., POS tags)\n",
    "    - Textual (e.g., named entity, machine translation, text generation)\n",
    "- Automatic vs. Human Evalation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-Modeling Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Post-Modeling Phases\n",
    "\n",
    "- Deployment of the model in a  production environment (e.g., web service)\n",
    "- Monitoring system performance on a regular basis\n",
    "- Updating system with new-coming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Chapter 2 of Practical Natural Language Processing. {cite}`vajjala2020`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{bibliography} ../book.bib\n",
    ":filter: docname in docnames\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
