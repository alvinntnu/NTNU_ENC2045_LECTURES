

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Natural Language Processing: A Primer &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/nlp-primer';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NLP Pipeline" href="nlp-pipeline.html" />
    <link rel="prev" title="ENC2045 Computational Linguistics" href="../intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-neural-network-from-scratch.html">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">Sequence Model with Attention for Addition Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/nlp-primer.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/nlp-primer.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Natural Language Processing: A Primer</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-applications">NLP Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-tasks-and-language-blocks">NLP Tasks and Language Blocks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-challenges">NLP Challenges</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence-machine-learning-deep-learning-and-nlp">Artificial Intelligence, Machine Learning, Deep Learning and NLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approaches-to-nlp">Approaches to NLP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristics-based-nlp"><strong>Heuristics-based NLP</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-nlp"><strong>Machine Learning NLP</strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-for-nlp"><strong>Deep Learning for NLP</strong>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-is-not-everything">Deep Learning is NOT Everything</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="natural-language-processing-a-primer">
<h1><a class="toc-backref" href="#id2">Natural Language Processing: A Primer</a><a class="headerlink" href="#natural-language-processing-a-primer" title="Permalink to this headline">#</a></h1>
<p>This section provides a quick overview of natural language processing, or, more broadly speaking, computational linguistics. This is a very active field in applied linguistics because it is closely connected to the development of human language technology. The term <strong>natural language processing</strong> is a more specific term referring to the sub-field  of computer science that deals with methods to <strong>analyze</strong>, <strong>model</strong>, and <strong>understand</strong> human language.</p>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#natural-language-processing-a-primer" id="id2">Natural Language Processing: A Primer</a></p>
<ul>
<li><p><a class="reference internal" href="#nlp-applications" id="id3">NLP Applications</a></p></li>
<li><p><a class="reference internal" href="#nlp-tasks-and-language-blocks" id="id4">NLP Tasks and Language Blocks</a></p></li>
<li><p><a class="reference internal" href="#nlp-challenges" id="id5">NLP Challenges</a></p></li>
<li><p><a class="reference internal" href="#artificial-intelligence-machine-learning-deep-learning-and-nlp" id="id6">Artificial Intelligence, Machine Learning, Deep Learning and NLP</a></p></li>
<li><p><a class="reference internal" href="#approaches-to-nlp" id="id7">Approaches to NLP</a></p>
<ul>
<li><p><a class="reference internal" href="#heuristics-based-nlp" id="id8"><strong>Heuristics-based NLP</strong></a></p></li>
<li><p><a class="reference internal" href="#machine-learning-nlp" id="id9"><strong>Machine Learning NLP</strong>:</a></p></li>
<li><p><a class="reference internal" href="#deep-learning-for-nlp" id="id10"><strong>Deep Learning for NLP</strong>:</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#deep-learning-is-not-everything" id="id11">Deep Learning is NOT Everything</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id12">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#references" id="id13">References</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="nlp-applications">
<h2><a class="toc-backref" href="#id3">NLP Applications</a><a class="headerlink" href="#nlp-applications" title="Permalink to this headline">#</a></h2>
<p><img alt="nlp-apps" src="../_images/nlp-apps.png" /></p>
</section>
<section id="nlp-tasks-and-language-blocks">
<h2><a class="toc-backref" href="#id4">NLP Tasks and Language Blocks</a><a class="headerlink" href="#nlp-tasks-and-language-blocks" title="Permalink to this headline">#</a></h2>
<p><img alt="lg-blocks" src="../_images/lg-blocks.png" /></p>
</section>
<section id="nlp-challenges">
<h2><a class="toc-backref" href="#id5">NLP Challenges</a><a class="headerlink" href="#nlp-challenges" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Ambiguity: Natural language is inherently ambiguous, with words and phrases often having multiple meanings or interpretations depending on context.</p>
<ul>
<li><p>I made her duck.</p></li>
<li><p>He is as good as John Doe.</p></li>
<li><p>The trophy doesn’t fit into the brown suitcase because it’s too [small/large]. What is too [small/large]?</p></li>
</ul>
</li>
<li><p>Creativity: Human language is highly creative, allowing for infinite possibilities in expression and interpretation. This creativity poses challenges for NLP systems in generating and understanding novel or unconventional language patterns.</p></li>
<li><p>Diversity: Language exhibits significant diversity across different contexts, cultures, dialects, and individual speakers. NLP systems trained on specific datasets may encounter difficulties when faced with linguistic variations that differ from the training data.</p></li>
<li><p>Common Knowledge (Context): Understanding language often requires background knowledge and context that may not be explicitly stated in the text. NLP systems must be able to incorporate common knowledge and contextual information to accurately interpret and generate language. However, capturing and representing common knowledge in machine-readable formats poses a significant challenge for NLP, as it often relies on world knowledge and domain-specific information that may not be readily available in structured data sources.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please check the examples from <a class="reference external" href="https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html">Winograd Schema Challenge</a>. These examples include sentences that differ in only a few words, but these minor differences often lead to drastic meaning changes.</p>
</div>
</section>
<section id="artificial-intelligence-machine-learning-deep-learning-and-nlp">
<h2><a class="toc-backref" href="#id6">Artificial Intelligence, Machine Learning, Deep Learning and NLP</a><a class="headerlink" href="#artificial-intelligence-machine-learning-deep-learning-and-nlp" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Artificial Intelligence</strong> is a branch of computer science that aims to build systems that can perform tasks that require human intelligence.</p></li>
<li><p><strong>Machine Learning</strong> is a branch of AI that deals with the development of algorithms that can learn to perform tasks automatically based on large number of examples, without requiring handcrafted rules.</p></li>
<li><p><strong>Deep Learning</strong> is a branch of machine learning that is based on the artificial neural network architectures.</p></li>
</ul>
</section>
<section id="approaches-to-nlp">
<h2><a class="toc-backref" href="#id7">Approaches to NLP</a><a class="headerlink" href="#approaches-to-nlp" title="Permalink to this headline">#</a></h2>
<section id="heuristics-based-nlp">
<h3><a class="toc-backref" href="#id8"><strong>Heuristics-based NLP</strong></a><a class="headerlink" href="#heuristics-based-nlp" title="Permalink to this headline">#</a></h3>
<p>Heuristics-based NLP approaches rely on predefined rules and patterns to process natural language. These rules are typically crafted by linguists or domain experts based on linguistic principles and knowledge. Heuristic methods involve designing algorithms that encode linguistic rules to perform tasks such as part-of-speech tagging, named entity recognition, and syntactic parsing. While heuristics-based NLP can be effective for handling specific linguistic phenomena and tasks, it often requires extensive manual effort to design and maintain the rules, and may struggle with handling the complexity and variability of natural language.</p>
<p>Examples:</p>
<ul class="simple">
<li><p>Dictionary-based sentiment analysis</p></li>
<li><p>WordNet for lexical relations</p></li>
<li><p>Common sense world knowledge (Open Mind Common Sense, Ontology)</p></li>
<li><p>Regular Expressions</p></li>
<li><p>Context-free grammar</p></li>
</ul>
<p>Strengths:</p>
<ul class="simple">
<li><p>Rules based on domain-specific knowledge can efficiently reduce the mistakes that are sometimes very expensive.</p></li>
</ul>
</section>
<section id="machine-learning-nlp">
<h3><a class="toc-backref" href="#id9"><strong>Machine Learning NLP</strong>:</a><a class="headerlink" href="#machine-learning-nlp" title="Permalink to this headline">#</a></h3>
<p>Machine Learning (ML) NLP approaches leverage statistical models and algorithms to learn patterns and structures from large amounts of annotated text data. These models are trained on labeled datasets to automatically extract features and make predictions for various NLP tasks. Common ML techniques used in NLP include Support Vector Machines (SVM), Hidden Markov Models (HMM), Conditional Random Fields (CRF), and Maximum Entropy Models (MaxEnt). Machine Learning NLP systems excel at tasks such as text classification, sentiment analysis, and machine translation, and can handle more complex linguistic phenomena compared to heuristics-based approaches. However, they require significant amounts of labeled data for training and may struggle with handling ambiguity and linguistic variability.</p>
<ul class="simple">
<li><p>Types of machine learning:</p>
<ul>
<li><p>Supervised vs. Unsupervised</p></li>
<li><p>Classification vs. Regression</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Three common steps for machine learning</p>
<ul>
<li><p>Extracting features from texts</p></li>
<li><p>Using the feature representation to train a model</p></li>
<li><p>Evaluating and refining the model</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Common methods:</p>
<ul>
<li><p>Naive Bayes</p></li>
<li><p>Logistic Regression</p></li>
<li><p>Support Vector Machine</p></li>
<li><p>Hidden Markov Model</p></li>
<li><p>Conditional Random Field</p></li>
</ul>
</li>
</ul>
</section>
<section id="deep-learning-for-nlp">
<h3><a class="toc-backref" href="#id10"><strong>Deep Learning for NLP</strong>:</a><a class="headerlink" href="#deep-learning-for-nlp" title="Permalink to this headline">#</a></h3>
<p>Deep Learning for NLP involves using neural network architectures, particularly deep neural networks with multiple layers, to model and process natural language data. Deep Learning models, such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer models, have shown remarkable success in various NLP tasks due to their ability to learn hierarchical representations of text data. These models can automatically learn intricate patterns and dependencies in the data, making them highly effective for tasks such as language modeling, sequence-to-sequence generation, and contextual word embeddings. Deep Learning for NLP has led to significant advancements in areas such as machine translation, text summarization, and question answering, and continues to be an active area of research in the field.</p>
<ul class="simple">
<li><p>Common methods:</p>
<ul>
<li><p>Convolutional Neural Network (CNN)</p></li>
<li><p>Sequence Models</p>
<ul>
<li><p>Recurrent Neural Network (RNN)</p></li>
<li><p>Long-Term Short-Term Memory (LSTM)</p></li>
</ul>
</li>
<li><p>Attention and Transformers</p></li>
</ul>
</li>
</ul>
<p><img alt="rnn" src="../_images/s2s-rnn.jpeg" /></p>
<ul class="simple">
<li><p>Strengths of Sequence Models</p>
<ul>
<li><p>It reflects the fact that a sentence in language flows from one direction to another.</p></li>
<li><p>The model can progressively read an input text from one end to another.</p></li>
<li><p>The model have neural units capable of remembering what it has processed so far.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Transformers</p>
<ul>
<li><p>The state-of-the-art model in major NLP tasks.</p></li>
<li><p>These models excel at capturing long-range dependencies and have become the backbone of many state-of-the-art NLP systems.</p></li>
<li><p>It leverages the <strong>attention</strong> mechansim to model the textual context in a non-sequential manner, capturing long-range dependencies and contextual information within a sequence.</p></li>
</ul>
</li>
</ul>
<p><img alt="self-atten" src="../_images/seq2seq-self-atten.gif" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Simply put, now the deep learning NLP capitalizes on the power of <strong>attention</strong>. It is a mechanism that allows the model to learn which parts of the input sequence (i.e., the contextual information) are more relevant to the target task, giving more weight to important words or phrases.</p>
<p>Imagine you’re reading a long story. Sometimes, certain parts of the story are more important than others. The attention mechanism in deep learning works like a spotlight, highlighting these important parts as the story unfolds. It helps the model focus on the most relevant information at each step of processing, just like how you might pay attention to key details as you read.</p>
<p>Traditional NLP approaches typically treat language as a fixed sequence of words and process it in a linear fashion. They often process entire sequences in one go, which can be computationally challenging for long texts. However, with attention, the model can selectively focus on different parts of the sequence, giving more weight to important words or phrases. This makes the model more accurate and efficient, especially when dealing with complex language tasks like translation or summarization.</p>
</div>
<ul class="simple">
<li><p>Transfer Learning</p>
<ul>
<li><p>Transfer learning is a machine learning technique where a model pre-trained on one task or dataset is reused as a starting point for training a model on a different but related task or dataset.</p></li>
<li><p>This reuse is often done through fine-tuning of the pre-trained models.</p></li>
<li><p>In Deep Learning NLP, transfer learning is important because it allows leveraging pre-trained models to achieve good performance with limited task-specific data.</p></li>
<li><p>By adapting pre-trained models to specific domains or tasks, transfer learning reduces training time and improves performance on task-specific objectives.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="deep-learning-is-not-everything">
<h2><a class="toc-backref" href="#id11">Deep Learning is NOT Everything</a><a class="headerlink" href="#deep-learning-is-not-everything" title="Permalink to this headline">#</a></h2>
<p>The trend now is to leverage large transformer models (large language models) for generic NLP tasks. Subsequently, these pre-trained models are customized and fine-tuned to address smaller, more specific downstream tasks.</p>
<ul class="simple">
<li><p><strong>Overfitting on Small Datasets</strong>:</p>
<ul>
<li><p><em>Occam’s Razor</em>: Large language models may suffer from overfitting when trained on small datasets, as they have a high capacity to memorize the training data, leading to poor generalization to unseen data. Balancing model complexity with generalizability, following Occam’s razor principle, becomes crucial.</p></li>
<li><p><em>Data Accessibility</em>: Access to large, diverse datasets is limited, especially for specialized domains. Few-shot learning techniques aim to address this challenge by enabling models to learn from a small amount of annotated data.</p></li>
</ul>
</li>
<li><p><strong>Domain Adaptation (Genres)</strong>:</p>
<ul>
<li><p>Adapting pre-trained models to specific domains or genres remains a challenge. Domain-specific linguistic variations, styles, and terminologies require fine-tuning or retraining of the pre-trained models to achieve optimal performance across different domains.</p></li>
</ul>
</li>
<li><p><strong>Interpretable Models</strong>:</p>
<ul>
<li><p>Understanding and interpreting the decisions made by large language models pose significant challenges. The complex, non-linear nature of the deep-learning architectures makes it difficult to extract meaningful insights or explanations for model predictions, hindering model interpretability and trustworthiness.</p></li>
</ul>
</li>
<li><p><strong>Common Sense and World Knowledge</strong>:</p>
<ul>
<li><p>Incorporating common sense reasoning and world knowledge into large language models remains an ongoing challenge. While models can learn from vast amounts of text data, it is still not clear how the model can capture and encode implicit knowledge and understanding of the world, affecting the quality of generated text and responses (e.g., hullucinations, biases).</p></li>
</ul>
</li>
<li><p><strong>High Cost of Deep Learning</strong>:</p>
<ul>
<li><p>Deep Learning requires high computational costs, both in terms of data requirements and specialized hardware such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs). Large language models are data-intensive and require extensive computational resources for training, making them inaccessible to smaller research teams or organizations with limited resources.</p></li>
<li><p>Deployment and maintenance of deep learning models add to the overall cost, as it involves infrastructure setup, model optimization, and ongoing monitoring and updates.</p></li>
</ul>
</li>
<li><p><strong>On-device Deployment</strong>:</p>
<ul>
<li><p>Deploying large language models on edge devices or resource-constrained environments presents challenges in terms of model size, memory requirements, and computational efficiency. Optimizing models for on-device deployment while maintaining performance and accuracy is an area of active research and development.</p></li>
</ul>
</li>
</ul>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id12">Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<p>The future of AI in natural language processing (NLP) is evolving, and several key points shape its trajectory:</p>
<ol class="arabic simple">
<li><p><strong>Diversification of Approaches</strong>:</p>
<ul class="simple">
<li><p>Deep learning, while powerful, may not always be the optimal solution for industrial NLP applications. Other traditional machine learning methods can offer competitive performance, particularly in scenarios with limited data or where interpretability is critical (e.g., linguistic/language studies).</p></li>
</ul>
</li>
<li><p><strong>Interpretability Challenges</strong>:</p>
<ul class="simple">
<li><p>Many deep learning models lack interpretability, hindering our ability to understand the sources of empirical gains. Addressing this challenge is crucial for building trust in AI systems and ensuring accountability in decision-making processes.</p></li>
</ul>
</li>
<li><p><strong>Baseline Performance Evaluation</strong>:</p>
<ul class="simple">
<li><p>It’s essential to start by evaluating traditional machine learning methods on target tasks to establish baseline performance. This approach provides valuable insights into the strengths and limitations of different techniques and helps guide the development of more sophisticated AI systems.</p></li>
</ul>
</li>
<li><p><strong>Incorporating Linguistic Knowledge</strong>:</p>
<ul class="simple">
<li><p>Linguists play a crucial role in incorporating linguistic knowledge into computational modeling. By leveraging linguistic principles and theories, such as syntax, semantics, and pragmatics, linguists can enhance the effectiveness and interpretability of AI systems. This collaboration between linguists and AI researchers leads to more robust and linguistically informed NLP models.</p></li>
</ul>
</li>
</ol>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id13">References</a><a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Chapter 1 of Practical Natural Language Processing. <span id="id1">[<a class="reference internal" href="../intro.html#id14" title="Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana. Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems. O'Reilly Media, 2020.">Vajjala <em>et al.</em>, 2020</a>]</span></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">ENC2045 Computational Linguistics</p>
      </div>
    </a>
    <a class="right-next"
       href="nlp-pipeline.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">NLP Pipeline</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-applications">NLP Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-tasks-and-language-blocks">NLP Tasks and Language Blocks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-challenges">NLP Challenges</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence-machine-learning-deep-learning-and-nlp">Artificial Intelligence, Machine Learning, Deep Learning and NLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approaches-to-nlp">Approaches to NLP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristics-based-nlp"><strong>Heuristics-based NLP</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-nlp"><strong>Machine Learning NLP</strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-for-nlp"><strong>Deep Learning for NLP</strong>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-is-not-everything">Deep Learning is NOT Everything</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>