
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network From Scratch &#8212; ENC2045 Computational Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Learning: A Simple Example" href="dl-simple-case.html" />
    <link rel="prev" title="2. Topic Modeling: A Naive Example" href="topic-modeling-naive.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">ENC2045 Computational Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  INTRODUCTION
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="nlp-primer.html">
   Natural Language Processing: A Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nlp-pipeline.html">
   NLP Pipeline
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="text-preprocessing.html">
   Text Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="text-normalization-eng.html">
     Text Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-tokenization.html">
     Text Tokenization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-enrichment.html">
     Text Enrichment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="chinese-word-seg.html">
     Chinese Word Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="google-colab.html">
     Google Colab
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Vectorization
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="text-vec-traditional.html">
   Text Vectorization Using Traditional Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning Basics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ml-overview.html">
   1. Machine Learning: Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml-simple-case.html">
   2. Machine Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml-algorithm.html">
   3. Classification Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine-Learning NLP
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ml-sklearn-classification.html">
   1. Sentiment Analysis Using Bag-of-Words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic-modeling-naive.html">
   2. Topic Modeling: A Naive Example
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning NLP
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Network From Scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-simple-case.html">
   Deep Learning: A Simple Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl-sentiment-case.html">
   Deep Learning: Sentiment Analysis
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Embeddings and Language Model
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/text-vec-embedding.html">
   Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/text-vec-embedding-keras.html">
   Word Embedding Using Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-statistical-language-model.html">
   Statistical Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-sequence-models-intuition.html">
   Sequence Models Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-neural-language-model-primer.html">
   Neural Language Model: A Start
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Seq2Seq, Attention, Transformers, and Transfer Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-seq-to-seq-types.html">
   Attention: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-seq-to-seq-attention-addition.html">
   Seqeunce Model with Attention for Addition Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-transformers-intuition.html">
   Transformers: Intuition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../temp/dl-transformers-keras.html">
   Text Classification with Transformer
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1-python-basics.html">
   Assignment I: Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2-journal-review.html">
   Assignment II: Journal Articles Review
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/3-preprocessing.html">
   Assignment III: Preprocessing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-3-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-3-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/4-chinese-nlp.html">
   Assignment IV: Chinese Language Processing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-4-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-4-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../exercise/5-text-vectorization.html">
   Assignment V: Text Vectorization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-5-1.html">
     Student Sample 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../exercise-student/Assignment-5-2.html">
     Student Sample 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/6-machine-learning.html">
   Assignment VI: Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/midterm-exam.html">
   Midterm Exam
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/7-topic-modeling.html">
   Assignment VII: Topic Modeling
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:center">
<i class="fas fa-chalkboard-teacher fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinntnu.github.io/NTNU_ENC2045/" target='_blank'>ENC2045 Course Website</a><br>
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nlp/dl-neural-network-from-scratch.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/NTNU_ENC2045_LECTURES/main?urlpath=tree/nlp/dl-neural-network-from-scratch.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-neural-network-from-scratch.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflow-of-neural-network">
   Workflow of Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-overview">
   Neural Network Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-learning">
     Deep Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-propagation">
     Forward Propagation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backward-propagation">
     Backward Propagation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neurons">
     Neurons
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-functions">
     Activation Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-functions-in-python">
     Activation Functions in Python
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-nodes-to-layers">
     From Nodes to Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-parameters-and-matrix-mutiplication">
     Layer, Parameters, and Matrix Mutiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-neural-networks">
     Types of Neural Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-neural-network-model-in-python">
   Building a Neural Network Model in Python
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#packages-in-focus">
     Packages in Focus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-model">
     Create Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#useful-modules-in-keras">
     Useful Modules in keras
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Useful Modules in keras
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#keras-utils">
       <code class="docutils literal notranslate">
        <span class="pre">
         keras.utils
        </span>
       </code>
       :
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#keras-layers">
       <code class="docutils literal notranslate">
        <span class="pre">
         keras.layers
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-modules">
       Other modules
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-quick-example-of-model-training">
   A Quick Example of Model Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data">
     Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-training">
     Model Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-and-prediction">
     Evaluation and Prediction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-and-training">
   Learning and Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-does-the-neural-network-learn-the-parameters">
     How does the neural network learn the parameters?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions">
     Loss Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principles-for-loss-functions">
     Principles for Loss Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#categorical-variables-encoding">
   Categorical Variables Encoding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-s-wrong-with-categorical-variables">
     What’s wrong with categorical variables?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#three-ways-of-encodings">
     Three Ways of Encodings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#steps">
     Steps
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-encoding-and-loss-function">
     One-hot Encoding and Loss Function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples-of-loss-errors-skipped">
   Examples of Loss Errors (skipped)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-based-on-one-sample">
     Error based on One Sample
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#errors-based-on-batch-samples">
     Errors based on Batch Samples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-adjust-the-model-parameters">
     How do we adjust the model parameters?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients">
     Gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition-of-gradients-an-example-skipped">
     Intuition of Gradients: An Example (skipped)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-gradient-descent">
     Types of Gradient Descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-in-python-skipped">
     Gradients in Python (skipped)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overfitting-and-underfitting">
   Overfitting and Underfitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-vs-generalization">
     Optimization vs. Generalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting-underfitting">
     Overfitting/Underfitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-deal-with-overfitting">
     How to Deal With Overfitting?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization">
     Regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heuristics-for-regularization">
     Heuristics for Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-math-skipped">
   Some Math (Skipped)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chain-rule-and-back-propagation">
     Chain Rule and Back Propagation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elementwise-operations-of-matrix">
     Elementwise Operations of Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#broadcast">
     Broadcast
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrices-dot-production">
     Matrices Dot Production
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrices-dot-production-and-forward-propagation">
     Matrices Dot Production and Forward Propagation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivatives">
     Derivatives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivatives">
     Partial Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-network-from-scratch">
<h1>Neural Network From Scratch<a class="headerlink" href="#neural-network-from-scratch" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>This lecture note introduces fundamentals of the mechanism for neural network, or deep learning.</p></li>
</ul>
<ul class="simple">
<li><p>We will break our discussions into three main parts:</p>
<ul>
<li><p>Building A Neural Network (How does the network work?)</p>
<ul>
<li><p>Forward Propagation</p></li>
<li><p>Weights, Biases, and Activation functions</p></li>
<li><p>Matrix multiplication</p></li>
</ul>
</li>
<li><p>Learning and Training (How does it learn?)</p>
<ul>
<li><p>Loss Function</p></li>
<li><p>Gradients</p></li>
<li><p>Derivatives and Partial Derivatives</p></li>
<li><p>Gradient Descent</p></li>
</ul>
</li>
<li><p>Gradient Descent (More on how does it learn.)</p>
<ul>
<li><p>Batch</p></li>
<li><p>Mini-batch</p></li>
<li><p>Stochastic gradient descent</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Loading Dependencies</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="workflow-of-neural-network">
<h2>Workflow of Neural Network<a class="headerlink" href="#workflow-of-neural-network" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/nn-flowchart.png" /></p>
</div>
<div class="section" id="neural-network-overview">
<h2>Neural Network Overview<a class="headerlink" href="#neural-network-overview" title="Permalink to this headline">¶</a></h2>
<div class="section" id="deep-learning">
<h3>Deep Learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/neural-network-propagation.gif" /></p>
</div>
<div class="section" id="forward-propagation">
<h3>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Neural network is a type of machine learning algorithm modeled on human brains and nervous system.</p></li>
<li><p>The model is believed to process information in a similar way to the human brain:</p>
<ul>
<li><p>inputs and weights as the <strong>dendrites</strong></p></li>
<li><p>neuron operations of weighted sums and activation as <strong>neurons</strong></p></li>
<li><p>outputs as <strong>axons</strong></p></li>
</ul>
</li>
<li><p>A neural network often consists of a large number of elements, known as <strong>nodes</strong>, working in parallel to solve a specific problem. These nodes are often organized into different <strong>layers</strong>.</p></li>
<li><p>Each layer of the network transforms the input values into the output values based on the weights (parameters) of the nodes.</p></li>
<li><p>The data transformation from the input to the output is in general referred to as <strong>forward propagation</strong> of the network.</p></li>
</ul>
</div>
<div class="section" id="backward-propagation">
<h3>Backward Propagation<a class="headerlink" href="#backward-propagation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>When the predicted output is compared with the true label, we can evaluate the network performance by computing the <strong>loss</strong> of the network.</p></li>
<li><p>Then we determine the proportion of the losses that may be attributed to each model parameter. This process goes from the losses of the predicted output backward to the original inputs. This step is referred to as the <strong>back propagation</strong> of the network.</p></li>
</ul>
</div>
<div class="section" id="neurons">
<h3>Neurons<a class="headerlink" href="#neurons" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Neural network consists of neurons, which allow us to model non-linear relationships between input and output data.</p></li>
<li><p>Given an input vector, traditional linear transformation can only model a linear relationship between X and y:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\hat{y} = w_0 + w_1 x_1 + w_2x_2 + w_3x_3 +...+w_nx_n
\]</div>
<ul class="simple">
<li><p>A neron is like a linear transformation but with an extra <strong>activation function</strong>.</p></li>
<li><p>This mechanism of activation function in each neuron will ultimately determine the output of the neuron.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\textit{Neuron Output Value} &amp; =  h(\hat{y}) \\
&amp; = h(w_0 + w_1 x_1 + w_2x_2 + w_3x_3 +...+w_nx_n)
\end{align}\end{split}\]</div>
<p><img alt="" src="../_images/neuron.png" /></p>
</div>
<div class="section" id="activation-functions">
<h3>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In neural network, the activation function of a node determines whether the node would activate the output given the <strong>weighted sum of the input values</strong>.</p></li>
<li><p>Different types of activation functions may determine the cut-offs for output activation in different ways.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Sigmoid</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into values within the range of 0 and 1 (i.e., a probability-like value).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ h(y) = \frac{1}{1 + \exp(-y)}\]</div>
<ul class="simple">
<li><p><strong>Step</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into binary ones, with only the positive values activated.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(y)= \left\{ 
     \begin{array}\\
     0 &amp; (y \leq 0) \\
     1 &amp; (y &gt; 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p><strong>ReLU</strong> (Rectified Linear Unit) function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values by passing only positive values and zero for negative <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(y)= \left\{ 
     \begin{array}\\
     y &amp; (y &gt; 0) \\
     0 &amp; (y \leq 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Softmax</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into normalized probability values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h(y_k) = \frac{\exp(y_k)}{\sum_{i = 1}^{n} \exp({y_i})}
\]</div>
</div>
<div class="section" id="activation-functions-in-python">
<h3>Activation Functions in Python<a class="headerlink" href="#activation-functions-in-python" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="c1"># def softmax(x):</span>
<span class="c1">#     exp_x = np.exp(x)</span>
<span class="c1">#     sum_exp_x = np.sum(exp_x)</span>
<span class="c1">#     y = exp_x/sum_exp_x</span>
<span class="c1">#     return y</span>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>  <span class="c1"># avoid overflow issues</span>
    <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">sum_exp_x</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_20_0.png" src="../_images/dl-neural-network-from-scratch_20_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## sigmoid function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_21_0.png" src="../_images/dl-neural-network-from-scratch_21_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ReLU</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_22_0.png" src="../_images/dl-neural-network-from-scratch_22_0.png" />
</div>
</div>
</div>
<div class="section" id="from-nodes-to-layers">
<h3>From Nodes to Layers<a class="headerlink" href="#from-nodes-to-layers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A neural network can be defined in terms of <strong>depths</strong> and <strong>widths</strong> of its layers.</p>
<ul>
<li><p><strong>Depth</strong>: How many layers does the network have?</p></li>
<li><p><strong>Width</strong>: How many neurons does each layer have?</p></li>
</ul>
</li>
<li><p>A network can consist of several layers.</p></li>
<li><p>Each layer can have various numbers of neurons.</p></li>
<li><p>For each layer, the shape of the input tensor, the number of its neurons, and the shape of its output are inter-connected. These settings will determine the number of parameters (i.e., <strong>weights</strong>) needed to train.</p></li>
</ul>
<p><img alt="" src="../_images/neural-network-dense-layer.gif" /></p>
</div>
<div class="section" id="layer-parameters-and-matrix-mutiplication">
<h3>Layer, Parameters, and Matrix Mutiplication<a class="headerlink" href="#layer-parameters-and-matrix-mutiplication" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Each layer transforms the input values into the output values based on its layer parameters.</p></li>
<li><p>Mathematically, these values transformation is a matrix multiplication, running in parallel for all nodes of the layer.</p></li>
<li><p>In Deep Learning, the input and output values are represented as a multi-dimensional tensor.</p>
<ul>
<li><p>A 1D tensor is a vector.</p></li>
<li><p>A 2D tensor is a two-dimensional array.</p></li>
<li><p>A 3D tensor is a three-dimensional array.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/neural-network-dense-layer-1.gif" /></p>
<p><img alt="" src="../_images/neural-network-dense-layer-2.gif" /></p>
</div>
<div class="section" id="types-of-neural-networks">
<h3>Types of Neural Networks<a class="headerlink" href="#types-of-neural-networks" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Multi-Layer Perceptron</strong> (Fully Connected Network)</p>
<ul>
<li><p>Input Layer, one or more hidden layers, and output layer.</p></li>
<li><p>A hidden layer consists of neurons (perceptrons) which process certain aspect of the features and send the processed information into the next hidden layer.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Convolutional Neural Network (CNN)</strong></p>
<ul>
<li><p>Mainly for image and audio processing</p></li>
<li><p>Convolution Layer, Pooling Layer, Fully Connected Layer</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Recurrent Neural Network (RNN)</strong></p>
<ul>
<li><p>Preferred methods in NLP</p></li>
<li><p>Most fully-connected networks do not remember the steps from previous situations and therefore do not learn to make decisions based on <strong>context</strong> in training.</p></li>
<li><p>RNN stores the past information and all its decisions are taken from what it has learned from the past.</p></li>
<li><p>RNN is effective in dealing with time-series data (e.g., text, speech).</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/s2s-rnn1.jpeg" /></p>
</div>
</div>
<div class="section" id="building-a-neural-network-model-in-python">
<h2>Building a Neural Network Model in Python<a class="headerlink" href="#building-a-neural-network-model-in-python" title="Permalink to this headline">¶</a></h2>
<div class="section" id="packages-in-focus">
<h3>Packages in Focus<a class="headerlink" href="#packages-in-focus" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>: It is an open source machine learning library used for numerical computational tasks developed by Google.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code>: It is a high level API built on top of Tensorflow. It originated as an independent library and now has been incorporated as part of Tensorflow 2+.</p></li>
</ul>
<ul class="simple">
<li><p>Usually we need to define the architecture of the neural network model in terms of <strong>depths</strong> and <strong>widths</strong> of the layers.</p></li>
<li><p>After we define the structure of the network and initialize the values for all parameters, the training requires an iterative processing involving:</p>
<ul>
<li><p><strong>Forward Propagation</strong>: It refers to the process of transforming the data values by moving the input data through the network to get output.</p></li>
<li><p>Define your <strong>loss function</strong>.</p></li>
<li><p>Calculate <strong>Total Error</strong> based on the loss function.</p></li>
<li><p>Calculate <strong>Gradients</strong> via <strong>Back Propagation</strong></p></li>
<li><p>Update the <strong>weights</strong> based on gradients.</p></li>
<li><p>Iterate the process until the stop-condition is reached.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="create-model">
<h3>Create Model<a class="headerlink" href="#create-model" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code>, we can create models in two ways:</p>
<ul>
<li><p>Sequential API (<code class="docutils literal notranslate"><span class="pre">keras.Sequential</span></code>)</p></li>
<li><p>Functional API (<code class="docutils literal notranslate"><span class="pre">keras.model</span></code>)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="c1">## Sequential API to create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_1&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_2&quot;</span><span class="p">))</span>

<span class="c1"># Sequential API (Another alternative)</span>
<span class="c1"># model = keras.Sequential(</span>
<span class="c1">#     [</span>
<span class="c1">#         keras.Input(shape=(2)),</span>
<span class="c1">#         layers.Dense(4, activation=&quot;relu&quot;),</span>
<span class="c1">#         layers.Dense(2, activation=&quot;relu&quot;)</span>
<span class="c1">#     ]</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Functional API (A bit more flexible)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_1&quot;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_2&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="useful-modules-in-keras">
<h3>Useful Modules in keras<a class="headerlink" href="#useful-modules-in-keras" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">to_categorical()</span></code>: To convert a class/label list into a one-hot encoding matrix</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">print_summary()</span></code>: Print the summary of the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plot_model()</span></code>: Plot the model structure</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span><span class="p">,</span> <span class="n">print_summary</span><span class="p">,</span> <span class="n">plot_model</span>

<span class="n">plot_model</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you create the model using the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> method, you will not be able to check the <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> or <code class="docutils literal notranslate"><span class="pre">plot_model()</span></code> until the model has been compiled and fitted.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
dense_layer_1 (Dense)        (None, 4)                 12        
_________________________________________________________________
dense_layer_2 (Dense)        (None, 2)                 10        
=================================================================
Total params: 22
Trainable params: 22
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_layer_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_44_0.png" src="../_images/dl-neural-network-from-scratch_44_0.png" />
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Useful Modules in keras<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="section" id="keras-utils">
<h4><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code>:<a class="headerlink" href="#keras-utils" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">to_categorical()</span></code>: To convert a class/label list into a one-hot encoding matrix</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plot_model()</span></code>: Plot the model structure</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span><span class="p">,</span> <span class="n">plot_model</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Text Labels to Sequences</span>
<span class="n">labels_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 2, 1, 1, 0, 0]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Sequences to One-Hot Encoding</span>
<span class="n">to_categorical</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 0., 1.],
       [0., 0., 1.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [1., 0., 0.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="keras-layers">
<h4><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code><a class="headerlink" href="#keras-layers" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Dense</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SimpleRNN</span></code>, <code class="docutils literal notranslate"><span class="pre">LSTM</span></code>, <code class="docutils literal notranslate"><span class="pre">GRU</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">Flatten</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Attention</span></code>, <code class="docutils literal notranslate"><span class="pre">AdditiveAttention</span></code></p></li>
</ul>
</div>
<div class="section" id="other-modules">
<h4>Other modules<a class="headerlink" href="#other-modules" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">keras.losses</span></code>: Loss functions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers</span></code>: Optimization is an important process which optimizes the input weights by comparing the prediction and the loss function.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers.RMSprop</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers.Adam</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimziers.SGD</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.metrics</span></code>: Metrics for model evaluation</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="a-quick-example-of-model-training">
<h2>A Quick Example of Model Training<a class="headerlink" href="#a-quick-example-of-model-training" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/keras-workflow1.png" /></p>
<ul class="simple">
<li><p>Steps for Model Training</p>
<ul>
<li><p>Create model</p></li>
<li><p>Compile model</p></li>
<li><p>Fit model</p></li>
<li><p>Evaluate model</p></li>
<li><p>Predict</p></li>
</ul>
</li>
</ul>
<div class="section" id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We create random samples, where each sample is characterized by two random numbers (e.g., grades in subjects)</p></li>
<li><p>Each sample is also labeled with a binary class label (e.g., fail or pass)</p></li>
<li><p>We create three datasets: <strong>training</strong>, <strong>validation</strong>, and <strong>testing</strong> sets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># train set</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># val set</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># test set</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100, 2)
(100,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.69646919 0.28613933]
 [0.22685145 0.55131477]
 [0.71946897 0.42310646]
 [0.9807642  0.68482974]
 [0.4809319  0.39211752]]
[1 1 0 1 0]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="model-training">
<h3>Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compile</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
4/4 [==============================] - 3s 666ms/step - loss: 0.6888 - accuracy: 0.5226 - val_loss: 0.6882 - val_accuracy: 0.5400
Epoch 2/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6995 - accuracy: 0.4976 - val_loss: 0.6890 - val_accuracy: 0.5400
Epoch 3/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6878 - accuracy: 0.5090 - val_loss: 0.6898 - val_accuracy: 0.5400
Epoch 4/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6762 - accuracy: 0.5705 - val_loss: 0.6902 - val_accuracy: 0.5300
Epoch 5/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6759 - accuracy: 0.5507 - val_loss: 0.6908 - val_accuracy: 0.5300
Epoch 6/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6848 - accuracy: 0.5340 - val_loss: 0.6901 - val_accuracy: 0.5300
Epoch 7/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6883 - accuracy: 0.5070 - val_loss: 0.6898 - val_accuracy: 0.5400
Epoch 8/50
4/4 [==============================] - 0s 16ms/step - loss: 0.6824 - accuracy: 0.5420 - val_loss: 0.6899 - val_accuracy: 0.5500
Epoch 9/50
4/4 [==============================] - 0s 44ms/step - loss: 0.6781 - accuracy: 0.5629 - val_loss: 0.6902 - val_accuracy: 0.5600
Epoch 10/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6734 - accuracy: 0.5886 - val_loss: 0.6906 - val_accuracy: 0.5400
Epoch 11/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6826 - accuracy: 0.5573 - val_loss: 0.6910 - val_accuracy: 0.5400
Epoch 12/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6771 - accuracy: 0.5737 - val_loss: 0.6913 - val_accuracy: 0.5400
Epoch 13/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6712 - accuracy: 0.6152 - val_loss: 0.6918 - val_accuracy: 0.5400
Epoch 14/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6862 - accuracy: 0.5744 - val_loss: 0.6924 - val_accuracy: 0.5500
Epoch 15/50
4/4 [==============================] - 0s 31ms/step - loss: 0.6791 - accuracy: 0.5723 - val_loss: 0.6930 - val_accuracy: 0.5400
Epoch 16/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6699 - accuracy: 0.6088 - val_loss: 0.6938 - val_accuracy: 0.5500
Epoch 17/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6787 - accuracy: 0.6004 - val_loss: 0.6948 - val_accuracy: 0.5400
Epoch 18/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6800 - accuracy: 0.5509 - val_loss: 0.6965 - val_accuracy: 0.5400
Epoch 19/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6827 - accuracy: 0.5480 - val_loss: 0.6982 - val_accuracy: 0.5400
Epoch 20/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6708 - accuracy: 0.5556 - val_loss: 0.6989 - val_accuracy: 0.5400
Epoch 21/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6830 - accuracy: 0.5396 - val_loss: 0.6989 - val_accuracy: 0.5300
Epoch 22/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6806 - accuracy: 0.5705 - val_loss: 0.6989 - val_accuracy: 0.5500
Epoch 23/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6658 - accuracy: 0.6244 - val_loss: 0.6988 - val_accuracy: 0.5200
Epoch 24/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6776 - accuracy: 0.6034 - val_loss: 0.6980 - val_accuracy: 0.5300
Epoch 25/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6672 - accuracy: 0.6291 - val_loss: 0.6979 - val_accuracy: 0.4800
Epoch 26/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6689 - accuracy: 0.6371 - val_loss: 0.6986 - val_accuracy: 0.5200
Epoch 27/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6702 - accuracy: 0.6836 - val_loss: 0.7004 - val_accuracy: 0.4500
Epoch 28/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6650 - accuracy: 0.6204 - val_loss: 0.7028 - val_accuracy: 0.4400
Epoch 29/50
4/4 [==============================] - 0s 17ms/step - loss: 0.6703 - accuracy: 0.5768 - val_loss: 0.7053 - val_accuracy: 0.4600
Epoch 30/50
4/4 [==============================] - 0s 16ms/step - loss: 0.6707 - accuracy: 0.5870 - val_loss: 0.7074 - val_accuracy: 0.4800
Epoch 31/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6744 - accuracy: 0.5431 - val_loss: 0.7081 - val_accuracy: 0.4800
Epoch 32/50
4/4 [==============================] - 0s 16ms/step - loss: 0.6702 - accuracy: 0.6070 - val_loss: 0.7074 - val_accuracy: 0.4600
Epoch 33/50
4/4 [==============================] - 0s 16ms/step - loss: 0.6677 - accuracy: 0.6037 - val_loss: 0.7065 - val_accuracy: 0.4300
Epoch 34/50
4/4 [==============================] - 0s 16ms/step - loss: 0.6728 - accuracy: 0.5620 - val_loss: 0.7067 - val_accuracy: 0.4300
Epoch 35/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6663 - accuracy: 0.6171 - val_loss: 0.7066 - val_accuracy: 0.4500
Epoch 36/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6650 - accuracy: 0.5973 - val_loss: 0.7073 - val_accuracy: 0.4500
Epoch 37/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6648 - accuracy: 0.5921 - val_loss: 0.7086 - val_accuracy: 0.4400
Epoch 38/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6621 - accuracy: 0.5964 - val_loss: 0.7105 - val_accuracy: 0.4600
Epoch 39/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6714 - accuracy: 0.5603 - val_loss: 0.7121 - val_accuracy: 0.4600
Epoch 40/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6684 - accuracy: 0.5839 - val_loss: 0.7126 - val_accuracy: 0.4600
Epoch 41/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6675 - accuracy: 0.5799 - val_loss: 0.7127 - val_accuracy: 0.4400
Epoch 42/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6628 - accuracy: 0.5935 - val_loss: 0.7122 - val_accuracy: 0.4500
Epoch 43/50
4/4 [==============================] - 0s 18ms/step - loss: 0.6761 - accuracy: 0.5527 - val_loss: 0.7117 - val_accuracy: 0.4400
Epoch 44/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6648 - accuracy: 0.5777 - val_loss: 0.7117 - val_accuracy: 0.4700
Epoch 45/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6628 - accuracy: 0.6175 - val_loss: 0.7120 - val_accuracy: 0.4700
Epoch 46/50
4/4 [==============================] - 0s 13ms/step - loss: 0.6627 - accuracy: 0.6803 - val_loss: 0.7129 - val_accuracy: 0.4900
Epoch 47/50
4/4 [==============================] - 0s 43ms/step - loss: 0.6586 - accuracy: 0.7022 - val_loss: 0.7145 - val_accuracy: 0.5000
Epoch 48/50
4/4 [==============================] - 0s 15ms/step - loss: 0.6599 - accuracy: 0.6227 - val_loss: 0.7151 - val_accuracy: 0.5200
Epoch 49/50
4/4 [==============================] - 0s 26ms/step - loss: 0.6596 - accuracy: 0.6393 - val_loss: 0.7159 - val_accuracy: 0.5200
Epoch 50/50
4/4 [==============================] - 0s 14ms/step - loss: 0.6439 - accuracy: 0.6902 - val_loss: 0.7171 - val_accuracy: 0.5200
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 16)                48        
_________________________________________________________________
dense_1 (Dense)              (None, 16)                272       
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 17        
=================================================================
Total params: 337
Trainable params: 337
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_66_0.png" src="../_images/dl-neural-network-from-scratch_66_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot history from model.fit()</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;model accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_67_0.png" src="../_images/dl-neural-network-from-scratch_67_0.png" />
</div>
</div>
</div>
<div class="section" id="evaluation-and-prediction">
<h3>Evaluation and Prediction<a class="headerlink" href="#evaluation-and-prediction" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Evaludate</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 1ms/step - loss: 0.7566 - accuracy: 0.3800
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.7565879821777344, 0.3799999952316284]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Predict</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.5010157 ],
       [0.51096517],
       [0.5250439 ],
       [0.45609704],
       [0.6086711 ]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="learning-and-training">
<h2>Learning and Training<a class="headerlink" href="#learning-and-training" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-does-the-neural-network-learn-the-parameters">
<h3>How does the neural network learn the parameters?<a class="headerlink" href="#how-does-the-neural-network-learn-the-parameters" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Forward propagation shows how the network takes the input values, transforms them, and produces the predicted output values based on the network parameters (i.e., weights).</p></li>
<li><p>The network needs to learn the weights that best produce the output values according to some <strong>loss function</strong>, i.e., how different is the prediction from the true label?</p></li>
<li><p>Crucially, we need to compute the differences between the <strong>predicted</strong> outputs of the network and the <strong>true</strong> target outputs.</p></li>
<li><p>The model should aim to minimize these differences, which are commonly referred to as <strong>errors</strong> of the model.</p></li>
</ul>
</div>
<div class="section" id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>If the target ouputs are numeric values, we can evaluate the errors (i.e., the differences between the actual model outputs and the target outputs) using the <strong>mean square error</strong> function.</p>
<ul>
<li><p>Mean Square Error: <span class="math notranslate nohighlight">\(E = \frac{1}{2}\sum(y_k - t_k)^2\)</span></p></li>
</ul>
</li>
<li><p>If the target outputs are labels, we can evaluate the errors (i.e., the differences between the actual model labels and the target labels) using the <strong>cross entory error</strong> function.</p>
<ul>
<li><p>Cross Entropy Error: <span class="math notranslate nohighlight">\(E= -\sum_{k}t_k\log(y_k)\)</span></p></li>
</ul>
</li>
<li><p>The function used to compute the errors of the model is referred to as the <strong>loss function</strong>.</p></li>
</ul>
</div>
<div class="section" id="principles-for-loss-functions">
<h3>Principles for Loss Functions<a class="headerlink" href="#principles-for-loss-functions" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem Type</p></th>
<th class="head"><p>Last-Layer Activation</p></th>
<th class="head"><p>Loss Function in Keras</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">y</span></code> Encoding</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary Classification</p></td>
<td><p>sigmoid</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">binary_crossentropy()</span></code></p></td>
<td><p>Integer Encoding</p></td>
</tr>
<tr class="row-odd"><td><p>Binary Classification</p></td>
<td><p>softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy(from_logits=False)</span></code></p></td>
<td><p>Integer Encoding</p></td>
</tr>
<tr class="row-even"><td><p>Multiclass Classification</p></td>
<td><p>softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">categorical_crossentropy()</span></code></p></td>
<td><p>One-hot Encoding</p></td>
</tr>
<tr class="row-odd"><td><p>Regression</p></td>
<td><p>None</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mes</span></code></p></td>
<td><p>Floating-point</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="categorical-variables-encoding">
<h2>Categorical Variables Encoding<a class="headerlink" href="#categorical-variables-encoding" title="Permalink to this headline">¶</a></h2>
<div class="section" id="what-s-wrong-with-categorical-variables">
<h3>What’s wrong with categorical variables?<a class="headerlink" href="#what-s-wrong-with-categorical-variables" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Before we look at the examples of loss computation, we need to talk about the ways of encoding <strong>categorical</strong> labels.</p></li>
<li><p>Remember, machine doesn’t understand characters. If we have class labels like male/female, positive/negative, or each word tokens, we may need to convert these strings into machine-readable numerical values.</p></li>
<li><p>This step is called <strong>encoding</strong>.</p></li>
<li><p>Most importantly, machine learning and deep learning both require input and output variables to be <strong>numbers</strong>.</p></li>
</ul>
</div>
<div class="section" id="three-ways-of-encodings">
<h3>Three Ways of Encodings<a class="headerlink" href="#three-ways-of-encodings" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Integer</strong> (Sequence) Encoding: Each unique label is mapped to an integer</p></li>
<li><p><strong>One-hot</strong> Encoding: Each unique label is mapped to a binary vector</p></li>
<li><p><strong>Embeddings</strong> Encoding: Each unique label is mapped to a learned vectorized representation (i.e., embeddings)</p></li>
</ul>
</div>
<div class="section" id="steps">
<h3>Steps<a class="headerlink" href="#steps" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Given a corpus, we can create its <strong>vocabulary</strong> (at the word-token level, or sometimes at the character level).</p></li>
<li><p>We can associate every word type with a <strong>unique integer index</strong> (i.e., integer encoding).</p></li>
<li><p>We can further turn this integer index <span class="math notranslate nohighlight">\(i\)</span> into a <strong>binary vector</strong> of size <span class="math notranslate nohighlight">\(N\)</span> (the size of vocabulary). The vector is all zeros except for the <span class="math notranslate nohighlight">\(i\)</span>th entry, which is 1 (i.e., one-hot encoding).</p></li>
<li><p>We can turn the integer index into a <strong>dense, low-dimensional floating-point vectors</strong> (i.e., embedding encoding).</p>
<ul>
<li><p>Learn embeddings jointly with the main task (i.e., <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer)</p></li>
<li><p>Use pretrained word embeddings that were precomputed using a different machine learning task.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Given Label</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span>

<span class="c1">## Create Dictionary</span>
<span class="n">labels_dict</span> <span class="o">=</span> <span class="p">{</span> <span class="n">x</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_dict</span><span class="p">)</span>

<span class="c1">## Integer Encoding</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>

<span class="c1">## One-hot Encoding</span>
<span class="n">labels_oh</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_oh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;male&#39;: 0, &#39;female&#39;: 1}
[0, 1, 1, 0]
[[1. 0.]
 [0. 1.]
 [0. 1.]
 [1. 0.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###########################################</span>
<span class="c1">## Integer Encoding and One-hot Encoding ##</span>
<span class="c1">## Using sklearn                         ##</span>
<span class="c1">###########################################</span>

<span class="c1">## Integer Encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">oe</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int16&#39;</span><span class="p">)</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="n">oe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>

<span class="c1">## One-hot Encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">ohe</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">labels_ohe</span> <span class="o">=</span> <span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">labels_ohe</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1]
 [0]
 [0]
 [1]]
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="one-hot-encoding-and-loss-function">
<h3>One-hot Encoding and Loss Function<a class="headerlink" href="#one-hot-encoding-and-loss-function" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>With one-hot encoding, we can convert any categorical variable into a numeric vector.</p></li>
<li><p>And if we specify our network output to be a vector of the same size, we can compute the differences between the output vector and the true numeric vector.</p></li>
</ul>
<ul class="simple">
<li><p>When the target class label is a binary one (e.g., name-gender prediction):</p></li>
</ul>
<p><img alt="" src="../_images/neural-network-loss-binary.jpeg" /></p>
<ul class="simple">
<li><p>When the target class label is a multi-level one (e.g., next-word prediction):
<img alt="" src="../_images/neural-network-loss-multicat.jpeg" /></p></li>
</ul>
</div>
</div>
<div class="section" id="examples-of-loss-errors-skipped">
<h2>Examples of Loss Errors (skipped)<a class="headerlink" href="#examples-of-loss-errors-skipped" title="Permalink to this headline">¶</a></h2>
<div class="section" id="error-based-on-one-sample">
<h3>Error based on One Sample<a class="headerlink" href="#error-based-on-one-sample" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The following is a simple example showing how to compute the loss for a case of prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_square_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c1"># avoid log(0)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## mean square error</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>  <span class="c1"># predicted values</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># true label</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mean_square_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.40625
2.302584092994546
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="errors-based-on-batch-samples">
<h3>Errors based on Batch Samples<a class="headerlink" href="#errors-based-on-batch-samples" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The following is a simple example showing how to compute the average loss of a sample of batch size N cases.</p></li>
</ul>
<ul class="simple">
<li><p>If the training is based on a sample of batch size <em>N</em>, we can compute the average loss (or total errors) of the batch sample:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ E = - \frac{1}{N}\sum_n\sum_k t_{nk}\log y_{nk}\]</div>
<ul class="simple">
<li><p>We can revise the <code class="docutils literal notranslate"><span class="pre">cross_entropy_error()</span></code> function to work with outputs from a min-batch sample.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># adjust the function to for batch sample outputs</span>
<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When the labels uses one-hot encoding, the function can be simplified as follows:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># because for one-hot labels</span>
    <span class="c1"># cross-entropy sums only the values of the true labels `1`</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-do-we-adjust-the-model-parameters">
<h3>How do we adjust the model parameters?<a class="headerlink" href="#how-do-we-adjust-the-model-parameters" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>With the <strong>Loss Function</strong>, we can now perform the most important step in model training — adjusting the <strong>weights</strong> (i.e., <strong>parameters</strong>) of the model.</p></li>
<li><p>The mechanism behind the neural network training steps is that we need to figure out:</p>
<ul>
<li><p>how the change of a specific <strong>parameter</strong> (i.e., weight) in the model may lead to the change (i.e., decrease) of the values from the <strong>Loss Function</strong>? (i.e., How much does a change in a specific weight affect the total error?)</p></li>
</ul>
</li>
<li><p>Then we would know how much of the total error each parameter in the model is responsible for.</p></li>
<li><p>These are the bases for parameter adjustments.</p></li>
<li><p>All we need to do is the adjust the weights <strong>in proportion to</strong> the changes that the parameter is responsible for.</p></li>
<li><p>This optimization method to finding a combination of weights that minimize the loss function is called <strong>Gradient Descent</strong>.</p></li>
</ul>
</div>
<div class="section" id="gradients">
<h3>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The error that a specific weight is responsible for is referred to as the <strong>gradient</strong> of the parameter.</p></li>
<li><p>Mathematically, the gradient of a weight is the <strong>partial derivative</strong> of a weight in relation to the <strong>loss function</strong>.</p></li>
<li><p>Then we adjust the weight in proportion to its gradient:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
{W}_{new} = W_{original} + \eta \times \textit{Gradient}_{W_{original}}
\]</div>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is a hyperparameter in deep learning. This parameter controls how fast the model learns. This <span class="math notranslate nohighlight">\(\eta\)</span> is referred to as the <strong>learning rates</strong>.</p></li>
</ul>
</div>
<div class="section" id="intuition-of-gradients-an-example-skipped">
<h3>Intuition of Gradients: An Example (skipped)<a class="headerlink" href="#intuition-of-gradients-an-example-skipped" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Let’s assume that we are training a model with two parameters, <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, and the loss function is <span class="math notranslate nohighlight">\(f(x_0, x_1)\)</span>.</p>
<ul>
<li><p>When a function includes more than one parameters, we can compute the partial derivative of the function with respect to each parameter.</p></li>
<li><p>When all partial derivatives are concatenated into a vector, this vector is called the <strong>gradient</strong>.</p></li>
<li><p>That is, for the above loss function with two parameters (e.g., <span class="math notranslate nohighlight">\(f(x_0, x_1) = \beta x_0 + \beta x_1\)</span>), we can calculate the partial derivatives of each parameter all at once,and represent them in a vector, which is referred to as <strong>gradient</strong>, i.e:
$<span class="math notranslate nohighlight">\(
(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1})
\)</span>$</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Each parameter estimation value pair, <span class="math notranslate nohighlight">\((x_0,x_1)\)</span>, should correspond to a gradient.</p></li>
<li><p>Intuitive understanding of the gradient:</p>
<ul>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> indicates how the changes of the parameter values <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> may contribute to the change of <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span>.</p></li>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is a <strong>vector</strong> with the direction pointing at the <strong>global minimum</strong> of the loss function.</p></li>
<li><p>The farther the <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is way from the global minimum, the larger the gradient vector.</p></li>
</ul>
</li>
<li><p>In Deep Learning, the training goes as follows:</p>
<ul>
<li><p>We initialize the parameters <span class="math notranslate nohighlight">\(x_0,x_1\)</span> with some values <span class="math notranslate nohighlight">\(p_0, p_1\)</span>;</p></li>
<li><p>We compute the loss function <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span></p></li>
<li><p>We compute the gradient of <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span> when the parameter <span class="math notranslate nohighlight">\(x_0 = p_0\)</span> and <span class="math notranslate nohighlight">\(x_1 = p_1\)</span></p></li>
<li><p>We use the gradient to determine how to update/modify all the model parameters, i.e.,</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
x_0 = x_0 + \eta\frac{\partial f}{\partial x_0} \\
x_1 = x_1 + \eta\frac{\partial f}{\partial x_1} 
\end{split}\]</div>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is again <strong>learning rate</strong>.</p></li>
</ul>
</div>
<div class="section" id="types-of-gradient-descent">
<h3>Types of Gradient Descent<a class="headerlink" href="#types-of-gradient-descent" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Batch</strong> Gradient Descent: Update the model weights after one epoch of the entire training set.</p></li>
<li><p><strong>Stochastic</strong> Gradient Descent (SGD): Update the model weights after every instance of the training set (online).</p></li>
<li><p><strong>Mini-batch</strong> Gradient Descent: Update the model weights after a subset of the training set. (Recommended!)</p></li>
</ul>
</div>
<div class="section" id="gradients-in-python-skipped">
<h3>Gradients in Python (skipped)<a class="headerlink" href="#gradients-in-python-skipped" title="Permalink to this headline">¶</a></h3>
<p>In the following graph, each vector represents the gradient at a specific <span class="math notranslate nohighlight">\((x_0, x_1)\)</span>, i.e., when <span class="math notranslate nohighlight">\(x_0 = p_0\)</span> and <span class="math notranslate nohighlight">\(x_1 = p_1\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_103_0.png" src="../_images/dl-neural-network-from-scratch_103_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="overfitting-and-underfitting">
<h2>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this headline">¶</a></h2>
<div class="section" id="optimization-vs-generalization">
<h3>Optimization vs. Generalization<a class="headerlink" href="#optimization-vs-generalization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>ML is always dealing with the tension between <strong>optimization</strong> and <strong>generalization</strong>.</p>
<ul>
<li><p>Optimization: the process of adjusting a model to get the best performance possible on the training data (i.e., to minimize the <em>bias</em> of the model)</p></li>
<li><p>Generalization: the performance of the model on data it has never seen before (i.e., to minimize the <strong>variation</strong> of the model performance on different datasets.)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="overfitting-underfitting">
<h3>Overfitting/Underfitting<a class="headerlink" href="#overfitting-underfitting" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>During the training stage, if both the losses on training data and validation data are dropping, the model is <strong>underfit</strong> and in a way to be better fit. (Great bias, Small variation)</p></li>
<li><p>During the training, if the loss on the validation data stalls while that of the training data still drops, the model starts to <strong>overfit</strong>. (Small bias, Great variation)</p></li>
</ul>
</div>
<div class="section" id="how-to-deal-with-overfitting">
<h3>How to Deal With Overfitting?<a class="headerlink" href="#how-to-deal-with-overfitting" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Get more training data</p></li>
<li><p>Reduce the capacity of the network (e.g., number of layers/nodes)</p></li>
<li><p>Add weight <strong>Regularization</strong></p></li>
<li><p>Add <strong>dropout</strong></p></li>
</ul>
</div>
<div class="section" id="regularization">
<h3>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>What is <strong>regularization</strong>?</p>
<ul>
<li><p>Modulate the quantity of information that the model is allowed to store</p></li>
<li><p>Add constraints on what information the model is allowed to store</p></li>
</ul>
</li>
<li><p>Weight regularization is to add to the loss function of the network a <strong>cost</strong> associated with having large weights.</p></li>
<li><p>That is, large weights will have greater penalties to the loss function, hence discouraged.</p></li>
<li><p>Common methods:</p>
<ul>
<li><p>L1 regularization: the cost added is proportional to the <strong>absolute values</strong> of the weights (<code class="docutils literal notranslate"><span class="pre">keras.regularizer.l1()</span></code>)</p></li>
<li><p>L2 regularization: the cost added is propositional to the <strong>square values</strong> of the weights (<code class="docutils literal notranslate"><span class="pre">keras/regularizer.l2()</span></code>)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Dropout consists of randomly dropping out a number of output features of the applied layer during training.</p></li>
<li><p>The dropout rate is the <strong>fraction</strong> of the features that are <strong>zeroed out</strong>.</p></li>
<li><p>The dropout rate is usually set between 0.2 and 0.5.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">keras</span></code>, it can an independent layer, <code class="docutils literal notranslate"><span class="pre">keras.layers.dropout()</span></code>, or paremters to be set within specific layers (e.g., <code class="docutils literal notranslate"><span class="pre">keras.layers.LSTM()</span></code>).</p></li>
</ul>
</div>
<div class="section" id="heuristics-for-regularization">
<h3>Heuristics for Regularization<a class="headerlink" href="#heuristics-for-regularization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Applying dropout before a recurrent layer hinders learning rather than helping with regularization.</p></li>
<li><p>The same dropout mask (the same pattern of dropped units) should be applied at every time step, instead of a dropout mask that varies randomly from timestep to timestep. (Yarin Gal)</p></li>
<li><p>In order to regularize the representations formed by the recurrent gates of layers (e.g., LSTM and GRU), a temporally constant dropout mask should be applied to the inner activations of the layer (a recurrent dropout mask). (Yarin Gal)</p></li>
</ul>
</div>
</div>
<div class="section" id="some-math-skipped">
<h2>Some Math (Skipped)<a class="headerlink" href="#some-math-skipped" title="Permalink to this headline">¶</a></h2>
<p>The following presents some important mathematical constructs related to the understanding of neural network.</p>
<div class="section" id="chain-rule-and-back-propagation">
<h3>Chain Rule and Back Propagation<a class="headerlink" href="#chain-rule-and-back-propagation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Because there are many parameters in a network, we can compute the gradients (or partial derivatives) of all the weights using the chain rules of derivatives.</p></li>
<li><p>Specifically, the total error is essentially broken up and distributed back through the network to every single weight with the help of chain rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}.\frac{\partial y}{\partial x}\]</div>
<ul class="simple">
<li><p>This process is referred to as <strong>back propagation</strong>: moving back through the network, back-propagating the total errors to every single weight, and updating the weights.</p></li>
<li><p>The principle of weights-updating: the larger the gradient, the more the adjustments.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W_1 = W_1 - \eta \frac{\partial E}{\partial W_1}\]</div>
<ul class="simple">
<li><p>The above adjustment formula suggests that the weight updates are proportional to the partial derivatives of the weight.</p></li>
<li><p>The <strong><span class="math notranslate nohighlight">\(\eta\)</span></strong> in the formula controls the amount of adjustment, which is referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</div>
<div class="section" id="elementwise-operations-of-matrix">
<h3>Elementwise Operations of Matrix<a class="headerlink" href="#elementwise-operations-of-matrix" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A two-dimensional matrix</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} x = 
\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6 \\
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 2],
       [3, 4],
       [4, 6]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 3]
 [4 5]
 [5 7]]
[[ 5 10]
 [15 20]
 [20 30]]
[[0.2 0.4]
 [0.6 0.8]
 [0.8 1.2]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Matrix Elementwise Multiplication</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
5&amp;6 \\
7&amp;8
\end{pmatrix} =
\begin{pmatrix}
5&amp;12 \\
21&amp;32
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1 2]
 [3 4]]
[[5 6]
 [7 8]]
[[ 5 12]
 [21 32]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="broadcast">
<h3>Broadcast<a class="headerlink" href="#broadcast" title="Permalink to this headline">¶</a></h3>
<p>In matrix elementwise computation, the smaller tensor will be <strong>broadcasted</strong> to match the shape of the larger tensor.</p>
<ul class="simple">
<li><p>Axes (called broadcast axes) are added to the smaller tensor to match the <code class="docutils literal notranslate"><span class="pre">ndim</span></code> of the larger tensor.</p></li>
<li><p>The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
10&amp;20
\end{pmatrix} =
\begin{pmatrix}
10&amp;40 \\
30&amp;80
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2, 2)
(2,)
[[10 40]
 [30 80]]
(2, 2)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="matrices-dot-production">
<h3>Matrices Dot Production<a class="headerlink" href="#matrices-dot-production" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/matrices-dot-product.png" />
(Source: Chollet [2018], Ch 2., Figure 2.5)</p>
<ul class="simple">
<li><p>The most common applications may be the <strong>dot product</strong> between two matrices.</p></li>
<li><p>You can take the dot product of two matrices x and y (<code class="docutils literal notranslate"><span class="pre">dot(x,</span> <span class="pre">y)</span></code>) if and only if <code class="docutils literal notranslate"><span class="pre">x.shape[1]</span> <span class="pre">==</span> <span class="pre">y.shape[0]</span></code>. The result is a matrix with shape (<code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code>, <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>), where the coefficients are the vector products between the rows of <span class="math notranslate nohighlight">\(x\)</span> and the columns of <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6
\end{pmatrix}
\begin{pmatrix}
5&amp;6&amp;7 \\
8&amp;9&amp;10
\end{pmatrix} =
\begin{pmatrix}
21&amp;24&amp;27 \\
47&amp;54&amp;62 \\
73&amp;84&amp;95
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>

<span class="n">xy_dot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy_dot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[21 24 27]
 [47 54 61]
 [73 84 95]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="matrices-dot-production-and-forward-propagation">
<h3>Matrices Dot Production and Forward Propagation<a class="headerlink" href="#matrices-dot-production-and-forward-propagation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>For example, let’s assume that we have a word, one-hot encoded as [0,1,0,0,0,0].</p></li>
<li><p>An embedding model consists of parameters like the two-dimensional tensor shown below.</p></li>
<li><p>The output of the model is the dot product of the input word vector and the model parameter tensor.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
-2.8&amp;1.6&amp;0.9&amp;0.7&amp;-0.7&amp;-1.8 \\
0.3&amp;-2.3&amp;0.8&amp;1.8&amp;1.5&amp;0.7\\
0.9&amp;0.3&amp;-1.6&amp;-2.8&amp;0.5&amp;0.4\\
1.8&amp;-0.5&amp;-1.6&amp;-2.8&amp;-1.7&amp;1.7
\end{pmatrix}
\begin{pmatrix}
0\\
1\\
0\\
0\\
0\\
0\\
\end{pmatrix}=
\begin{pmatrix}
1.6 \\
-2.3 \\
0.3\\
-0.5
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">word_one_hot</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_one_hot</span><span class="p">)</span>
<span class="n">model_parameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0]
 [1]
 [0]
 [0]
 [0]
 [0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">,</span> <span class="n">word_one_hot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.6],
       [-2.3],
       [ 0.3],
       [-0.5]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="derivatives">
<h3>Derivatives<a class="headerlink" href="#derivatives" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Given a normal function, <span class="math notranslate nohighlight">\(f(x) = y \)</span> , if the <span class="math notranslate nohighlight">\(x\)</span> value changes, <span class="math notranslate nohighlight">\(y\)</span> will change as well.</p></li>
<li><p>So if we increase <span class="math notranslate nohighlight">\(x\)</span> by a small factor <span class="math notranslate nohighlight">\(h\)</span>, this results in a small change to <span class="math notranslate nohighlight">\(y\)</span>, i.e., <span class="math notranslate nohighlight">\(f(x+h) - f(x)\)</span>.</p></li>
<li><p>We can then compute the change of <span class="math notranslate nohighlight">\(y\)</span> relative to the small change of <span class="math notranslate nohighlight">\(x\)</span>, i.e., <span class="math notranslate nohighlight">\(\frac{f(x+h) - f(x)}{h}\)</span></p></li>
<li><p>When <span class="math notranslate nohighlight">\(h\)</span> is very very small around a certain point <span class="math notranslate nohighlight">\(p\)</span>, we can then estimate the change of <span class="math notranslate nohighlight">\(y\)</span> at the point when  <span class="math notranslate nohighlight">\(x = p\)</span>, i.e., <span class="math notranslate nohighlight">\(\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\)</span></p></li>
</ul>
<ul class="simple">
<li><p>This instantaneous change of <span class="math notranslate nohighlight">\(y\)</span> is called the <strong>derivetaive</strong> of <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\(p\)</span>.</p>
<ul>
<li><p>If it is negative, it means a small change of <span class="math notranslate nohighlight">\(x\)</span> around <span class="math notranslate nohighlight">\(p\)</span> will result in a decrease of <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
<li><p>If it is positive, a small change in <span class="math notranslate nohighlight">\(x\)</span> will result in an increase of <span class="math notranslate nohighlight">\(f(x)\)</span>.</p></li>
<li><p>The absolute value (i.e., the magnitude) of the derivative indicates how quickly this increase or decrease will happen.</p></li>
</ul>
</li>
<li><p>This can be mathematically represented as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x}= \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]</div>
<ul class="simple">
<li><p>The <strong>derivative</strong> turns out to be the <strong>slope of the tangent line</strong> at <span class="math notranslate nohighlight">\(x = p\)</span>.</p></li>
<li><p>If we are trying to update <span class="math notranslate nohighlight">\(x\)</span> by a factor <span class="math notranslate nohighlight">\(h\)</span> in order to minimize <span class="math notranslate nohighlight">\(f(x)\)</span>, and we know the derivative of <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>, we have everything we need:</p>
<ul>
<li><p>The derivative completely describes how <span class="math notranslate nohighlight">\(f(x)\)</span> evolves when we change <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>If we want to reduce the value of <span class="math notranslate nohighlight">\(f(x)\)</span>, we just need to move <span class="math notranslate nohighlight">\(x\)</span> a little in the opposite direction from the derivative.</p></li>
</ul>
</li>
<li><p>In Deep Learning, the <span class="math notranslate nohighlight">\(f(x)\)</span> is often the <strong>loss function</strong>, and <span class="math notranslate nohighlight">\(x\)</span> is often the parameter of the model.</p>
<ul>
<li><p>We initialize the parameter <span class="math notranslate nohighlight">\(x\)</span> with some value <span class="math notranslate nohighlight">\(p\)</span>;</p></li>
<li><p>We compute the loss function <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
<li><p>We compute the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> when the parameter <span class="math notranslate nohighlight">\(x = p\)</span></p></li>
<li><p>We use the derivative to determine how to update/modify the parameter, i.e., <span class="math notranslate nohighlight">\(x_{new} = x_{old} + \eta\frac{\partial f}{\partial x} \)</span></p></li>
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is commonly referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1">## change of y when a very small change in x</span>
    <span class="c1">#print(d)</span>
    <span class="c1"># d turns out to be the slope of the tangent line</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">d</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Take the derivates of the following function when x = 5 and 10:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[y = 4x^2 + 2x\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">fun_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">tf5</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y5</span> <span class="o">=</span> <span class="n">tf5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">tf10</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y10</span> <span class="o">=</span> <span class="n">tf10</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_144_0.png" src="../_images/dl-neural-network-from-scratch_144_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dl-neural-network-from-scratch_145_0.png" src="../_images/dl-neural-network-from-scratch_145_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1"># small change of x when x = 5 will slighly change y in positive direction</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1">## small change of x when x = 10 will greatly change y in positive direction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>41.99999999997317
81.99999999987995
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In python, we use the <strong>numerical differentiation</strong> method to find the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> when x = 5 and 10.</p></li>
<li><p>We can use the <strong>analytic differentiation</strong> method and derive the <strong>derivatie function</strong> <span class="math notranslate nohighlight">\(f'(x)\)</span> first:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f'(x) = \frac{\partial f}{\partial x}= 4x^2 + 2x = 8x + 2
\]</div>
<ul class="simple">
<li><p>Numerical differentiation produces derivatives with errors; analytic differentiation produces exact derivatives.</p></li>
</ul>
</div>
<div class="section" id="partial-derivatives">
<h3>Partial Derivatives<a class="headerlink" href="#partial-derivatives" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>When a function has many parameters, we can take the derivate of the function with respect to one particular parameter.</p></li>
<li><p>This parameter-specific derivative is called <strong>partial derivative</strong>.</p></li>
<li><p>Take the partial derivatives of the following function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x_0, x_1)=x_0^2 + x_1^2 \]</div>
<ul class="simple">
<li><p>Once if we have defined the function for the model (e.g., the Loss Function), we can calculate to what extent the change in weights would affect the change in the function.</p></li>
<li><p>The partial derivative refers to how a change in a specific weight <span class="math notranslate nohighlight">\(x_1\)</span> affects the function, i.e., the Loss Function or the total error.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial x_1}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## fun_2 has two variables/weights</span>
<span class="k">def</span> <span class="nf">fun_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_0\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_0}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp1</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">4.0</span><span class="o">**</span><span class="mi">2</span>


<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.00000000000378
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_1}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp2</span><span class="p">(</span><span class="n">x1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">3.0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span>


<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7.999999999999119
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>I highly recommend the following two books for deep learning with Python:</p>
<ul>
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010761759">Deep Learning：用Python進行深度學習的基礎理論實作</a></p></li>
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010817138?loc=P_br_r0vq68ygz_D_2aabd0_B_1">Deep Learning 2｜用Python進行自然語言處理的基礎理論實作</a></p></li>
</ul>
</li>
<li><p>This post collections a comprehensive list of learning resourcess for deep learning: <a class="reference external" href="https://buzzorange.com/techorange/2017/08/21/the-best-ai-lesson/">史上最完整機器學習自學攻略！我不相信有人看完這份不會把它加進我的最愛</a>.</p></li>
<li><p>Taylor, Michael. (2017). Neural Networks: A Visual Introduction for Beginners. (cf. Course Data)</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="topic-modeling-naive.html" title="previous page"><span class="section-number">2. </span>Topic Modeling: A Naive Example</a>
    <a class='right-next' id="next-link" href="dl-simple-case.html" title="next page">Deep Learning: A Simple Example</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>