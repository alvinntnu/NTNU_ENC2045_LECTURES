

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural Network From Scratch &#8212; ENC2045 Computational Linguistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/dl-neural-network-from-scratch';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Learning: A Simple Example" href="dl-simple-case.html" />
    <link rel="prev" title="Topic Modeling: A Naive Example" href="topic-modeling-naive.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ntnu-word-2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ntnu-word-2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">INTRODUCTION</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp-primer.html">Natural Language Processing: A Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-pipeline.html">NLP Pipeline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="text-preprocessing.html">Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="text-normalization-eng.html">Text Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-tokenization.html">Text Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="text-enrichment.html">Text Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese-word-seg.html">Chinese Word Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="google-colab.html">Google Colab</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Vectorization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="text-vec-traditional.html">Text Vectorization Using Traditional Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-overview.html">Machine Learning: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-simple-case.html">Machine Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-algorithm.html">Classification Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine-Learning NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml-common-nlp-tasks.html">Common NLP Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-sklearn-classification.html">Sentiment Analysis Using Bag-of-Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml-emsemble-learning.html">Emsemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic-modeling-naive.html">Topic Modeling: A Naive Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning NLP</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Neural Network From Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-simple-case.html">Deep Learning: A Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-sentiment-case.html">Deep Learning: Sentiment Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Language Model and Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-sequence-models-intuition.html">Sequence Models Intuition</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-neural-language-model-primer.html">Neural Language Model: A Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-vec-embedding.html">Word Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequence Models, Attention, Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dl-attention-transformer-intuition.html">Attention and Transformers: Intuitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl-seq-to-seq-attention-addition.html">Sequence Model with Attention for Addition Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain-llm-intro.html">Large Language Model (Under Construction…)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1-python-basics.html">1. Assignment I: Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2-preprocessing.html">2. Assignment II: Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3-chinese-nlp.html">3. Assignment III: Chinese Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4-text-vectorization.html">4. Assignment IV: Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5-machine-learning.html">5. Assignment V: Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6-topic-modeling.html">6. Assignment VI: Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7-dl-chinese-name-gender.html">7. Assignment VII: Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/8-sentiment-analysis-dl.html">8. Assignment VIII: Sentiment Analysis Using Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exams</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/midterm-exam-112.html">Midterm Exam</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-neural-network-from-scratch.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nlp/dl-neural-network-from-scratch.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural Network From Scratch</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-of-neural-network">Workflow of Neural Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-overview">Neural Network Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation">Backward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons">Neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions-in-python">Activation Functions in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-nodes-to-layers">From Nodes to Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-parameters-and-matrix-mutiplication-self-study">Layer, Parameters, and Matrix Mutiplication (Self-Study)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-neural-networks">Types of Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-neural-network-model-in-python">Building a Neural Network Model in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-in-focus">Packages in Focus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model">Create Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-modules-in-keras">Useful Modules in keras</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-utils"><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-layers"><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-modules">Other modules</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-example">A Quick Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pseudo-data">Pseudo Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-and-prediction">Evaluation and Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-neural-network-learn-the-parameters">How does the neural network learn the parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principles-for-loss-functions">Principles for Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-variables-encoding">Categorical Variables Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-wrong-with-categorical-variables">What’s wrong with categorical variables?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-ways-of-encodings">Three Ways of Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-and-loss-function">One-hot Encoding and Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-loss-errors-skipped">Examples of Loss Errors (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-based-on-one-sample">Error based on One Sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#errors-based-on-batch-samples">Errors based on Batch Samples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-adjust-the-model-parameters">How do we adjust the model parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-gradients-an-example-skipped">Intuition of Gradients: An Example (skipped)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent">Types of Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-in-python-skipped">Gradients in Python (skipped)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting">Overfitting and Underfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-vs-generalization">Optimization vs. Generalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-underfitting">Overfitting/Underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-underfitting">How to Deal with Underfitting?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-overfitting">How to Deal With Overfitting?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristics-for-regularization">Heuristics for Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization-and-layer-normalization">Batch Normalization and Layer Normalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-math-skipped">Some Math (Skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-and-back-propagation">Chain Rule and Back Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementwise-operations-of-matrix">Elementwise Operations of Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcast">Broadcast</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production">Matrices Dot Production</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production-and-forward-propagation">Matrices Dot Production and Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives">Derivatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives">Partial Derivatives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-network-from-scratch">
<h1><a class="toc-backref" href="#id2">Neural Network From Scratch</a><a class="headerlink" href="#neural-network-from-scratch" title="Permalink to this headline">#</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#neural-network-from-scratch" id="id2">Neural Network From Scratch</a></p>
<ul>
<li><p><a class="reference internal" href="#workflow-of-neural-network" id="id3">Workflow of Neural Network</a></p></li>
<li><p><a class="reference internal" href="#neural-network-overview" id="id4">Neural Network Overview</a></p>
<ul>
<li><p><a class="reference internal" href="#deep-learning" id="id5">Deep Learning</a></p></li>
<li><p><a class="reference internal" href="#forward-propagation" id="id6">Forward Propagation</a></p></li>
<li><p><a class="reference internal" href="#backward-propagation" id="id7">Backward Propagation</a></p></li>
<li><p><a class="reference internal" href="#neurons" id="id8">Neurons</a></p></li>
<li><p><a class="reference internal" href="#activation-functions" id="id9">Activation Functions</a></p></li>
<li><p><a class="reference internal" href="#activation-functions-in-python" id="id10">Activation Functions in Python</a></p></li>
<li><p><a class="reference internal" href="#from-nodes-to-layers" id="id11">From Nodes to Layers</a></p></li>
<li><p><a class="reference internal" href="#layer-parameters-and-matrix-mutiplication-self-study" id="id12">Layer, Parameters, and Matrix Mutiplication (Self-Study)</a></p></li>
<li><p><a class="reference internal" href="#types-of-neural-networks" id="id13">Types of Neural Networks</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#building-a-neural-network-model-in-python" id="id14">Building a Neural Network Model in Python</a></p>
<ul>
<li><p><a class="reference internal" href="#packages-in-focus" id="id15">Packages in Focus</a></p></li>
<li><p><a class="reference internal" href="#create-model" id="id16">Create Model</a></p></li>
<li><p><a class="reference internal" href="#useful-modules-in-keras" id="id17">Useful Modules in keras</a></p>
<ul>
<li><p><a class="reference internal" href="#keras-utils" id="id18"><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code></a></p></li>
<li><p><a class="reference internal" href="#keras-layers" id="id19"><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code></a></p></li>
<li><p><a class="reference internal" href="#other-modules" id="id20">Other modules</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#a-quick-example" id="id21">A Quick Example</a></p>
<ul>
<li><p><a class="reference internal" href="#pseudo-data" id="id22">Pseudo Data</a></p></li>
<li><p><a class="reference internal" href="#model-training" id="id23">Model Training</a></p></li>
<li><p><a class="reference internal" href="#evaluation-and-prediction" id="id24">Evaluation and Prediction</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#loss-functions" id="id25">Loss Functions</a></p>
<ul>
<li><p><a class="reference internal" href="#how-does-the-neural-network-learn-the-parameters" id="id26">How does the neural network learn the parameters?</a></p></li>
<li><p><a class="reference internal" href="#id1" id="id27">Loss Functions</a></p></li>
<li><p><a class="reference internal" href="#principles-for-loss-functions" id="id28">Principles for Loss Functions</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#categorical-variables-encoding" id="id29">Categorical Variables Encoding</a></p>
<ul>
<li><p><a class="reference internal" href="#what-s-wrong-with-categorical-variables" id="id30">What’s wrong with categorical variables?</a></p></li>
<li><p><a class="reference internal" href="#three-ways-of-encodings" id="id31">Three Ways of Encodings</a></p></li>
<li><p><a class="reference internal" href="#steps" id="id32">Steps</a></p></li>
<li><p><a class="reference internal" href="#one-hot-encoding-and-loss-function" id="id33">One-hot Encoding and Loss Function</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#examples-of-loss-errors-skipped" id="id34">Examples of Loss Errors (skipped)</a></p>
<ul>
<li><p><a class="reference internal" href="#error-based-on-one-sample" id="id35">Error based on One Sample</a></p></li>
<li><p><a class="reference internal" href="#errors-based-on-batch-samples" id="id36">Errors based on Batch Samples</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#gradient-descent" id="id37">Gradient Descent</a></p>
<ul>
<li><p><a class="reference internal" href="#how-do-we-adjust-the-model-parameters" id="id38">How do we adjust the model parameters?</a></p></li>
<li><p><a class="reference internal" href="#gradients" id="id39">Gradients</a></p></li>
<li><p><a class="reference internal" href="#intuition-of-gradients-an-example-skipped" id="id40">Intuition of Gradients: An Example (skipped)</a></p></li>
<li><p><a class="reference internal" href="#types-of-gradient-descent" id="id41">Types of Gradient Descent</a></p></li>
<li><p><a class="reference internal" href="#gradients-in-python-skipped" id="id42">Gradients in Python (skipped)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#overfitting-and-underfitting" id="id43">Overfitting and Underfitting</a></p>
<ul>
<li><p><a class="reference internal" href="#optimization-vs-generalization" id="id44">Optimization vs. Generalization</a></p></li>
<li><p><a class="reference internal" href="#overfitting-underfitting" id="id45">Overfitting/Underfitting</a></p></li>
<li><p><a class="reference internal" href="#how-to-deal-with-underfitting" id="id46">How to Deal with Underfitting?</a></p></li>
<li><p><a class="reference internal" href="#how-to-deal-with-overfitting" id="id47">How to Deal With Overfitting?</a></p></li>
<li><p><a class="reference internal" href="#regularization" id="id48">Regularization</a></p></li>
<li><p><a class="reference internal" href="#dropout" id="id49">Dropout</a></p></li>
<li><p><a class="reference internal" href="#heuristics-for-regularization" id="id50">Heuristics for Regularization</a></p></li>
<li><p><a class="reference internal" href="#normalization" id="id51">Normalization</a></p></li>
<li><p><a class="reference internal" href="#batch-normalization-and-layer-normalization" id="id52">Batch Normalization and Layer Normalization</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#some-math-skipped" id="id53">Some Math (Skipped)</a></p>
<ul>
<li><p><a class="reference internal" href="#chain-rule-and-back-propagation" id="id54">Chain Rule and Back Propagation</a></p></li>
<li><p><a class="reference internal" href="#elementwise-operations-of-matrix" id="id55">Elementwise Operations of Matrix</a></p></li>
<li><p><a class="reference internal" href="#broadcast" id="id56">Broadcast</a></p></li>
<li><p><a class="reference internal" href="#matrices-dot-production" id="id57">Matrices Dot Production</a></p></li>
<li><p><a class="reference internal" href="#matrices-dot-production-and-forward-propagation" id="id58">Matrices Dot Production and Forward Propagation</a></p></li>
<li><p><a class="reference internal" href="#derivatives" id="id59">Derivatives</a></p></li>
<li><p><a class="reference internal" href="#partial-derivatives" id="id60">Partial Derivatives</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id61">References</a></p></li>
</ul>
</li>
</ul>
</div>
<ul class="simple">
<li><p>This section provides an introduction to the fundamentals of neural network mechanisms, specifically within the context of deep learning.</p></li>
</ul>
<ul class="simple">
<li><p>We will break our discussions into three main parts:</p>
<ul>
<li><p>Building A Neural Network (How does the network work?)</p>
<ul>
<li><p>Forward Propagation</p></li>
<li><p>Weights, Biases, and Activation functions</p></li>
<li><p>Matrix multiplication</p></li>
</ul>
</li>
<li><p>Learning and Training (How does it learn?)</p>
<ul>
<li><p>Loss Function</p></li>
<li><p>Gradients</p></li>
<li><p>Derivatives and Partial Derivatives</p></li>
<li><p>Gradient Descent</p></li>
</ul>
</li>
<li><p>Gradient Descent (More on how does it learn.)</p>
<ul>
<li><p>Batch</p></li>
<li><p>Mini-batch</p></li>
<li><p>Stochastic gradient descent</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Loading Dependencies</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<section id="workflow-of-neural-network">
<h2><a class="toc-backref" href="#id3">Workflow of Neural Network</a><a class="headerlink" href="#workflow-of-neural-network" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/nn-flowchart.png" /></p>
</section>
<section id="neural-network-overview">
<h2><a class="toc-backref" href="#id4">Neural Network Overview</a><a class="headerlink" href="#neural-network-overview" title="Permalink to this headline">#</a></h2>
<section id="deep-learning">
<h3><a class="toc-backref" href="#id5">Deep Learning</a><a class="headerlink" href="#deep-learning" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/neural-network-propagation.gif" /></p>
</section>
<section id="forward-propagation">
<h3><a class="toc-backref" href="#id6">Forward Propagation</a><a class="headerlink" href="#forward-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural network is a type of machine learning algorithm modeled on human brains and nervous system.</p></li>
<li><p>The model is believed to process information in a similar way to the human brain:</p>
<ul>
<li><p>inputs and weights as the <strong>dendrites</strong></p></li>
<li><p>neuron operations of weighted sums and activation as <strong>neurons</strong></p></li>
<li><p>outputs as <strong>axons</strong></p></li>
</ul>
</li>
<li><p>A neural network often consists of a large number of elements, known as <strong>nodes</strong>, working in parallel to solve a specific problem. These nodes are often organized into different <strong>layers</strong>.</p></li>
<li><p>Each layer of the network transforms the input values into the output values based on the weights (parameters) of the nodes.</p></li>
<li><p>The data transformation from the input to the output is in general referred to as <strong>forward propagation</strong> of the network.</p></li>
</ul>
</section>
<section id="backward-propagation">
<h3><a class="toc-backref" href="#id7">Backward Propagation</a><a class="headerlink" href="#backward-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>When the predicted output is compared with the true label, we can evaluate the network performance by computing the <strong>loss</strong> of the network.</p></li>
<li><p>Then we determine the proportion of the losses that may be attributed to each model parameter. This process goes from the losses of the predicted output backward to the original inputs. This step is referred to as the <strong>back propagation</strong> of the network.</p></li>
</ul>
</section>
<section id="neurons">
<h3><a class="toc-backref" href="#id8">Neurons</a><a class="headerlink" href="#neurons" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural network consists of neurons, which allow us to model non-linear relationships between input and output data.</p></li>
<li><p>Given an input vector, traditional linear transformation can only model a linear relationship between X and y:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\hat{y} = w_0 + w_1 x_1 + w_2x_2 + w_3x_3 +...+w_nx_n
\]</div>
<ul class="simple">
<li><p>A neron is like a linear transformation but with an extra <strong>activation function</strong>.</p></li>
<li><p>This mechanism of activation function in each neuron will ultimately determine the output of the neuron.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\textit{Neuron Output Value} &amp; =  h(\hat{y}) \\
&amp; = h(w_0 + w_1 x_1 + w_2x_2 + w_3x_3 +...+w_nx_n)
\end{align}\end{split}\]</div>
<p><img alt="" src="../_images/neuron.png" /></p>
</section>
<section id="activation-functions">
<h3><a class="toc-backref" href="#id9">Activation Functions</a><a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In neural network, the activation function of a node determines whether the node would activate the output given the <strong>weighted sum of the input values</strong>.</p></li>
<li><p>Different types of activation functions may determine the cut-offs for output activation in different ways.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Sigmoid</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into values within the range of 0 and 1 (i.e., a probability-like value).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ h(y) = \frac{1}{1 + \exp(-y)}\]</div>
<ul class="simple">
<li><p><strong>Step</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into binary ones, with only the positive values activated.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(y)= \left\{ 
     \begin{array}\\
     0 &amp; (y \leq 0) \\
     1 &amp; (y &gt; 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p><strong>ReLU</strong> (Rectified Linear Unit) function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values by passing only positive values and zero for negative <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(y)= \left\{ 
     \begin{array}\\
     y &amp; (y &gt; 0) \\
     0 &amp; (y \leq 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Softmax</strong> function: This function converts the <span class="math notranslate nohighlight">\(y\)</span> values into normalized probability values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h(y_k) = \frac{\exp(y_k)}{\sum_{i = 1}^{n} \exp({y_i})}
\]</div>
</section>
<section id="activation-functions-in-python">
<h3><a class="toc-backref" href="#id10">Activation Functions in Python</a><a class="headerlink" href="#activation-functions-in-python" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="c1"># def softmax(x):</span>
<span class="c1">#     exp_x = np.exp(x)</span>
<span class="c1">#     sum_exp_x = np.sum(exp_x)</span>
<span class="c1">#     y = exp_x/sum_exp_x</span>
<span class="c1">#     return y</span>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>  <span class="c1"># avoid overflow issues</span>
    <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">sum_exp_x</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/249f766bf4d74a963a5e3cc97a966451cb4c2fd7384220862a299df80f9bc51e.png" src="../_images/249f766bf4d74a963a5e3cc97a966451cb4c2fd7384220862a299df80f9bc51e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## sigmoid function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/33a74037b6fb32335a1886afc2a94757a9179fbc9ce89fc187b8f9aaf4c3ce46.png" src="../_images/33a74037b6fb32335a1886afc2a94757a9179fbc9ce89fc187b8f9aaf4c3ce46.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ReLU</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/28fe0dc3c9d5d2cea82040e9656db0a1d85cb6d654a9100bba3709101c15ebe1.png" src="../_images/28fe0dc3c9d5d2cea82040e9656db0a1d85cb6d654a9100bba3709101c15ebe1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.   0.01 0.05 0.95]
</pre></div>
</div>
</div>
</div>
</section>
<section id="from-nodes-to-layers">
<h3><a class="toc-backref" href="#id11">From Nodes to Layers</a><a class="headerlink" href="#from-nodes-to-layers" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A neural network can be defined in terms of <strong>depths</strong> and <strong>widths</strong> of its layers.</p>
<ul>
<li><p><strong>Depth</strong>: How many layers does the network have?</p></li>
<li><p><strong>Width</strong>: How many neurons does each layer have?</p></li>
</ul>
</li>
<li><p>A network can consist of several layers.</p></li>
<li><p>Each layer can have various numbers of neurons.</p></li>
<li><p>For each layer, the shape of the input tensor, the number of its neurons, and the shape of its output are inter-connected. These settings will determine the number of parameters (i.e., <strong>weights</strong>) needed to train.</p></li>
</ul>
<p><img alt="" src="../_images/neural-network-dense-layer.gif" /></p>
</section>
<section id="layer-parameters-and-matrix-mutiplication-self-study">
<h3><a class="toc-backref" href="#id12">Layer, Parameters, and Matrix Mutiplication (Self-Study)</a><a class="headerlink" href="#layer-parameters-and-matrix-mutiplication-self-study" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Each layer transforms the input values into the output values based on its layer parameters.</p></li>
<li><p>Mathematically, these values transformation is a matrix multiplication, running in parallel for all nodes of the layer.</p></li>
<li><p>In Deep Learning, the input and output values are represented as a multi-dimensional tensor.</p>
<ul>
<li><p>A 1D tensor is a vector.</p></li>
<li><p>A 2D tensor is a two-dimensional array.</p></li>
<li><p>A 3D tensor is a three-dimensional array.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/neural-network-dense-layer-1.gif" /></p>
<p><img alt="" src="../_images/neural-network-dense-layer-2.gif" /></p>
</section>
<section id="types-of-neural-networks">
<h3><a class="toc-backref" href="#id13">Types of Neural Networks</a><a class="headerlink" href="#types-of-neural-networks" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Multi-Layer Perceptron</strong> (Fully Connected Network)</p>
<ul>
<li><p>Input Layer, one or more hidden layers, and output layer.</p></li>
<li><p>A hidden layer consists of neurons (perceptrons) which process certain aspect of the features and send the processed information into the next hidden layer.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Convolutional Neural Network (CNN)</strong></p>
<ul>
<li><p>Mainly for image and audio processing</p></li>
<li><p>Convolution Layer, Pooling Layer, Fully Connected Layer</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Sequence Models</strong></p>
<ul>
<li><p>A sequence model is a type of machine learning model designed to handle sequential data, where the order of elements matters. In a sequence model, input data is processed in a sequential manner, and the model makes predictions or decisions based on the sequence of input elements.</p></li>
<li><p>Common variants: Recurrent Neural Network (RNN), Long Short-Term Memory Networks (LSTM), Gate Recurrent Units (GRU), Transformers</p></li>
<li><p><strong>Preferred methods in NLP</strong>: Sequence models are widely used in NLP tasks due to their ability to handle sequential data, such as text. They are well-suited for tasks like language modeling, sentiment analysis, machine translation, and text generation.</p></li>
<li><p><strong>Memory of previous steps</strong>: Unlike other fully-connected networks, which process each input independently, RNNs have the ability to retain information about previous steps in the sequence. This allows them to learn from the context of the data and make decisions based on this context. This capability is crucial for tasks where the current input’s meaning depends on the sequence of previous inputs, such as in language understanding.</p></li>
<li><p><strong>Effective in dealing with time-series data</strong>: Sequence models excel in processing time-series data, where the order of data points matters. In the context of NLP, text can be viewed as a sequence of words or characters, and sequence models can effectively capture the dependencies between these elements. This makes them well-suited for tasks like language modeling, where predicting the next word in a sentence depends on the words that came before it.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/s2s-rnn.jpeg" /></p>
</section>
</section>
<section id="building-a-neural-network-model-in-python">
<h2><a class="toc-backref" href="#id14">Building a Neural Network Model in Python</a><a class="headerlink" href="#building-a-neural-network-model-in-python" title="Permalink to this headline">#</a></h2>
<section id="packages-in-focus">
<h3><a class="toc-backref" href="#id15">Packages in Focus</a><a class="headerlink" href="#packages-in-focus" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>: It is an open source machine learning library used for numerical computational tasks developed by Google.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code>: It is a high level API built on top of Tensorflow. It originated as an independent library and now has been incorporated as part of Tensorflow 2+.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras</span> <span class="pre">3.0</span></code>: Now Keras 3.0 is a full rewrite of Keras that enables you to run Keras workflows on top of several popular deep learning packages, such as JAX, TensorFlow, or PyTorch.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Moving from Keras 2 to Keras 3 is generally smooth and backward compatible, with most users not needing to modify their code. However, in this course, we will stay with keras 2 for the commment.</p>
<p>If accessing Keras via <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> in TensorFlow, there are no immediate changes until TensorFlow 2.16. From TensorFlow 2.16 onwards, Keras 3 is the default.</p>
<p>If you are using TensorFlow &gt; 2.15, to continue using Keras 2, install <code class="docutils literal notranslate"><span class="pre">tf_keras</span></code> and set the environment variable <code class="docutils literal notranslate"><span class="pre">TF_USE_LEGACY_KERAS=1</span></code>. This will direct TensorFlow to resolve <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> to the locally-installed <code class="docutils literal notranslate"><span class="pre">tf_keras</span></code> package. However, be cautious as this change affects any package importing <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> in the Python process. To ensure changes only impact your own code, use the <code class="docutils literal notranslate"><span class="pre">tf_keras</span></code> package.</p>
</div>
<ul class="simple">
<li><p>Usually we need to define the architecture of the neural network model in terms of <strong>depths</strong> and <strong>widths</strong> of the layers.</p></li>
<li><p>After we define the structure of the network and initialize the values for all parameters, the training requires an iterative processing involving:</p>
<ul>
<li><p><strong>Forward Propagation</strong>: It refers to the process of transforming the data values by moving the input data through the network to get output.</p></li>
<li><p>Define your <strong>loss function</strong>.</p></li>
<li><p>Calculate <strong>Total Error</strong> based on the loss function.</p></li>
<li><p>Calculate <strong>Gradients</strong> via <strong>Back Propagation</strong></p></li>
<li><p>Update the <strong>weights</strong> based on gradients.</p></li>
<li><p>Iterate the process until the stop-condition is reached.</p></li>
</ul>
</li>
</ul>
</section>
<section id="create-model">
<h3><a class="toc-backref" href="#id16">Create Model</a><a class="headerlink" href="#create-model" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">tensorflow.keras</span></code>, we can create models in two ways:</p>
<ul>
<li><p>Sequential API (<code class="docutils literal notranslate"><span class="pre">keras.Sequential</span></code>)</p></li>
<li><p>Functional API (<code class="docutils literal notranslate"><span class="pre">keras.model</span></code>)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.13.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Sequential API to create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_1&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_2&quot;</span><span class="p">))</span>

<span class="c1"># Sequential API (Another alternative)</span>
<span class="c1"># model = keras.Sequential(</span>
<span class="c1">#     [</span>
<span class="c1">#         keras.Input(shape=(2)),</span>
<span class="c1">#         layers.Dense(4, activation=&quot;relu&quot;),</span>
<span class="c1">#         layers.Dense(2, activation=&quot;relu&quot;)</span>
<span class="c1">#     ]</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Functional API (A bit more flexible)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_1&quot;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense_layer_2&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you create the model using the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> method, you will not be able to check the <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> or <code class="docutils literal notranslate"><span class="pre">plot_model()</span></code> until the model has been compiled and fitted.</p>
</div>
<ul class="simple">
<li><p>Two ways to inspect the model architecture:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model.summary()</span></code>: A printed summary of the model structure</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow.keras.utils.plot_model(model)</span></code>: A visual representation of the model structure</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 2)]               0         
                                                                 
 dense_layer_1 (Dense)       (None, 4)                 12        
                                                                 
 dense_layer_2 (Dense)       (None, 2)                 10        
                                                                 
=================================================================
Total params: 22 (88.00 Byte)
Trainable params: 22 (88.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_layer_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7d9e5304684551a38480b6f01a821dc6a8000a87f9c66ceb24b847713bd869cc.png" src="../_images/7d9e5304684551a38480b6f01a821dc6a8000a87f9c66ceb24b847713bd869cc.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you see error messages when running the <code class="docutils literal notranslate"><span class="pre">plot_model()</span></code>, please follow the instructions provided in the error messages and install:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pydot</span></code></p></li>
<li><p>Install <a class="reference external" href="https://graphviz.gitlab.io/download/">graphviz</a> (choose the installation methods depending on your OS).</p></li>
</ul>
</div>
</section>
<section id="useful-modules-in-keras">
<h3><a class="toc-backref" href="#id17">Useful Modules in keras</a><a class="headerlink" href="#useful-modules-in-keras" title="Permalink to this headline">#</a></h3>
<section id="keras-utils">
<h4><a class="toc-backref" href="#id18"><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code></a><a class="headerlink" href="#keras-utils" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">to_categorical()</span></code>: To convert a class/label list into a one-hot encoding matrix</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plot_model()</span></code>: Plot the model structure</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Text Labels to Sequences</span>
<span class="n">labels_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1, 1, 0, 0, 2, 2]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Sequences to One-Hot Encoding</span>
<span class="n">to_categorical</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [0., 0., 1.],
       [0., 0., 1.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="keras-layers">
<h4><a class="toc-backref" href="#id19"><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code></a><a class="headerlink" href="#keras-layers" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Base layers: <code class="docutils literal notranslate"><span class="pre">Input</span></code>, <code class="docutils literal notranslate"><span class="pre">Dense</span></code>, <code class="docutils literal notranslate"><span class="pre">Embedding</span></code></p></li>
<li><p>Pooling layers: <code class="docutils literal notranslate"><span class="pre">MaxPooling</span></code>, <code class="docutils literal notranslate"><span class="pre">AveragePooling</span></code>, <code class="docutils literal notranslate"><span class="pre">GlobalMaxPooling</span></code>, <code class="docutils literal notranslate"><span class="pre">GlobalAveragePooling</span></code></p></li>
<li><p>Recurrent layers: <code class="docutils literal notranslate"><span class="pre">SimpleRNN</span></code>, <code class="docutils literal notranslate"><span class="pre">LSTM</span></code>, <code class="docutils literal notranslate"><span class="pre">GRU</span></code>, <code class="docutils literal notranslate"><span class="pre">Bidirectional</span></code></p></li>
<li><p>Regularization layers: <code class="docutils literal notranslate"><span class="pre">Dropout</span></code></p></li>
<li><p>Attention layers: <code class="docutils literal notranslate"><span class="pre">Attention</span></code>, <code class="docutils literal notranslate"><span class="pre">AdditiveAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code></p></li>
<li><p>Reshaping: <code class="docutils literal notranslate"><span class="pre">Flatten</span></code></p></li>
<li><p>Merging: <code class="docutils literal notranslate"><span class="pre">Concatenate</span></code></p></li>
</ul>
</section>
<section id="other-modules">
<h4><a class="toc-backref" href="#id20">Other modules</a><a class="headerlink" href="#other-modules" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">keras.losses</span></code>: Loss functions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers</span></code>: Optimization is an important process which optimizes the input weights by comparing the prediction and the loss function.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers.RMSprop</span></code>: RMSprop is an optimization algorithm that uses a moving average of squared gradients to normalize the gradient updates.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimizers.Adam</span></code>: Adam is an optimization algorithm that adapts the learning rate based on the historical gradient information.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.optimziers.SGD</span></code>: Stochastic Gradient Descent (SGD) is a simple optimization algorithm that updates the parameters by computing the gradient of the loss function with respect to the parameters.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">keras.metrics</span></code>: Metrics for model evaluation</p></li>
</ul>
</section>
</section>
</section>
<section id="a-quick-example">
<h2><a class="toc-backref" href="#id21">A Quick Example</a><a class="headerlink" href="#a-quick-example" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../_images/keras-workflow.png" /></p>
<ul class="simple">
<li><p>Steps for Model Training</p>
<ul>
<li><p>Create model</p></li>
<li><p>Compile model</p></li>
<li><p>Fit model</p></li>
<li><p>Evaluate model</p></li>
<li><p>Predict</p></li>
</ul>
</li>
</ul>
<section id="pseudo-data">
<h3><a class="toc-backref" href="#id22">Pseudo Data</a><a class="headerlink" href="#pseudo-data" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>We create random samples, where each sample is characterized by two random numbers (e.g., grades in subjects)</p></li>
<li><p>Each sample is also labeled with a binary class label (e.g., <code class="docutils literal notranslate"><span class="pre">fail</span> <span class="pre">=</span> <span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">pass</span> <span class="pre">=</span> <span class="pre">1</span></code>)</p></li>
<li><p>We create three datasets: <strong>training</strong>, <strong>validation</strong>, and <strong>testing</strong> sets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># train set</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># val set</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># test set</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100, 2)
(100,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.69646919 0.28613933]
 [0.22685145 0.55131477]
 [0.71946897 0.42310646]
 [0.9807642  0.68482974]
 [0.4809319  0.39211752]]
[1 1 0 1 0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-training">
<h3><a class="toc-backref" href="#id23">Model Training</a><a class="headerlink" href="#model-training" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compile model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>epochs</strong>: the number of times that the entire training set is passed forward and backward through the neural network.</p></li>
<li><p><strong>batch_size</strong>: the number of samples that are used for parameter updates each time (i.e., passed through the network forward and backward)</p></li>
</ul>
<p>If we specify <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">=</span> <span class="pre">10</span></code> and <code class="docutils literal notranslate"><span class="pre">epochs=50</span></code>, the model will try to train the model parameters (i.e., all the weights in the layers) by running through the entire training set <strong>50</strong> times (epochs, or iterations).</p>
<p>And in each epoch, the model will update the model parameters according to the average loss values based on a batch of <strong>10</strong> samples.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10/10 [==============================] - 0s 8ms/step - loss: 0.7034 - accuracy: 0.4800 - val_loss: 0.6988 - val_accuracy: 0.4700
Epoch 2/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6988 - accuracy: 0.4700 - val_loss: 0.6963 - val_accuracy: 0.4400
Epoch 3/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6953 - accuracy: 0.4400 - val_loss: 0.6939 - val_accuracy: 0.5100
Epoch 4/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5700 - val_loss: 0.6928 - val_accuracy: 0.5200
Epoch 5/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5400
Epoch 6/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.5200 - val_loss: 0.6920 - val_accuracy: 0.5400
Epoch 7/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.5200 - val_loss: 0.6922 - val_accuracy: 0.5400
Epoch 8/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.5300 - val_loss: 0.6925 - val_accuracy: 0.5400
Epoch 9/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.5200 - val_loss: 0.6926 - val_accuracy: 0.5400
Epoch 10/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.5200 - val_loss: 0.6930 - val_accuracy: 0.5300
Epoch 11/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6817 - accuracy: 0.5300 - val_loss: 0.6935 - val_accuracy: 0.5200
Epoch 12/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6803 - accuracy: 0.5800 - val_loss: 0.6939 - val_accuracy: 0.5000
Epoch 13/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6797 - accuracy: 0.5700 - val_loss: 0.6945 - val_accuracy: 0.4900
Epoch 14/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6782 - accuracy: 0.5900 - val_loss: 0.6950 - val_accuracy: 0.4900
Epoch 15/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.5800 - val_loss: 0.6955 - val_accuracy: 0.5000
Epoch 16/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.5900 - val_loss: 0.6963 - val_accuracy: 0.4900
Epoch 17/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6751 - accuracy: 0.6100 - val_loss: 0.6967 - val_accuracy: 0.4900
Epoch 18/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.6200 - val_loss: 0.6973 - val_accuracy: 0.5000
Epoch 19/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6737 - accuracy: 0.6400 - val_loss: 0.6980 - val_accuracy: 0.5000
Epoch 20/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6726 - accuracy: 0.6300 - val_loss: 0.6985 - val_accuracy: 0.4900
Epoch 21/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.6400 - val_loss: 0.6996 - val_accuracy: 0.5000
Epoch 22/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.6400 - val_loss: 0.7002 - val_accuracy: 0.5000
Epoch 23/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.6400 - val_loss: 0.7007 - val_accuracy: 0.4900
Epoch 24/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.6300 - val_loss: 0.7017 - val_accuracy: 0.4900
Epoch 25/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6683 - accuracy: 0.6300 - val_loss: 0.7023 - val_accuracy: 0.5000
Epoch 26/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6400 - val_loss: 0.7030 - val_accuracy: 0.4900
Epoch 27/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6669 - accuracy: 0.6500 - val_loss: 0.7038 - val_accuracy: 0.5000
Epoch 28/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6660 - accuracy: 0.6400 - val_loss: 0.7044 - val_accuracy: 0.4900
Epoch 29/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.6400 - val_loss: 0.7049 - val_accuracy: 0.5000
Epoch 30/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.6400 - val_loss: 0.7059 - val_accuracy: 0.5000
Epoch 31/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6642 - accuracy: 0.6700 - val_loss: 0.7063 - val_accuracy: 0.5100
Epoch 32/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.6700 - val_loss: 0.7072 - val_accuracy: 0.5100
Epoch 33/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6628 - accuracy: 0.6900 - val_loss: 0.7081 - val_accuracy: 0.5000
Epoch 34/50
10/10 [==============================] - 0s 1ms/step - loss: 0.6612 - accuracy: 0.6700 - val_loss: 0.7087 - val_accuracy: 0.5100
Epoch 35/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.6800 - val_loss: 0.7093 - val_accuracy: 0.5000
Epoch 36/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6600 - accuracy: 0.6800 - val_loss: 0.7100 - val_accuracy: 0.5000
Epoch 37/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.6700 - val_loss: 0.7113 - val_accuracy: 0.5000
Epoch 38/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6700 - val_loss: 0.7120 - val_accuracy: 0.5100
Epoch 39/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.6800 - val_loss: 0.7125 - val_accuracy: 0.5100
Epoch 40/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.6700 - val_loss: 0.7133 - val_accuracy: 0.5200
Epoch 41/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.6700 - val_loss: 0.7140 - val_accuracy: 0.5100
Epoch 42/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.6700 - val_loss: 0.7148 - val_accuracy: 0.5000
Epoch 43/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.6700 - val_loss: 0.7160 - val_accuracy: 0.5000
Epoch 44/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.6700 - val_loss: 0.7168 - val_accuracy: 0.5200
Epoch 45/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.6700 - val_loss: 0.7177 - val_accuracy: 0.5000
Epoch 46/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.6700 - val_loss: 0.7185 - val_accuracy: 0.5000
Epoch 47/50
10/10 [==============================] - 0s 1ms/step - loss: 0.6530 - accuracy: 0.6700 - val_loss: 0.7196 - val_accuracy: 0.5100
Epoch 48/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6525 - accuracy: 0.6800 - val_loss: 0.7201 - val_accuracy: 0.5100
Epoch 49/50
10/10 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.6900 - val_loss: 0.7204 - val_accuracy: 0.5200
Epoch 50/50
10/10 [==============================] - 0s 1ms/step - loss: 0.6513 - accuracy: 0.6700 - val_loss: 0.7215 - val_accuracy: 0.5200
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_6 (Dense)             (10, 16)                  48        
                                                                 
 dense_7 (Dense)             (10, 16)                  272       
                                                                 
 dense_8 (Dense)             (10, 1)                   17        
                                                                 
=================================================================
Total params: 337 (1.32 KB)
Trainable params: 337 (1.32 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_layer_activations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/614132672404f258ad9630be8c4567c8e24416dda2fb2c2d6909bbea6068f5e6.png" src="../_images/614132672404f258ad9630be8c4567c8e24416dda2fb2c2d6909bbea6068f5e6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot accuracy changes from model.fit()</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dd0d2ff0da2fd0d6b24469693dd8686e61c923d7991980ca2e6c03fdc7063817.png" src="../_images/dd0d2ff0da2fd0d6b24469693dd8686e61c923d7991980ca2e6c03fdc7063817.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot loss changes from model.fit()</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ed1fef1a6a2aa59baef66bbe4c679814301715407c41a24a9c59ba244367ee14.png" src="../_images/ed1fef1a6a2aa59baef66bbe4c679814301715407c41a24a9c59ba244367ee14.png" />
</div>
</div>
</section>
<section id="evaluation-and-prediction">
<h3><a class="toc-backref" href="#id24">Evaluation and Prediction</a><a class="headerlink" href="#evaluation-and-prediction" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Evaludate</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 874us/step - loss: 0.7519 - accuracy: 0.4200
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.7519487738609314, 0.41999998688697815]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Predict</span>

<span class="c1">## generate pseudo test data</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1">## check</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 9ms/step
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.535356  ],
       [0.5599342 ],
       [0.6352474 ],
       [0.38696763],
       [0.4239076 ]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="loss-functions">
<h2><a class="toc-backref" href="#id25">Loss Functions</a><a class="headerlink" href="#loss-functions" title="Permalink to this headline">#</a></h2>
<section id="how-does-the-neural-network-learn-the-parameters">
<h3><a class="toc-backref" href="#id26">How does the neural network learn the parameters?</a><a class="headerlink" href="#how-does-the-neural-network-learn-the-parameters" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Forward propagation shows how the network takes the input values, transforms them, and produces the predicted output values based on the network parameters (i.e., weights).</p></li>
<li><p>The network needs to learn the weights that best produce the output values according to some <strong>loss function</strong>, i.e., how different is the prediction from the true label?</p></li>
<li><p>Crucially, we need to compute the differences between the <strong>predicted</strong> outputs of the network and the <strong>true</strong> target outputs.</p></li>
<li><p>The model should aim to minimize these differences, which are commonly referred to as <strong>errors</strong> of the model.</p></li>
</ul>
</section>
<section id="id1">
<h3><a class="toc-backref" href="#id27">Loss Functions</a><a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>If the target ouputs are numeric values, we can evaluate the errors (i.e., the differences between the actual model outputs and the target outputs) using the <strong>mean square error</strong> function.</p>
<ul>
<li><p>Mean Square Error: <span class="math notranslate nohighlight">\(E = \frac{1}{2}\sum(y_k - t_k)^2\)</span></p></li>
</ul>
</li>
<li><p>If the target outputs are labels, we can evaluate the errors (i.e., the differences between the actual model labels and the target labels) using the <strong>cross entory error</strong> function.</p>
<ul>
<li><p>Cross Entropy Error: <span class="math notranslate nohighlight">\(E= -\sum_{k}t_k\log(y_k)\)</span></p></li>
</ul>
</li>
<li><p>The function used to compute the errors of the model is referred to as the <strong>loss function</strong>.</p></li>
</ul>
</section>
<section id="principles-for-loss-functions">
<h3><a class="toc-backref" href="#id28">Principles for Loss Functions</a><a class="headerlink" href="#principles-for-loss-functions" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem Type</p></th>
<th class="head"><p>Last-Layer Activation</p></th>
<th class="head"><p>Loss Function in Keras</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">y</span></code> Encoding</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary Classification</p></td>
<td><p>sigmoid</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">binary_crossentropy()</span></code></p></td>
<td><p>Integer Encoding</p></td>
</tr>
<tr class="row-odd"><td><p>Binary/Multiclass Classification</p></td>
<td><p>softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy(from_logits=False)</span></code></p></td>
<td><p>Integer Encoding</p></td>
</tr>
<tr class="row-even"><td><p>Multiclass Classification</p></td>
<td><p>softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">categorical_crossentropy()</span></code></p></td>
<td><p>One-hot Encoding</p></td>
</tr>
<tr class="row-odd"><td><p>Regression</p></td>
<td><p>None</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mes</span></code></p></td>
<td><p>Floating-point</p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For multi-class classification, as long as the label is in <strong>integer</strong> encodings, then we use the <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code> for loss; if the label is in <strong>one-hot</strong> encodings, then we use the <code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code> for loss.</p>
<p>For more information about loss functions, please check <a class="reference external" href="https://keras.io/api/losses/">Keras Losses</a>.</p>
</div>
</section>
</section>
<section id="categorical-variables-encoding">
<h2><a class="toc-backref" href="#id29">Categorical Variables Encoding</a><a class="headerlink" href="#categorical-variables-encoding" title="Permalink to this headline">#</a></h2>
<section id="what-s-wrong-with-categorical-variables">
<h3><a class="toc-backref" href="#id30">What’s wrong with categorical variables?</a><a class="headerlink" href="#what-s-wrong-with-categorical-variables" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Before we look at the examples of loss computation, we need to talk about the ways of encoding <strong>categorical</strong> labels.</p></li>
<li><p>Remember, machine doesn’t understand characters. If we have class labels like male/female, positive/negative, or each word tokens, we may need to convert these strings into machine-readable numerical values.</p></li>
<li><p>This step is called <strong>encoding</strong>.</p></li>
<li><p>Most importantly, machine learning and deep learning both require input and output variables to be <strong>numbers</strong>.</p></li>
</ul>
</section>
<section id="three-ways-of-encodings">
<h3><a class="toc-backref" href="#id31">Three Ways of Encodings</a><a class="headerlink" href="#three-ways-of-encodings" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Integer</strong> (Sequence) Encoding: Each unique label is mapped to an integer</p></li>
<li><p><strong>One-hot</strong> Encoding: Each unique label is mapped to a binary vector</p></li>
<li><p><strong>Embeddings</strong> Encoding: Each unique label is mapped to a learned vectorized representation (i.e., embeddings)</p></li>
</ul>
</section>
<section id="steps">
<h3><a class="toc-backref" href="#id32">Steps</a><a class="headerlink" href="#steps" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Given a corpus, we can create its <strong>vocabulary</strong> (at the word-token level, or sometimes at the character level).</p></li>
<li><p>We can associate every word type with a <strong>unique integer index</strong> (i.e., integer encoding).</p></li>
<li><p>We can further turn this integer index <span class="math notranslate nohighlight">\(i\)</span> into a <strong>binary vector</strong> of size <span class="math notranslate nohighlight">\(N\)</span> (the size of vocabulary). The vector is all zeros except for the <span class="math notranslate nohighlight">\(i\)</span>th entry, which is 1 (i.e., one-hot encoding).</p></li>
<li><p>We can turn the integer index into a <strong>dense, low-dimensional floating-point vectors</strong> (i.e., embedding encoding).</p>
<ul>
<li><p>Learn embeddings jointly with the main task (i.e., <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer)</p></li>
<li><p>Use pretrained word embeddings that were precomputed using a different machine learning task.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Given Label</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span>

<span class="c1">## Create Dictionary</span>
<span class="n">labels_dict</span> <span class="o">=</span> <span class="p">{</span> <span class="n">x</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_dict</span><span class="p">)</span>

<span class="c1">## Integer Encoding</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>

<span class="c1">## One-hot Encoding</span>
<span class="n">labels_oh</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_oh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;female&#39;: 0, &#39;male&#39;: 1}
[1, 0, 0, 1]
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###########################################</span>
<span class="c1">## Integer Encoding and One-hot Encoding ##</span>
<span class="c1">## Using sklearn                         ##</span>
<span class="c1">###########################################</span>

<span class="c1">## Integer Encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">oe</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int16&#39;</span><span class="p">)</span>
<span class="n">labels_int</span> <span class="o">=</span> <span class="n">oe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels_int</span><span class="p">)</span>

<span class="c1">## One-hot Encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">ohe</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">labels_ohe</span> <span class="o">=</span> <span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">labels_ohe</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1]
 [0]
 [0]
 [1]]
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="one-hot-encoding-and-loss-function">
<h3><a class="toc-backref" href="#id33">One-hot Encoding and Loss Function</a><a class="headerlink" href="#one-hot-encoding-and-loss-function" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>With one-hot encoding, we can convert any categorical variable into a numeric vector.</p></li>
<li><p>And if we specify our network output to be a vector of the same size, we can compute the differences between the output vector and the true numeric vector.</p></li>
</ul>
<ul class="simple">
<li><p>When the target class label is a binary one (e.g., name-gender prediction):</p></li>
</ul>
<p><img alt="" src="../_images/neural-network-loss-binary.jpeg" /></p>
<ul class="simple">
<li><p>When the target class label is a multi-level one (e.g., next-word prediction):
<img alt="" src="../_images/neural-network-loss-multicat.jpeg" /></p></li>
</ul>
</section>
</section>
<section id="examples-of-loss-errors-skipped">
<h2><a class="toc-backref" href="#id34">Examples of Loss Errors (skipped)</a><a class="headerlink" href="#examples-of-loss-errors-skipped" title="Permalink to this headline">#</a></h2>
<section id="error-based-on-one-sample">
<h3><a class="toc-backref" href="#id35">Error based on One Sample</a><a class="headerlink" href="#error-based-on-one-sample" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The following is a simple example showing how to compute the loss for a case of prediction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_square_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c1"># avoid log(0)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## mean square error</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>  <span class="c1"># predicted values</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># true label</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mean_square_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.40625
2.302584092994546
</pre></div>
</div>
</div>
</div>
</section>
<section id="errors-based-on-batch-samples">
<h3><a class="toc-backref" href="#id36">Errors based on Batch Samples</a><a class="headerlink" href="#errors-based-on-batch-samples" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The following is a simple example showing how to compute the average loss of a sample of batch size N cases.</p></li>
</ul>
<ul class="simple">
<li><p>If the training is based on a sample of batch size <em>N</em>, we can compute the average loss (or total errors) of the batch sample:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ E = - \frac{1}{N}\sum_n\sum_k t_{nk}\log y_{nk}\]</div>
<ul class="simple">
<li><p>We can revise the <code class="docutils literal notranslate"><span class="pre">cross_entropy_error()</span></code> function to work with outputs from a min-batch sample.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># adjust the function to for batch sample outputs</span>
<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When the labels uses one-hot encoding, the function can be simplified as follows:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># because for one-hot labels</span>
    <span class="c1"># cross-entropy sums only the values of the true labels `1`</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="gradient-descent">
<h2><a class="toc-backref" href="#id37">Gradient Descent</a><a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h2>
<section id="how-do-we-adjust-the-model-parameters">
<h3><a class="toc-backref" href="#id38">How do we adjust the model parameters?</a><a class="headerlink" href="#how-do-we-adjust-the-model-parameters" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Now that we have the <strong>Loss Function</strong>, we’re ready for the crucial step in model training — tweaking the <strong>weights</strong> (or <strong>parameters</strong>) of the model.</p></li>
<li><p>Essentially, in neural network training, we want to understand how adjusting each <strong>parameter</strong> (like weights) impacts the overall <strong>Loss Function</strong>. In other words, we need to know: how much does changing a specific <strong>parameter</strong> affect the total error?</p></li>
<li><p>This understanding helps us identify the contribution of each parameter to the total error. This knowledge forms the foundation for making adjustments to the parameters.</p></li>
<li><p>Our goal is to tweak the weights in proportion to their impact on the error.</p></li>
<li><p>This optimization process of finding the best combination of weights to minimize the loss function is known as <strong>Gradient Descent</strong>.</p></li>
</ul>
</section>
<section id="gradients">
<h3><a class="toc-backref" href="#id39">Gradients</a><a class="headerlink" href="#gradients" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The error that a specific weight is responsible for is referred to as the <strong>gradient</strong> of the parameter.</p></li>
<li><p>Mathematically, the gradient of a weight is the <strong>partial derivative</strong> of a weight in relation to the <strong>loss function</strong>.</p></li>
<li><p>Then we adjust the weight in proportion to its gradient:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
{W}_{new} = W_{original} + \eta \times \textit{Gradient}_{W_{original}}
\]</div>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is a hyperparameter in deep learning. This parameter controls how fast the model learns. This <span class="math notranslate nohighlight">\(\eta\)</span> is referred to as the <strong>learning rates</strong>.</p></li>
<li><p>Common values for learning rates typically fall within the range of 0.1 to 0.0001.</p></li>
<li><p>It’s often recommended to start with a learning rate around 0.001 and then adjust it based on the performance of the model during training.</p></li>
</ul>
<div class="dropdown admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><strong>Derivative</strong>:</p>
<ul>
<li><p>A derivative measures how a function changes as its input changes.</p></li>
<li><p>For a one-variable function, the derivative at a specific point gives the slope of the tangent line to the function’s graph at that point.</p></li>
<li><p>In essence, it tells you the rate at which the function is increasing or decreasing at that particular point.</p></li>
<li><p>Mathematically, the derivative of a function <span class="math notranslate nohighlight">\(f(x)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is denoted as <span class="math notranslate nohighlight">\(f'(x)\)</span> or <span class="math notranslate nohighlight">\( \frac{{df}}{{dx}}\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Partial Derivative</strong>:</p>
<ul>
<li><p>A partial derivative is similar to a derivative, but it’s used for functions with multiple variables.</p></li>
<li><p>When you have a function of multiple variables (e.g., <span class="math notranslate nohighlight">\(f(x, y)\)</span>), taking the partial derivative with respect to one variable (e.g., <span class="math notranslate nohighlight">\( x \)</span>) means you’re looking at how the function changes when only that variable changes, while keeping other variables constant.</p></li>
<li><p>For example, if you have a function <span class="math notranslate nohighlight">\( f(x, y) \)</span> representing the surface of a mountain, the partial derivative with respect to <span class="math notranslate nohighlight">\( x \)</span> would tell you how the slope changes in the east-west direction, while keeping <span class="math notranslate nohighlight">\( y \)</span> constant.</p></li>
<li><p>Mathematically, the partial derivative of a function <span class="math notranslate nohighlight">\( f(x, y) \)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is denoted as <span class="math notranslate nohighlight">\( \frac{{\partial f}}{{\partial x}} \)</span>.</p></li>
</ul>
</li>
<li><p><strong>Gradient</strong>:</p>
<ul>
<li><p>The gradient represents the direction and magnitude of the steepest increase of a function.</p></li>
<li><p>In simpler terms, imagine you’re standing on a hill represented by a mathematical function. The gradient tells you which direction to move in to climb the hill as quickly as possible.</p></li>
<li><p>Mathematically, the gradient of a function is a vector that consists of partial derivatives with respect to each variable in the function.</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="intuition-of-gradients-an-example-skipped">
<h3><a class="toc-backref" href="#id40">Intuition of Gradients: An Example (skipped)</a><a class="headerlink" href="#intuition-of-gradients-an-example-skipped" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Let’s assume that we are training a model with two parameters, <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, and the loss function is <span class="math notranslate nohighlight">\(f(x_0, x_1)\)</span>.</p>
<ul>
<li><p>When a function includes more than one parameters, we can compute the partial derivative of the function with respect to each parameter.</p></li>
<li><p>When all partial derivatives are concatenated into a vector, this vector is called the <strong>gradient</strong>.</p></li>
<li><p>That is, for the above loss function with two parameters (e.g., <span class="math notranslate nohighlight">\(f(x_0, x_1) = \beta x_0 + \beta x_1\)</span>), we can calculate the partial derivatives of each parameter all at once,and represent them in a vector, which is referred to as <strong>gradient</strong>, i.e:
<span class="math notranslate nohighlight">\(
(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1})
\)</span></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Each parameter estimation value pair, <span class="math notranslate nohighlight">\((x_0,x_1)\)</span>, should correspond to a gradient.</p></li>
<li><p>Intuitive understanding of the gradient:</p>
<ul>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> indicates how the changes of the parameter values <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> may contribute to the change of the loss function, <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span>.</p></li>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is a <strong>vector</strong>, which points at the <strong>global minimum</strong> of the loss function.</p></li>
<li><p>The farther the <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is way from the global minimum, the larger the gradient vector.</p></li>
</ul>
</li>
<li><p>In Deep Learning, the training goes as follows:</p>
<ul>
<li><p>We initialize the parameters <span class="math notranslate nohighlight">\(x_0,x_1\)</span> with some values <span class="math notranslate nohighlight">\(p_0, p_1\)</span>;</p></li>
<li><p>We compute the loss function <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span></p></li>
<li><p>We compute the gradient of <span class="math notranslate nohighlight">\(f(x_0,x_1)\)</span> when the parameter <span class="math notranslate nohighlight">\(x_0 = p_0\)</span> and <span class="math notranslate nohighlight">\(x_1 = p_1\)</span></p></li>
<li><p>We use the gradient to determine how to update/modify all the model parameters, i.e.,</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
x_0 = x_0 + \eta\frac{\partial f}{\partial x_0} \\
x_1 = x_1 + \eta\frac{\partial f}{\partial x_1} 
\end{split}\]</div>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is again <strong>learning rate</strong> (e.g., 0.001).</p></li>
</ul>
</section>
<section id="types-of-gradient-descent">
<h3><a class="toc-backref" href="#id41">Types of Gradient Descent</a><a class="headerlink" href="#types-of-gradient-descent" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Batch</strong> Gradient Descent: Update the model weights after one epoch of the entire training set.</p></li>
<li><p><strong>Stochastic</strong> Gradient Descent (SGD): Update the model weights after every instance of the training set (online).</p></li>
<li><p><strong>Mini-batch</strong> Gradient Descent: Update the model weights after a subset of the training set. (Recommended!)</p></li>
</ul>
<p><img alt="" src="../_images/dl-gradient-stochastic.gif" /></p>
<p><img alt="" src="../_images/dl-gradient-mini.gif" /></p>
<p><img alt="" src="../_images/dl-gradient-batch.gif" /></p>
</section>
<section id="gradients-in-python-skipped">
<h3><a class="toc-backref" href="#id42">Gradients in Python (skipped)</a><a class="headerlink" href="#gradients-in-python-skipped" title="Permalink to this headline">#</a></h3>
<p>In the following graph, each vector represents the gradient at a specific <span class="math notranslate nohighlight">\((x_0, x_1)\)</span>, i.e., when <span class="math notranslate nohighlight">\(x_0 = p_0\)</span> and <span class="math notranslate nohighlight">\(x_1 = p_1\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/59885582f09df353ce9df6967d22c1476b1df0eb6ba90a1afbc5aad2aa7bf90b.png" src="../_images/59885582f09df353ce9df6967d22c1476b1df0eb6ba90a1afbc5aad2aa7bf90b.png" />
</div>
</div>
</section>
</section>
<section id="overfitting-and-underfitting">
<h2><a class="toc-backref" href="#id43">Overfitting and Underfitting</a><a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this headline">#</a></h2>
<section id="optimization-vs-generalization">
<h3><a class="toc-backref" href="#id44">Optimization vs. Generalization</a><a class="headerlink" href="#optimization-vs-generalization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>ML is always dealing with the tension between <strong>optimization</strong> and <strong>generalization</strong>.</p>
<ul>
<li><p>Optimization: the process of adjusting a model to get the best performance possible on the training data (i.e., to minimize the <em>bias</em> of the model)</p></li>
<li><p>Generalization: the performance of the model on data it has never seen before (i.e., to minimize the <strong>variation</strong> of the model performance on different datasets.)</p></li>
</ul>
</li>
</ul>
</section>
<section id="overfitting-underfitting">
<h3><a class="toc-backref" href="#id45">Overfitting/Underfitting</a><a class="headerlink" href="#overfitting-underfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>During the training stage, if both the losses on training data and validation data are dropping, the model is <strong>underfit</strong> and in a way to be better fit. (Great bias, Small variation)</p></li>
<li><p>During the training, if the loss on the validation data stalls while that of the training data still drops, the model starts to <strong>overfit</strong>. (Small bias, Great variation)</p></li>
</ul>
</section>
<section id="how-to-deal-with-underfitting">
<h3><a class="toc-backref" href="#id46">How to Deal with Underfitting?</a><a class="headerlink" href="#how-to-deal-with-underfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Increase model complexity</p></li>
<li><p>Adding more training data</p></li>
<li><p>Train for more epochs</p></li>
<li><p>Change learning rates</p></li>
<li><p>Add weight <strong>Regularization</strong></p></li>
<li><p>Add <strong>dropout</strong></p></li>
</ul>
</section>
<section id="how-to-deal-with-overfitting">
<h3><a class="toc-backref" href="#id47">How to Deal With Overfitting?</a><a class="headerlink" href="#how-to-deal-with-overfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Get more training data</p></li>
<li><p>Reduce the complexity of the network (e.g., number of layers/nodes)</p></li>
<li><p>Add weight <strong>Regularization</strong></p></li>
<li><p>Add <strong>dropout</strong></p></li>
<li><p>Batch normalization</p></li>
</ul>
</section>
<section id="regularization">
<h3><a class="toc-backref" href="#id48">Regularization</a><a class="headerlink" href="#regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>What is <strong>regularization</strong>?</p>
<ul>
<li><p>Modulate the quantity of information that the model is allowed to store</p></li>
<li><p>Add constraints on what information the model is allowed to store</p></li>
</ul>
</li>
<li><p>Weight regularization is to add to the loss function of the network a <strong>cost</strong> associated with having large weights.</p></li>
<li><p>That is, large weights will have greater penalties to the loss function, hence discouraged.</p></li>
<li><p>Common methods:</p>
<ul>
<li><p>L1 regularization: the cost added is proportional to the <strong>absolute values</strong> of the weights (<code class="docutils literal notranslate"><span class="pre">keras.regularizer.l1()</span></code>)</p></li>
<li><p>L2 regularization: the cost added is propositional to the <strong>square values</strong> of the weights (<code class="docutils literal notranslate"><span class="pre">keras.regularizer.l2()</span></code>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="dropout">
<h3><a class="toc-backref" href="#id49">Dropout</a><a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Dropout consists of randomly dropping out a number of output features of the applied layer during training.</p></li>
<li><p>The dropout rate is the <strong>fraction</strong> of the features that are <strong>zeroed out</strong>.</p></li>
<li><p>The dropout rate is usually set between 0.2 and 0.5.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">keras</span></code>, it can an independent layer, <code class="docutils literal notranslate"><span class="pre">keras.layers.dropout()</span></code>, or paremters to be set within specific layers (e.g., <code class="docutils literal notranslate"><span class="pre">keras.layers.LSTM()</span></code>).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check <a class="reference external" href="https://keras.io/api/layers/regularization_layers/">Keras Regularization Layers</a> for more information (You can find <code class="docutils literal notranslate"><span class="pre">dropout()</span></code> layer here).</p>
<p>Also, check <a class="reference external" href="https://keras.io/api/layers/regularizers/">Keras Layer weight regularizers</a> for more information about the layer-specific regularization.</p>
</div>
</section>
<section id="heuristics-for-regularization">
<h3><a class="toc-backref" href="#id50">Heuristics for Regularization</a><a class="headerlink" href="#heuristics-for-regularization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Applying dropout before a recurrent layer hinders learning rather than helping with regularization. Instead, dropout is typically applied after recurrent layers, allowing the model to capture temporal dependencies more effectively. This approach helps prevent overfitting by adding noise to the activations of the recurrent units during training, encouraging the network to learn more robust representations.</p></li>
<li><p>The same dropout mask (the same pattern of dropped units) should be applied at every time step, instead of a dropout mask that varies randomly from timestep to timestep. (Yarin Gal)</p></li>
<li><p>In order to regularize the representations formed by the recurrent gates of layers (e.g., LSTM and GRU), a temporally constant dropout mask should be applied to the inner activations of the layer (a recurrent dropout mask). (Yarin Gal)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## pseudo code</span>

<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>

<span class="c1"># Define the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># Add an LSTM layer with recurrent dropout</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">features</span><span class="p">)))</span>

<span class="c1"># Add more layers as needed</span>
<span class="c1"># model.add(...)</span>
<span class="c1"># model.add(...)</span>

<span class="c1"># Compile the model and specify the loss function, optimizer, etc.</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="c1"># Train the model using model.fit(...)</span>
</pre></div>
</div>
</section>
<section id="normalization">
<h3><a class="toc-backref" href="#id51">Normalization</a><a class="headerlink" href="#normalization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>Normalization</strong> is to make sure that sets of numeric values are of a uniform scale (e.g., 0 to 1).</p></li>
<li><p>If the dataset contains numeric data varying in a huge range, it will skew the learning process, resulting in a bad model.</p></li>
<li><p>To properly add normalization layers can help speed up the model convergence for more effective training.</p></li>
</ul>
</section>
<section id="batch-normalization-and-layer-normalization">
<h3><a class="toc-backref" href="#id52">Batch Normalization and Layer Normalization</a><a class="headerlink" href="#batch-normalization-and-layer-normalization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>In Batch Normalization, input values of the same neuron for all the samples in the mini-batch are normalized.</p></li>
<li><p>In Layer Normalization, input values for all neurons in the same layer are normalized for each data sample.</p></li>
<li><p>Heuristics</p>
<ul>
<li><p>Batch Normalization depends on mini-batch size while Layer Normalization doesn’t.</p></li>
<li><p>Batch Normalization works better with CNN while Layer Normalization works better with RNN.</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check <a class="reference external" href="https://keras.io/api/layers/normalization_layers/">Keras Normalization Layers</a> for more information.</p>
</div>
</section>
</section>
<section id="some-math-skipped">
<h2><a class="toc-backref" href="#id53">Some Math (Skipped)</a><a class="headerlink" href="#some-math-skipped" title="Permalink to this headline">#</a></h2>
<p>The following presents some important mathematical constructs related to the understanding of neural network.</p>
<section id="chain-rule-and-back-propagation">
<h3><a class="toc-backref" href="#id54">Chain Rule and Back Propagation</a><a class="headerlink" href="#chain-rule-and-back-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Because there are many parameters in a network, we can compute the gradients (or partial derivatives) of all the weights using the chain rules of derivatives.</p></li>
<li><p>Specifically, the total error is essentially broken up and distributed back through the network to every single weight with the help of chain rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}.\frac{\partial y}{\partial x}\]</div>
<ul class="simple">
<li><p>This process is referred to as <strong>back propagation</strong>: moving back through the network, back-propagating the total errors to every single weight, and updating the weights.</p></li>
<li><p>The principle of weights-updating: the larger the gradient, the more the adjustments.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W_1 = W_1 - \eta \frac{\partial E}{\partial W_1}\]</div>
<ul class="simple">
<li><p>The above adjustment formula suggests that the weight updates are proportional to the partial derivatives of the weight.</p></li>
<li><p>The <strong><span class="math notranslate nohighlight">\(\eta\)</span></strong> in the formula controls the amount of adjustment, which is referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</section>
<section id="elementwise-operations-of-matrix">
<h3><a class="toc-backref" href="#id55">Elementwise Operations of Matrix</a><a class="headerlink" href="#elementwise-operations-of-matrix" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A two-dimensional matrix</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} x = 
\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6 \\
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 2],
       [3, 4],
       [4, 6]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 3]
 [4 5]
 [5 7]]
[[ 5 10]
 [15 20]
 [20 30]]
[[0.2 0.4]
 [0.6 0.8]
 [0.8 1.2]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Matrix Elementwise Multiplication</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
5&amp;6 \\
7&amp;8
\end{pmatrix} =
\begin{pmatrix}
5&amp;12 \\
21&amp;32
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1 2]
 [3 4]]
[[5 6]
 [7 8]]
[[ 5 12]
 [21 32]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="broadcast">
<h3><a class="toc-backref" href="#id56">Broadcast</a><a class="headerlink" href="#broadcast" title="Permalink to this headline">#</a></h3>
<p>In matrix elementwise computation, the smaller tensor will be <strong>broadcasted</strong> to match the shape of the larger tensor.</p>
<ul class="simple">
<li><p>Axes (called broadcast axes) are added to the smaller tensor to match the <code class="docutils literal notranslate"><span class="pre">ndim</span></code> of the larger tensor.</p></li>
<li><p>The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
10&amp;20
\end{pmatrix} =
\begin{pmatrix}
10&amp;40 \\
30&amp;80
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2, 2)
(2,)
[[10 40]
 [30 80]]
(2, 2)
</pre></div>
</div>
</div>
</div>
</section>
<section id="matrices-dot-production">
<h3><a class="toc-backref" href="#id57">Matrices Dot Production</a><a class="headerlink" href="#matrices-dot-production" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/matrices-dot-product.png" />
(Source: Chollet [2018], Ch 2., Figure 2.5)</p>
<ul class="simple">
<li><p>The most common applications may be the <strong>dot product</strong> between two matrices.</p></li>
<li><p>You can take the dot product of two matrices x and y (<code class="docutils literal notranslate"><span class="pre">dot(x,</span> <span class="pre">y)</span></code>) if and only if <code class="docutils literal notranslate"><span class="pre">x.shape[1]</span> <span class="pre">==</span> <span class="pre">y.shape[0]</span></code>. The result is a matrix with shape (<code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code>, <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>), where the coefficients are the vector products between the rows of <span class="math notranslate nohighlight">\(x\)</span> and the columns of <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6
\end{pmatrix}
\begin{pmatrix}
5&amp;6&amp;7 \\
8&amp;9&amp;10
\end{pmatrix} =
\begin{pmatrix}
21&amp;24&amp;27 \\
47&amp;54&amp;62 \\
73&amp;84&amp;95
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>

<span class="n">xy_dot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xy_dot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[21 24 27]
 [47 54 61]
 [73 84 95]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="matrices-dot-production-and-forward-propagation">
<h3><a class="toc-backref" href="#id58">Matrices Dot Production and Forward Propagation</a><a class="headerlink" href="#matrices-dot-production-and-forward-propagation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>For example, let’s assume that we have a word, one-hot encoded as [0,1,0,0,0,0].</p></li>
<li><p>An embedding model consists of parameters like the two-dimensional tensor shown below.</p></li>
<li><p>The output of the model is the dot product of the input word vector and the model parameter tensor.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
-2.8&amp;1.6&amp;0.9&amp;0.7&amp;-0.7&amp;-1.8 \\
0.3&amp;-2.3&amp;0.8&amp;1.8&amp;1.5&amp;0.7\\
0.9&amp;0.3&amp;-1.6&amp;-2.8&amp;0.5&amp;0.4\\
1.8&amp;-0.5&amp;-1.6&amp;-2.8&amp;-1.7&amp;1.7
\end{pmatrix}
\begin{pmatrix}
0\\
1\\
0\\
0\\
0\\
0\\
\end{pmatrix}=
\begin{pmatrix}
1.6 \\
-2.3 \\
0.3\\
-0.5
\end{pmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">word_one_hot</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_one_hot</span><span class="p">)</span>
<span class="n">model_parameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0]
 [1]
 [0]
 [0]
 [0]
 [0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">,</span> <span class="n">word_one_hot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.6],
       [-2.3],
       [ 0.3],
       [-0.5]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="derivatives">
<h3><a class="toc-backref" href="#id59">Derivatives</a><a class="headerlink" href="#derivatives" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Given a normal function, <span class="math notranslate nohighlight">\(f(x) = y \)</span> , if the <span class="math notranslate nohighlight">\(x\)</span> value changes, <span class="math notranslate nohighlight">\(y\)</span> will change as well.</p></li>
<li><p>So if we increase <span class="math notranslate nohighlight">\(x\)</span> by a small factor <span class="math notranslate nohighlight">\(h\)</span>, this results in a small change to <span class="math notranslate nohighlight">\(y\)</span>, i.e., <span class="math notranslate nohighlight">\(f(x+h) - f(x)\)</span>.</p></li>
<li><p>We can then compute the change of <span class="math notranslate nohighlight">\(y\)</span> relative to the small change of <span class="math notranslate nohighlight">\(x\)</span>, i.e., <span class="math notranslate nohighlight">\(\frac{f(x+h) - f(x)}{h}\)</span></p></li>
<li><p>When <span class="math notranslate nohighlight">\(h\)</span> is very very small around a certain point <span class="math notranslate nohighlight">\(p\)</span>, we can then estimate the change of <span class="math notranslate nohighlight">\(y\)</span> at the point when  <span class="math notranslate nohighlight">\(x = p\)</span>, i.e., <span class="math notranslate nohighlight">\(\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\)</span></p></li>
</ul>
<ul class="simple">
<li><p>This instantaneous change of <span class="math notranslate nohighlight">\(y\)</span> is called the <strong>derivetaive</strong> of <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\(p\)</span>.</p>
<ul>
<li><p>If it is negative, it means a small change of <span class="math notranslate nohighlight">\(x\)</span> around <span class="math notranslate nohighlight">\(p\)</span> will result in a decrease of <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
<li><p>If it is positive, a small change in <span class="math notranslate nohighlight">\(x\)</span> will result in an increase of <span class="math notranslate nohighlight">\(f(x)\)</span>.</p></li>
<li><p>The absolute value (i.e., the magnitude) of the derivative indicates how quickly this increase or decrease will happen.</p></li>
</ul>
</li>
<li><p>This can be mathematically represented as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x}= \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]</div>
<ul class="simple">
<li><p>The <strong>derivative</strong> turns out to be the <strong>slope of the tangent line</strong> at <span class="math notranslate nohighlight">\(x = p\)</span>.</p></li>
<li><p>If we are trying to update <span class="math notranslate nohighlight">\(x\)</span> by a factor <span class="math notranslate nohighlight">\(h\)</span> in order to minimize <span class="math notranslate nohighlight">\(f(x)\)</span>, and we know the derivative of <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>, we have everything we need:</p>
<ul>
<li><p>The derivative completely describes how <span class="math notranslate nohighlight">\(f(x)\)</span> evolves when we change <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>If we want to reduce the value of <span class="math notranslate nohighlight">\(f(x)\)</span>, we just need to move <span class="math notranslate nohighlight">\(x\)</span> a little in the opposite direction from the derivative.</p></li>
</ul>
</li>
<li><p>In Deep Learning, the <span class="math notranslate nohighlight">\(f(x)\)</span> is often the <strong>loss function</strong>, and <span class="math notranslate nohighlight">\(x\)</span> is often the parameter of the model.</p>
<ul>
<li><p>We initialize the parameter <span class="math notranslate nohighlight">\(x\)</span> with some value <span class="math notranslate nohighlight">\(p\)</span>;</p></li>
<li><p>We compute the loss function <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
<li><p>We compute the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> when the parameter <span class="math notranslate nohighlight">\(x = p\)</span></p></li>
<li><p>We use the derivative to determine how to update/modify the parameter, i.e., <span class="math notranslate nohighlight">\(x_{new} = x_{old} + \eta\frac{\partial f}{\partial x} \)</span></p></li>
<li><p>The <span class="math notranslate nohighlight">\(\eta\)</span> is commonly referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1">## change of y when a very small change in x</span>
    <span class="c1">#print(d)</span>
    <span class="c1"># d turns out to be the slope of the tangent line</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">d</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Take the derivates of the following function when x = 5 and 10:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[y = 4x^2 + 2x\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">fun_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">tf5</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y5</span> <span class="o">=</span> <span class="n">tf5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">tf10</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y10</span> <span class="o">=</span> <span class="n">tf10</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7f94e528b1b216518b8b72cd95fc358aa9df406f49b1a04ad8167dcfc8669c0e.png" src="../_images/7f94e528b1b216518b8b72cd95fc358aa9df406f49b1a04ad8167dcfc8669c0e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1d7c768fee2d2134bd8713705c1db0ae647c7afca53e674fb44b691df004f1cb.png" src="../_images/1d7c768fee2d2134bd8713705c1db0ae647c7afca53e674fb44b691df004f1cb.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1"># small change of x when x = 5 will slighly change y in positive direction</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>  <span class="c1">## small change of x when x = 10 will greatly change y in positive direction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>41.99999999997317
81.99999999987995
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In python, we use the <strong>numerical differentiation</strong> method to find the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> when x = 5 and 10.</p></li>
<li><p>We can use the <strong>analytic differentiation</strong> method and derive the <strong>derivatie function</strong> <span class="math notranslate nohighlight">\(f'(x)\)</span> first:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f'(x) = \frac{\partial f}{\partial x}= 4x^2 + 2x = 8x + 2
\]</div>
<ul class="simple">
<li><p>Numerical differentiation produces derivatives with errors; analytic differentiation produces exact derivatives.</p></li>
</ul>
</section>
<section id="partial-derivatives">
<h3><a class="toc-backref" href="#id60">Partial Derivatives</a><a class="headerlink" href="#partial-derivatives" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>When a function has many parameters, we can take the derivate of the function with respect to one particular parameter.</p></li>
<li><p>This parameter-specific derivative is called <strong>partial derivative</strong>.</p></li>
<li><p>Take the partial derivatives of the following function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x_0, x_1)=x_0^2 + x_1^2 \]</div>
<ul class="simple">
<li><p>Once if we have defined the function for the model (e.g., the Loss Function), we can calculate to what extent the change in weights would affect the change in the function.</p></li>
<li><p>The partial derivative refers to how a change in a specific weight <span class="math notranslate nohighlight">\(x_1\)</span> affects the function, i.e., the Loss Function or the total error.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial x_1}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## fun_2 has two variables/weights</span>
<span class="k">def</span> <span class="nf">fun_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_0\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_0}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp1</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">4.0</span><span class="o">**</span><span class="mi">2</span>


<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.00000000000378
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_1}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp2</span><span class="p">(</span><span class="n">x1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">3.0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span>


<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7.999999999999119
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id61">References</a><a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>I highly recommend the following two books for deep learning with Python:</p>
<ul>
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010761759">Deep Learning：用Python進行深度學習的基礎理論實作</a></p></li>
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010817138?loc=P_br_r0vq68ygz_D_2aabd0_B_1">Deep Learning 2｜用Python進行自然語言處理的基礎理論實作</a></p></li>
</ul>
</li>
<li><p>This post collections a comprehensive list of learning resourcess for deep learning: <a class="reference external" href="https://buzzorange.com/techorange/2017/08/21/the-best-ai-lesson/">史上最完整機器學習自學攻略！我不相信有人看完這份不會把它加進我的最愛</a>.</p></li>
<li><p>Taylor, Michael. (2017). Neural Networks: A Visual Introduction for Beginners. (cf. Course Data)</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="topic-modeling-naive.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Topic Modeling: A Naive Example</p>
      </div>
    </a>
    <a class="right-next"
       href="dl-simple-case.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Learning: A Simple Example</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow-of-neural-network">Workflow of Neural Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-overview">Neural Network Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation">Backward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons">Neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions-in-python">Activation Functions in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-nodes-to-layers">From Nodes to Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-parameters-and-matrix-mutiplication-self-study">Layer, Parameters, and Matrix Mutiplication (Self-Study)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-neural-networks">Types of Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-neural-network-model-in-python">Building a Neural Network Model in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-in-focus">Packages in Focus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model">Create Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-modules-in-keras">Useful Modules in keras</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-utils"><code class="docutils literal notranslate"><span class="pre">keras.utils</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#keras-layers"><code class="docutils literal notranslate"><span class="pre">keras.layers</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-modules">Other modules</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-example">A Quick Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pseudo-data">Pseudo Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training">Model Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-and-prediction">Evaluation and Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-neural-network-learn-the-parameters">How does the neural network learn the parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principles-for-loss-functions">Principles for Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-variables-encoding">Categorical Variables Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-wrong-with-categorical-variables">What’s wrong with categorical variables?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-ways-of-encodings">Three Ways of Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-and-loss-function">One-hot Encoding and Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-loss-errors-skipped">Examples of Loss Errors (skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-based-on-one-sample">Error based on One Sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#errors-based-on-batch-samples">Errors based on Batch Samples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-adjust-the-model-parameters">How do we adjust the model parameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-gradients-an-example-skipped">Intuition of Gradients: An Example (skipped)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent">Types of Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-in-python-skipped">Gradients in Python (skipped)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting">Overfitting and Underfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-vs-generalization">Optimization vs. Generalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-underfitting">Overfitting/Underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-underfitting">How to Deal with Underfitting?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-overfitting">How to Deal With Overfitting?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#heuristics-for-regularization">Heuristics for Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization-and-layer-normalization">Batch Normalization and Layer Normalization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-math-skipped">Some Math (Skipped)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-and-back-propagation">Chain Rule and Back Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementwise-operations-of-matrix">Elementwise Operations of Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcast">Broadcast</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production">Matrices Dot Production</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-dot-production-and-forward-propagation">Matrices Dot Production and Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives">Derivatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives">Partial Derivatives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alvin Cheng-Hsien Chen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023 Alvin Chen.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>